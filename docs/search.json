[
  {
    "objectID": "randomisation.html",
    "href": "randomisation.html",
    "title": "The Magic of Randomisation",
    "section": "",
    "text": "This chapter covers how randomisation solves the problem of selection bias, and why randomisation is considered the “gold standard” of causal inference.\n\nLet us say we are interested in this question:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nOur concern is a confounder. For example, smartness of an individual could mean they are more likely to get a scholarship. Since smart people tend to perform well at university, that means the people who get treated are different from those who don’t get treated.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmartness is not the only confounder. Other confounders could be family income, athletic ability, etc.\n\nBut what if randomness (like flipping a coin) controls who gets the treatment or not. A coin will be flipped to decide if every person in our study will get the scholarship. This means that the randomness (the coin), and not the confounder, are causing selection into treatment:\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;Y\n\n\n\n\n\n\nO\n\nCoin\n\n\n\nO-&gt;D\n\n\n\n\n\n\n\n\n\n\n\nSince the confounder is no longer causing who gets the treatment and who doesn’t, that means there is no more concern of selection bias.\n\nRandomisation also means that every individual has the same chance of being treated or untreated, so the two groups will, on average, the same as each other. That means:\n\\[\n\\textcolor{red}{\\text{grades}_\\text{untreated}^{(0)}} = \\textcolor{red}{\\text{grades}_\\text{treated}^{(0)}}\n\\]\nAnd thus, since that is true, that means correlation equals causation.\n\nThis is established by the law of large numbers, but it is a little technical for here.\n\nSo if our treatment is randomly assigned (individuals randomly assigned to treatment or control), correlation does equal causation.\n\nRandomisation is the gold standard of causal inference. There is no better method.\n\nRandomisation is possible if you are running your own experiment: you can use a random number generator to assign treatment.\nRandomisation is also possible if there is something that is being randomly assigned in the real world. For example, the US green card lottery randomly chooses who gets accepted.\n\nHowever, randomisation is not always possible to due to cost of running experiments, non-compliance of individuals within experiments, and impracticality.\n\nNon-compliance is an issue that can be solved pretty easily with an instrumental variable, given a few assumptions about the non-compliance.",
    "crumbs": [
      "Home",
      "Magic of Randomisation"
    ]
  },
  {
    "objectID": "latent/pca.html",
    "href": "latent/pca.html",
    "title": "Principle Components Analysis",
    "section": "",
    "text": "Note: PCA is not a latent variable model itself, but can be used to approximate a latent variable model.\n\nPrinciple components analysis (PCA) takes observed variables (called features), and change them into new variables called principle components, without losing any information.\nEssentially, what PCA does is it takes our features, and finds the “axis” in which there is the most variation, and makes that the 1st component. Then it finds the “axis” with the 2nd most variation, and makes that the 2nd component, and so on.\n\n\n\n\n\n\nNote that each principle component is orthogonal/uncorrelated with each other by design. Also note that the 1st component explains the most variance in our features, then the 2nd component, and so on.\n\nEach new principle component is a weighted average of the observed variables:\n\\[\n\\begin{align}\n\\comp_1 & = \\omega_{11}\\feat_1 + \\omega_{21}\\feat_2 + \\omega_{31}\\feat_3 + \\dots \\\\\n\\comp_2 & = \\omega_{12}\\feat_1 + \\omega_{22} \\feat_2 + \\omega_{32}\\feat_3 + \\dots \\\\\n\\end{align}\n\\]\nThe \\(\\omega\\)’s are the weights of each observed feature in creating a principle component, and are determined by the correlation matrix of the features. For interpreting a component, we often “normalise” the weights \\(\\omega\\) to get the correlation between a observed feature and a principle component:\n\\[\nCorr(\\feat_i, \\comp_j) =  \\omega_{ij} \\cdot \\sqrt{Var(\\comp_j)}\n\\]\n\nThis value is also called a component loading. Note that this is only true given we perform PCA on the correlation matrix (which is standard, but you can use a covariance matrix).\n\nInterpreting the principle components is identical to that of factor analysis - just using correlations rather than factor loadings. Below, the rows are features describing how much an individual trusts different institutions. The columns are the principle components.\n\n\n\n\n\nIn component 1, we see that the correlation is pretty high for all of the variables, and positive. We might conclude component 1 measures general trust in institutions.\nIn component 2, we can see that the loadgins for legal and police are the highest (in absolute terms) and negative. Meanwhile, politicians, pol_parties, and EP are positive and still not too small. The other loadings are quite small. We might interpret this component as sort of a tradeoff between political trust and legal/law enforcement trust, with higher values indicating more political trust, and lower values indicating more trust in legal/police.\nJust like factor analysis, we can also calculate principle component scores, which are the individual values of each principle component for each individual in our data.\n\nTo implement principle components analysis, we do the following:\n\npca &lt;- princomp(~ X1 + X2 + X3,\n                data = my_data,\n                cor = TRUE, #use correlation matrix\n                scores = TRUE, #calculate pc scores\n                na.action=na.exclude)\nsummary(pca)\n\nNow, to get the component loadings/corelation, we do the following:\n\n# grab the weights\nweights &lt;- loadings(pca)\n\n# grab the sqrt of variance\nsqrt.var &lt;- pca$sdev\n\n# calculate component loadings/correlation\nprint(t(t(weights)*sqrt.var), cutoff = 0, digits=4)\n\nTo access principle component scores, we do:\n\npca$score\n\nIf we are performing other statistical analysis, we might want to choose how many of the new principle components we want to use. A scree-plot shows the percentage of variance each component explains.\n\nscreeplot(pca, type='l', main=\"\")\n\n\nTo choose the amount of components to use, we look for the “elbow” in the plot - basically when adding another additional PC does not really increase the amount of variance explained significantly anymore.",
    "crumbs": [
      "Latent Variable Models",
      "Principle Components Analysis"
    ]
  },
  {
    "objectID": "latent/identify.html",
    "href": "latent/identify.html",
    "title": "Identification and Rotation",
    "section": "",
    "text": "Make sure you have read all the previous pages on factor analysis before this.\n\nWhen estimating factor analysis models, we encounter many identification issues. One identification issue is rotation.\nFor example, imagine our latent variable is left-right political spectrum leaning. Our latent variable could use positive values for right-wing and negative values for left-wing. Or, it could do the opposite. There is no real substantive difference.\nThus, in 1-factor models, most software will arbitrarily choose a rotation, as it does not really make a huge difference.\nIn multiple factor models, rotation becomes more complex. Because of some linear algebra, you can have almost infinite rotations of factors that make no difference. Just like in the 1-factor model, software will arbitrarily choose one rotation.\nHowever, in multiple factor models, the chosen rotation can make a big difference in how easy it is to interpret our solutions.\n\nThe default rotation is often an orthogonal rotation. This means the factors (if visualised in space) are perpendicular to each other, with zero correlation.\nAn oblique rotation is one where the factors are allowed to be correlated with each other (non-perpindicular).\n\n\n\n\n\n\nThe oblique rotations are generally easier to interpret. This is because they make some items have close to 0 factor loadings, which means we can easily say that factor is not measuring that specific item.\nRotations are a unique identification issue. Factor analysis also has some lack-of-parameters identification issues that result in no possible solutions. To avoid this, we should generally have at least 3 items for a 1-factor model, and 2 items per factor for a multiple-factor model.",
    "crumbs": [
      "Latent Variable Models",
      "Identification and Rotation"
    ]
  },
  {
    "objectID": "latent/longitudinal.html",
    "href": "latent/longitudinal.html",
    "title": "Lagged Response Models",
    "section": "",
    "text": "A lagged response model is a regression model that says that the outcome variable not only depends on the input variables, but also previous values of the outcome from the past.\n\\[\n\\eta_{it} = \\beta_0 + \\underbrace{\\beta_1 X_{1,it} + \\beta_2 X_{2,it}}_{t} + \\underbrace{\\beta_3 \\ \\eta_{i,t-1}}_{t-1} + \\eps_{it}\n\\]\n\n\n\n\n\n\n\n\nexample2\n\n\nY1\n\nη (t-1)\n\n\n\nY2\n\nη (t)\n\n\n\nY1-&gt;Y2\n\n\nLag\n\n\n\nX1\n\nX1 and X2\n\n\n\nX1-&gt;Y1\n\n\n\n\n\nX1-&gt;Y2\n\n\n\n\n\n\n\n\n\n\n\nRecall the response variable is itself a latent variable, measured with observed items. This also implies a measurement model relating the latent response with each observed item:\n\\[\n\\b X_{t} = \\b\\tau_t + \\b\\eta'\\b\\lambda_t + \\b\\delta_{t}\n\\]\n\nThis is just the model for one (of many) items. Note how the parameters have the subscript \\(t\\) - this implies a different measurement model for each item, at each time period.\n\nHowever, because each item is observed over multiple time periods, it is likely that an item from \\(t=1\\) is correlated with the same item from \\(t=2\\) ( autocorrelation). We have to take this into account in one of two ways:\n\nWe could constrain the model, assuming that the measurement model is identical throughout different time periods. That means \\(\\tau_t\\) and \\(\\lambda_t\\) will now no longer depend on the time period (the same for all time periods, for the same item).\nOr we could complicate the model, by including correlation between the error terms \\(\\eps\\) for the same item at different times.\n\n\nTo implement lagged response models, we need the lavaan package:\n\nlibrary(lavaan)\n\nFor a constrained two-period model, the estimation is as follows:\n\nmodel &lt;- '\n# explanatory variables Measurement models \n  explanatory1 =~ NA*X1 + X2 + X3\n  explanatory2 =~ NA*Z1 + Z2 + Z3\n\n# response variables for time = 1 and time = 2\n    response1 =~ la*Y1t1 + lb*Y2t1 + lc*Y3t1\n    response2 =~ la*Y1t2 + lb*Y2t2 + lc*Y3t2\n\n# time = 1 model\n  response1 ~ explanatory1 + explanatory2\n  \n# time = 2 model with lagged time = 1\n  response2 ~ explanatory1 + explanatory2 + response1\n\n# Fixing the variances of the exogenous latent variables LR and AL  \n  LR~~1*LR\n  AL~~1*AL\n'\n\nsem &lt;- sem(model, data = my_data, missing=\"FIML\")\nsummary(sem)\n\n\nThe labels la*, lb*, and lc* indicate to R to make all factor loadings with the same labels equivalent to each other. Notice how we put the same labels on the different time period measurement models.\n\nFor a unconstrained model, the code is as follows:\n\nmodel &lt;- '\n# explanatory variables Measurement models \n  explanatory1 =~ NA*X1 + X2 + X3\n  explanatory2 =~ NA*Z1 + Z2 + Z3\n\n# response variables for time = 1 and time = 2\n    response1 =~ Y1t1 + Y2t1 + Y3t1\n    response2 =~ Y1t2 + Y2t2 + Y3t2\n\n## Error covariance in the measurement model\n  Y3t1 ~~ Y3t2\n\n# time = 1 model\n  response1 ~ explanatory1 + explanatory2\n  \n# time = 2 model with lagged time = 1\n  response2 ~ explanatory1 + explanatory2 + response1\n\n# Fixing the variances of the exogenous latent variables LR and AL  \n  LR~~1*LR\n  AL~~1*AL\n'\n\nsem &lt;- sem(model, data = my_data, missing=\"FIML\")\nsummary(sem)",
    "crumbs": [
      "Latent Variable Models",
      "Lagged Response Models"
    ]
  },
  {
    "objectID": "latent/class.html",
    "href": "latent/class.html",
    "title": "Latent Class Model",
    "section": "",
    "text": "Latent class models are an extension of factor analysis, that deals with cases of a categorical latent variable with categorical/binary observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe factors a categorical variable, which each category \\(c\\) called a latent class. We also have categorical observed items. The latent class models connects each item with a factor through a item response probability:\n\\[\n\\begin{align}\n& Pr(X_1 = \\text{category }k_i \\ | \\ \\xi = \\text{category }c)\\\\\n& Pr(X_2 = \\text{category }k_i  \\ | \\ \\xi = \\text{category }c)\\\\\n& Pr(X_3 = \\text{category }k_i  \\ | \\ \\xi = \\text{category }c)\\\\\n\\end{align}\n\\]\n\n\\(k_1\\) is one specific category of item \\(1\\), same for \\(k_2\\) for item \\(2\\), and so on. \\(c\\) is one specific category of the factor.\n\nWe also have another part of the measurement mode, the structural model, which determines the probability of each category in the factor:\n\\[\n\\alpha_c = Pr(\\xi=\\text{category }c)\n\\]\nInterpretation of the latent factor depends on the item response probabilities. Below, the columns are the different classes/categories of the factor, and the big rows are each item.\n\n\n\n\n\nThe first class (the first column) has the highest probabilities if individuals never worry about crime, no real effect on quality of life, never worry about burglary, and no real effect on quality of life. Thus, we can conclude this first category of the latent variable is something like - not worried about crime.\nThe second class (the 2nd column), where the top responses have the highest probabilities except for the frequency of worry about burglaries - where the probabilities are highest for some of the time and just occasionally. This suggests that this second category is measuring something like - only worried about burglary, and no other crime.\nWe can also create factor scores - which is a little different, because now we are basically assigning every unit in our data to a category of the latent factor. This is done by calculating the posterior probability of being in each class:\n\\[\n\\widehat{Pr}(\\xi = \\text{category }c \\ | \\ X_1 = \\text{category }k_1, X_2 = \\text{category }k_2, \\dots )\n\\]\nWe calculate this probability for all categories \\(c\\) in the factor. Whichever category \\(c\\) of the latent variable has the highest probability, is the category a unit is assigned to.\n\nThis can be considered quite similar to that of cluster analysis, which will be introduced later.\n\n\nTo implement latent class models, we will need the polLCA package:\n\nlibrary(poLCA)\n\nThis package requires that our categories of items are labelled starting with 1. This means if you have a binary variable of 0 and 1, you will need to change it to 1 and 2.\nTo begin, we will first need to create a vector of our item names:\n\nvars &lt;- c(\"item1\",\"item2\",\"item3\",\"item4\")\n\nThen, let us fit our model as follows:\n\nform &lt;- cbind(item1, item2, item3, item4) ~1\nmodel &lt;- poLCA(form,\n              my_data[,vars],\n              nclass = 2, #number of categories for factor\n              na.rm = F,\n              nrep = 10) \n\n\nna.rm = F means to include missing values when estimating (which is recommended). nrep = 10 indicates how many times to run the gradient descent algorithm - more is better, but will take longer.\n\nThe traditional output is hard to read, so we will use a function:\n\n# function\nLCA.probs &lt;- function(res){\n  probs &lt;- res$probs\n  item.p &lt;- NULL\n  for(i in seq_along(probs)){\n        m.tmp &lt;- t(probs[[i]])\n        rownames(m.tmp) &lt;- paste(names(probs)[i],colnames(probs[[i]]),sep=\".\")\n        item.p &lt;- rbind(item.p,m.tmp)\n  }\n  item.p &lt;- round(item.p,3)\n  class.p &lt;- res$P\n  names(class.p) &lt;- colnames(item.p)\n  list(item.probabilities=item.p,class.probabilities=class.p)\n}\n\n# output results\nLCA.probs(model)\n\nWe can calculate factor scores/classification as follows:\n\nmodel$predclass\n\nWe can choose our model based on the AIC or BIC score.\n\nmodel$aic\nmodel$bic",
    "crumbs": [
      "Latent Variable Models",
      "Latent Class Models"
    ]
  },
  {
    "objectID": "latent/irt.html",
    "href": "latent/irt.html",
    "title": "Item Response Theory",
    "section": "",
    "text": "Item Response Theory (IRT) is an extension of factor analysis, that deals with cases with a continuous latent factor but a binary/categorical set of observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nWe must have at least 3 observed items for estimation purposes. The items are associated with the continuous latent factor with a binomial logistic model:\n\\[\n\\begin{align}\n\\text{logit}[Pr(X_1 = 1 \\ | \\ \\xi)] & = \\tau_1 + \\lambda_1 \\ \\xi \\\\\n\\text{logit}[Pr(X_2 = 1 \\ | \\ \\xi)] & = \\tau_2 + \\lambda_2 \\ \\xi \\\\\n\\text{logit}[Pr(X_3 = 1 \\ | \\ \\xi)] & = \\tau_3 + \\lambda_3 \\ \\xi \\\\\n\\end{align}\n\\]\n\nFor categorical (more than 2 categories) items, a multinomial or ordinal logistic model is used. But this is pretty rare for someone to actually fit a model of this type.\n\n\\(\\tau_i\\) is the intercept of the model, called the difficulty. \\(\\lambda_i\\) is the coefficeint that describes the relationship between any item \\(X_i\\) and the factor. These are called factor loadings. It is also called the discrimination parameter.\nThese factor loadings \\(\\lambda\\) are interepreted in a very similar way to factor analysis. The intercepts \\(\\tau_i\\) also tell us how “common” a value of 1 for the item is. For example, below, the items are policies that politicians support/don’t oppose.\n\n\n\n\n\n\na1 are the loadings \\(\\lambda\\), and d is the intercept. The rows are binary items - with 1 indicating a politician supports that, and 0 indicating a politician opposes that.\n\nWe can first look at the signs of \\(\\lambda\\): The latent factor increases with agreeing (a value of 1) with PartialBirth, CapitalGains, and CAFTA. The latent factor decreases with agreeing (a value of 1) with StemCell, IraqWithdraw, Immigration, and Minimum Wage.\nFrom this, we can kind of get a sense that latent variable is measuring right-left political affiliation, with higher values indicating right-wing, since agreeing left wing policies like IraqWithdraw, Immigration, and Minimum Wage are negative.\nThe absolute size of the factor loadings are quite similar - we can see that CAFTA is the least important item to the factor.\n\nTo run an item response theory mode, we need the mirt package:\n\nlibrary(mirt)\n\nThen, we run the model as follows (make sure to subset your data to only include the items).\n\nmodel &lt;- mirt(data = my_data, model = 1, SE = TRUE)\ncoef(result)\n\n\nmodel = 1 indicates how many factors you want to include.\n\nTo get factor scores, we do the following:\n\nscores &lt;- as.vector(fscores(model, method = \"EAP\"))\n\n\n\n\n\n\n\nMore on Choosing Between Models\n\n\n\n\n\nTo compare and choose between different models, we have a few ways.\n\nWe can use a likelihood ratio test for nested models (see the regression section for more details).\n\n\nlavTestLRT(model1, model2)\n\n\nWe can use a global goodness of fit test - essentially a likelihood ratio but with the full sample covariance matrix as the null model. We want to fail to reject the null, because we want our model to be as close to the sample covariance matrix as possible.\n\n\nlavTestLRT(model)\n\n\nWe can use AIC and BIC to compare models since factor models are estimated with MLE. These are included in the output.\n\nUnfortunately, fit indicies like RMSEA do not work on IRT models.",
    "crumbs": [
      "Latent Variable Models",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "latent/factor.html",
    "href": "latent/factor.html",
    "title": "Factor Analysis Model",
    "section": "",
    "text": "Factor analysis is a way to model a unobserved continuous latent variable \\(\\xi\\), based on a bunch of continuous observed items \\(X_1, X_2, \\dots\\).\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nEach item \\(X_i\\) is connected with the latent factor \\(\\xi\\) by a linear regression:\n\\[\n\\begin{align}\nX_1 & = \\tau_1 + \\blue{\\lambda_1} \\ \\xi + \\delta_1 \\\\\nX_2 & = \\tau_2 + \\blue{\\lambda_2} \\ \\xi + \\delta_2 \\\\\nX_3 & = \\tau_3 + \\blue{\\lambda_3} \\ \\xi + \\delta_3 \\\\\n\\end{align}\n\\]\n\n\\(\\tau\\) is the intercept of the model. \\(\\lambda\\) is the coefficient, called the factor loadings. \\(\\delta\\) is the error term - the part of an item not explained by the factor - they are called the unique factors.\n\n\n\n\n\n\n\nDetails: Assumptions of the Model\n\n\n\n\n\n\nWe make a few assumptions on this model:\n\nWe assume the factor is continuous, and normally distributed with mean \\(\\kappa\\) and variance \\(\\phi\\). We often assume \\(\\kappa = 0\\) and \\(\\phi = 1\\) for identification purposes.\nWe assume the items are also normally distributed.\nWe assume that the error terms are normally distributed \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\), that the different error terms \\(\\delta_1, \\dots, \\delta_p\\) are uncorrelated with each other. This implies that the correlation between any two items is entirely explained by the factor (there is no separate correlation between items).\nWe assume the factor is uncorrelated with the error term (exogeneity).\n\n\n\n\n\nThe factor loadings \\(\\blue{\\lambda_i}\\) represent the relationship/covariance between any item and a factor. These factor loadings help us interpret our latent variable. The sign of the factor loading tells us the direction in which our latent variable is measuring. The absolute size of the factor loading tells us how important that item is to the factor.\nFor example, take this factor variable called personality, explained by a set of items (rich, admire, success, respect) that show how important a certain quality is to an individual.\n\n\n\n\n\nWe can see all the factor loadings \\(\\lambda\\) are positive, which means that the higher values of the personality factor is measuring higher levels of importance of being rich, being admired, being successful, and being respected.\nWe can also see the loadings for respect and success are much higher than admire or rich. Thus, we can conclude the personality factor is more measuring the importance of being respected or successful, than being rich or admired.\n\n\n\n\n\n\nDetails: More and Factor Loadings\n\n\n\n\n\n\nIf all our items are standardised to a standard normal distribution, that also implies that our factor loadings \\(\\lambda\\) are equal to the correlation coefficients between items and factor.\nWe can conduct hypothesis testing with each factor loading \\(\\lambda\\) with a z-test to see if there is a significant relationship between a factor an an item.\n\n\n\n\n\nTo implement factor analysis, we will need the psych and GPArotation package:\n\nlibrary(psych)\nlibrary(GPArotation)\n\nFirst, we should get rid of missing observations:\n\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\nFor factor analysis with one factor, we use the syntax:\n\nfa &lt;- fa(data[,items], nfactors=1, fm=\"ml\")\nprint(fa1)",
    "crumbs": [
      "Latent Variable Models",
      "Factor Analysis Models"
    ]
  },
  {
    "objectID": "latent/scores.html",
    "href": "latent/scores.html",
    "title": "Reliability and Factor Scores",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nRecall our measurement models linking each item \\(i\\) to our factor:\n\\[\nX_i = \\tau_i + \\lambda_i\\ \\xi + \\delta_i\n\\]\nThe the error term has a mean of 0 and a variance of \\(\\theta_i\\). The variance of each item can be shown to equal:\n\\[\nVar(X_i) = \\lambda_i^2 + \\theta_i\n\\]\nThus, this allows us to essentially “split” the variance in any item into two parts:\n\n\\(\\lambda_i^2\\) is the part of the variance in the item explained by the latent factor. We call this the communality of the item \\(i\\).\n\\(\\theta_i\\) is the residual variance, the part of the variance not explained by our factor.\n\nWe can also calculate the percentage/proportion of variance in item \\(i\\) that our factor explains, called the reliability:\n\\[\n\\text{Reliability} = \\frac{\\lambda_i^2}{Var(X_i)} = \\frac{\\lambda_i^2}{\\lambda_i^2+\\theta_i}\n\\]\n\nIf all items are standarised to a standard normal, then \\(\\lambda_i^2\\) is equal to the reliability.\n\nItems with higher reliability are considered more “accurate” measures of the latent variable factor. They thus ensure better model estimation - and when a factor has low relaibility, we will often drop it.\nFactor scores \\(\\widehat\\xi\\) are essentially values of the latent variable for individuals in our study. This allows us to use our observed items to calculate the latent variable value that any individual should have, which we can then put into another statistical model.\n\\[\n\\widehat\\xi = \\omega_0 + \\omega_1 X_1 + \\omega_2 X_2 + \\dots\n\\]\nThe weights are calculated based on the communalities. The items with the highest communalities tend to get the strongest weights, while the items with the least communalities get the smallest weights.\n\nTo calculate factor analysis, we have to first run a factor analysis model in the same way we did in the last page.\n\nlibrary(psych)\nlibrary(GPArotation)\n\n# eliminate missing observations\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\n# factor model\nfa &lt;- fa(data[,items], nfactors=1, fm=\"ml\")\n\nR will automatically calculate factor scores in the estimation process, so all we have to do is access it within our output object:\n\nfa$scores\n\nYou can save this into your dataset, and use for other purposes.",
    "crumbs": [
      "Latent Variable Models",
      "Reliability and Factor Scores"
    ]
  },
  {
    "objectID": "soo/interact.html",
    "href": "soo/interact.html",
    "title": "Fully Interacted Estimator",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nFrom selection on observables, we know that our causal effect is a weighted average:\n\\[\n\\ate = \\sum\\tau_\\text{X} \\cdot Pr( X)\n\\]\nNotice how the weights are the probability of the confounder values of \\(\\b x\\). With some complex math (Angrist 1998), we can actually show that OLS actually estimates:\n\\[\n\\hat\\beta_\\text{OLS} = \\sum \\tau_X\\cdot \\underbrace{\\frac{Var(D_i | X)Pr(X)}{\\sum Var(D_i | X^c)Pr( X^c)}}_{\\text{weight}}\n\\]\n\nWhere \\(X^c\\) is the complement (not \\(X\\)).\n\nThese weights are not equivalent to the selection on observables \\(\\ate\\). Thus, if not all \\(\\tau_\\text{X}\\) are exactly the same (which implies heterogeneity), then our linear regression estimator will incorrectly estimate the ATE.\nHeterogeneity is present in almost all situations we are interested in. Lin (2013) proposes the fully interacted estimator, which allows for consistent estimation of the ATE even with heterogeneity:\n\\[\nY_i = \\alpha + D_i\\ \\ate + \\underbrace{(\\b X_i - \\b{\\mean X})'\\b\\beta + D_i(\\b X_i - \\b{\\mean X})'\\b\\gamma}_{\\text{interactions with de-meaned covariates}}+ \\eps_i\n\\]\nThe new OLS estimate of \\(\\ate\\) in this estimator will technically still be a biased estimator of the ATE, but the bias is negligible.\n\nBefore you implement the estimator, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the estimatr package:\n\nlibrary(estimatr)\n\nThen, we can use the lm_lin() function to estimate:\n\nate &lt;- estimatr::lm_lin(outcome ~ treatment,\n                        covariates = ~ covariate1 + covariate2,\n                        data = my_data)\nsummary(ate)\n\nThe output will be the ATE - the average treatment effect for all units in the study.",
    "crumbs": [
      "Selection on Observables",
      "Fully Interacted Estimator"
    ]
  },
  {
    "objectID": "soo/ipw.html",
    "href": "soo/ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nLet us look at this example, with a confounder.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nReceiving Scholarship\n\n\n\nY\n\nUniversity Grades\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s pretend there are only dumb and smart people (for simplicity). Our treated and control groups might be:\n\n\n\n\n\n\n\nTreated (Got Scholarship)\nUntreated (Did not get scholarship)\n\n\nSmart Students (x4)\nSmart Students (x1)\n\n\nDumb Students (x1)\nDumb Students (x4)\n\n\n\nOur two groups have pre-existing differences. However, by emphasising certain individuals, we can make it seem like there are no more imbalances. For example, weighting might make our above table become:\n\n\n\n\n\n\n\nTreated (Got Scholarship)\nUntreated (Did not get scholarship)\n\n\nSmart Students (x4)\nSmart Students (emphasise to x4)\n\n\nDumb Students (emphasise to x4)\nDumb Students (x4)\n\n\n\n\nSee how the underrepresented individuals in each group (treated/untreated) were weighted upwards. More technically, inverse probability weighting emphasises/weights an individual by the inverse of their likelihood to receive treatment.\n\nWe can see there is no more pre-existing differences after weighting. Thus, selection bias has been solved.\n\nBefore you inverse probability weighting, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the estimatr package:\n\nlibrary(estimatr)\n\nTo estimate the propensity scores and weights, we can use the glm() command:\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity, type = \"response\")\nmy_data$ipw &lt;- ifelse(mydata$D == 1,\n                      1/my_data$pscore,\n                      1/(1-my_data$pscore))\n\nFinally, we need to use the lm_robust() command to estimate our causal effects:\n\nate &lt;- estimatr::lm_robust(Y ~ D, data = my_data, weights = ipw)\nsummary(ate)\n\nThe output will be the ATE - the average treatment effect for all units in the study.",
    "crumbs": [
      "Selection on Observables",
      "Inverse Probability Weighting"
    ]
  },
  {
    "objectID": "soo/regress.html",
    "href": "soo/regress.html",
    "title": "Linear Regression Estimator",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nSelection on observables is about controlling for confounders. Linear regression is a very natural way to control for confounders.\nUsing this idea, we can implement causal inference with regression, with one of the explanatory variables being our treatment variable, and the rest of the explanatory variables being control variables.\n\\[\nY_i = \\alpha + D_i\\ \\ate + \\covs + \\eps_i\n\\]\nOur ordinary least squares (OLS) estimate \\(\\hat\\ate\\) is an unbiased estimator of the true ATE given three conditions are met:\n\nWe meet the selection on observables assumption of conditional ignorability. Conditional ignorability implies exogeneity, which means the estimate is unbiased.\nThe relationship between our continuous confounders and outcome variable is linear. This is because if the true relationship between these two is not linear, then our linear model is wrong, so it is not properly controlling for confounders.\nThere is no heterogeneity in treatment effects. Angrist (1998), Lin (2013), and Słoczyński (2022) have proven that when there is heterogeneity, OLS is estimating another quantity that is not the ATE. This is covered in more detail on fully interacted estimator page\n\n\nHeterogeneity means that different individuals have different individual treament effects \\(\\tau_i\\). OLS only estimates the ATE if there is homogeneity - all \\(\\tau_i\\) are equal.\n\n\nBefore you implement the estimator, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the fixest package:\n\nlibrary(fixest)\n\nTo run a regression estimator, we do:\n\nfeols(outcome ~ treatment + covariate1 + covariate2,\n      data = data,\n      se = \"hetero\")\n\n\nWe typically assume heteroscedasticity, so we use heteroscedasticity-robust standard errors. If you can prove homoscedasticity, then you can use normal standard errors.\n\nThe coefficient for the treatment variable will be the ATE - the average treatment effect for all units in the study.\n\nAssuming you have met all the assumptions of selection on observables, and the special assumptions for the linear regression estimator shown above.",
    "crumbs": [
      "Selection on Observables",
      "Linear Regression Estimator"
    ]
  },
  {
    "objectID": "soo/soochoose.html",
    "href": "soo/soochoose.html",
    "title": "Choosing an Estimator",
    "section": "",
    "text": "We know how selection on observables works now. But how do we actually implement selection on observables? Below are a list of estimators and their strengths/weaknesses. You can use multiple simultaneously for robustness.\n\n\n\n\n\n\nLinear Regression Estimator\n\nEstimand: ATE\nPros: very simple, works well with small datasets.\nCons: 1) requires linear relationship between confounders and outcome, 2) does not work when there are heterogenous treatment effects.\n\n\nKevin’s Comments: since heterogeneity is so common in almost everything, I would recommend against using this estimator unless nothing else is possible. The fully interacted estimator (below) is just a better version of this.\n\n\n\nFully Interacted Estimator\n\nEstimand: ATE\nPros: 1) modified regression to allow for heterogenous effects, 2) still relatively simple.\nCons: requires linear relationship between confounders and outcome\n\n\n\nDistance Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome. 2) relatively intuitive idea.\nCons: 1) can be badly biased when more than 3-5 confounders, 2) throws out unmatched data so wastes data.\n\n\nKevin’s Comments: there is little reason to use distance matching over genetic matching, unless your machine physically cannot estimate genetic matching.\n\n\n\nPropensity Score Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) can handle larger amounts of confounders than distance matching.\nCons: 1) needs a large sample size to not be biased, 2) throws out unmatched data so wastes data.\n\n\nKevin’s Comments: there is little reason to use propensity score matching over genetic matching, unless your machine physically cannot estimate genetic matching.\n\n\n\nGenetic Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) shown to be the best matching estimator\nCons: 1) throws out unmatched data so wastes data, 2) can be computationally taxing.\n\n\n\nInverse Probability Weighting\n\nEstimand: ATE\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) does not waste data like matching methods do.\nCons: requires a large sample size to be unbiased.",
    "crumbs": [
      "Selection on Observables",
      "Choosing an Estimator"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resources on Causal Inference and Social Science Statistics",
    "section": "",
    "text": "For my personal website, see http://kevinli03.github.io\nThis repository contains resources on causal inference and social statistics, specifically focused on applications to politics and economics.\n\nUse the top navigation bar to navigate between different methods.\nUse the side navigation bar to navigate within a method.\n\nR-code for implementation is included. There are some stuff that doesn’t fit into any category, which I have included in the home page sidebar.\n\nNotation Notes:\n\nFor potential outcomes, green means under treatment, and red means under control.\n\\(i\\) refers to units within our data, and \\(t\\) refers to time periods.\nGreek letters are used for parameters that need to be estimated. I typically use \\(\\beta\\) for most coefficients multiplied to variables, \\(\\tau\\) to represent causal effects, and \\(\\eps\\) to represent the error term.",
    "crumbs": [
      "Home",
      "Homepage"
    ]
  },
  {
    "objectID": "frameworks.html",
    "href": "frameworks.html",
    "title": "Basics of Causality",
    "section": "",
    "text": "This page covers the potential outcomes framework and the causal estimands.\n\nIn causal inference, we are interested in causal questions:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nTreatment (D)\n\n\n\nY\n\nOutcome (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\n\nWe generally assume the treatment \\(D\\) is binary.\n\nImagine we have two hypothetical parallel worlds that are copies of each other. Both of these worlds are identical except for one aspect: the treatment:\n\n\n\n\n\n\n\n\nParallel World\nTreatment\nPotential Outcome\n\n\nUnit \\(i\\) does not Receive Treatment\n\\(\\text{treatment}_i = 0\\)\n\\(\\T\\)\n\n\nUnit \\(i\\) Receives Treatment\n\\(\\text{treatment}_i = 1\\)\n\\(\\Cred\\)\n\n\n\n\nThe only difference between the two worlds is the treatment. Thus, any difference in outcomes between the two worlds must be the causal effect of the treatment.\n\\[\n\\tau_i = \\T - \\Cred\n\\]\n\nTechnically, we need another assumption, SUTVA, for this to be true. I will explain this assumption as part of the identification assumptions.\n\nHowever, in reality, we do not have two parallel worlds. Thus, by definition, one of the potential outcomes is not observed in our real world - the counterfactual.\n\n\n\n\n\n\n\n\nIn the Real World\nObserved Outcome\nCounterfactual\n\n\n\\(i\\) receives treatment\n\\(\\Y= \\T\\)\n\\(\\Cred\\)\n\n\n\\(i\\) did not receive treatment\n\\(\\Y = \\Cred\\)\n\\(\\T\\)\n\n\n\n\nThe fundamental problem of causal inference is that in order to calculate our individual treatment effect \\(\\tau\\), we need both potential outcomes. Our goal is to estimate causal effects without observing counterfactuals. This is difficult at the individual level, so instead, we focus on average treatment effects for groups:\n\n\n\n\n\n\n\n\nGroup Effects\nNotation\nDefinition\n\n\nAverage Treatment Effect (ATE)\n\\(\\tau_\\text{ATE}\\)\nThe average treatment effects for all individuals in our study (treated and untreated).\n\n\nAverage Treatment Effect on the Treated (ATT)\n\\(\\tau_\\text{ATT}\\)\nThe average treatment effect but only for individuals who receive the treatment in our study.\n\n\nLocal Average Treatment Effect (LATE)\n\\(\\tau_\\text{LATE}\\)\nThe average treatment effect but only for a specific (local) group of individuals in a study.",
    "crumbs": [
      "Home",
      "Basics of Causality"
    ]
  },
  {
    "objectID": "did/ifect.html",
    "href": "did/ifect.html",
    "title": "Liu et al (2024) Interactive Fixed Effects Counterfactual Estimator",
    "section": "",
    "text": "# Packages Needed:\nlibrary(fect)\n\nTWFE compares groups that can be “forbidden” or incorrect, such as using already-treated units as controls. Liu et al (2024) instead proposes to estimate individual counterfactual untreated outcomes for treated units, and directly calculating the individual treatment effects:\n\\[\n\\tau_{it} = \\pT - \\pCred\n\\]\nThus, the goal of this interactive fixed effects counterfactual estimator (IFEct) is to estimate \\(\\pCred\\). They model these counterfactuals as:\n\\[\n\\pCred = \\underbrace{\\unit + \\time + \\cov}_{\\text{classic TWFE}} + \\b\\lambda_i'\\b\\xi_t + \\eps_{it}\n\\]\n\n\\(\\b\\xi_t\\) is a vector of latent variables/factors \\(\\xi_{1t}, \\xi_{2t}, \\dots\\). These latent variables change in magnitude depending on time \\(t\\) (they are time-varying). \\(\\b\\lambda_i\\) is a vector of factor loadings \\(\\lambda_{1i}, \\lambda_{2i}, \\dots\\), that describes the “influence” every latent factor has on unit \\(i\\). Different units \\(i\\) can have different relationships with each factor.\n\n\nset.seed(17) #for consistent replication\nmodel &lt;- fect(\n  formula = outcome ~ treatment,\n      #add covariates after treatment\n  data = df,\n  index = c(\"unit\",\"time\"),\n      #replace unit and time with your unit and time variables\n  method = \"ife\",\n      #do not change\n  se = TRUE,\n  nboots = 50) #for standard errors. more is more accurate, but takes longer\nprint(model)\n\nCall:\nfect.formula(formula = outcome ~ treatment, data = df, index = c(\"unit\", \n    \"time\"), method = \"ife\", se = TRUE, nboots = 50)\n\nATT:\n                            ATT    S.E. CI.lower CI.upper p.value\nTr obs equally weighted   2.281 0.07924    2.125    2.436       0\nTr units equally weighted 2.180 0.07979    2.024    2.337       0\n\n\nWe have two outputs - both are plausible ATT’s. It is generally recommended to use the first row - treated observations equally weighted, as it is typically the most accurate ATT estimate.\nWe can also plot our results for dynamic treatment effects and parallel trends tests:\n\nplot(model)\n\n\n\n\n\n\n\n\n\nSee Liu et al (2024) for more technical details.",
    "crumbs": [
      "Difference-in-Differences",
      "IFE Counterfactual Estimator"
    ]
  },
  {
    "objectID": "did/did2s.html",
    "href": "did/did2s.html",
    "title": "Gardner (2021) Two-Stage Difference-in-Differences",
    "section": "",
    "text": "# Packages Needed:\nlibrary(did2s)\n\nThe TWFE estimator implies that untreated units \\(D_{it} = 0\\) equals the following:\n\\[\nY_{it} (D_{it} = 0) = \\unit + \\time + \\cov + \\eps_{it}\n\\]\n\nThis equation is obtained from plugging in \\(D_{it} = 0\\) into the two-way fixed effects equation.\n\nThis means parameters \\(\\unit\\), \\(\\time\\), and \\(\\beta\\) should be estimated with only units \\(D_{it} = 0\\). However, in two-way fixed effects, we estimate \\(\\unit\\), \\(\\time\\), and \\(\\beta\\) with both units \\(D_{it} = 0\\) and \\(D_{it} = 1\\). So, we are actually improperly estimating \\(Y_{it}(D_{it} = 0)\\) in two-way fixed effects.\n\nThis is another fancy way of explaining the forbidden comparisons problem - in TWFE, \\(Y_{it}(D_{it} = 0)\\) is mis-estimated to include already-treated units, which is the forbidden comparison.\n\nGardner (2021) proposes to solve this by spliting the TWFE estimator into two parts.\nIn the first-stage, we estimate \\(\\unit\\), \\(\\time\\), and \\(\\beta\\) in a regression with only untreated units \\(D_{it} = 0\\). This will ensure the proper estimation of \\(Y_{it}(D_{it} = 0)\\). Then, using our first-stage estimates of \\(\\widehat\\unit\\), \\(\\widehat\\time\\), and \\(\\widehat\\beta\\), in the second stage regression, estimate \\(\\att\\).\n\nmodel &lt;- did2s(\n  data = df,\n  yname = \"outcome\",\n  first_stage = ~ covariate | unit + time,\n      # if no covariate, should be ~ 0 | unit + time\n  second_stage = ~ i(treatment), #replace D with your treatment\n  treatment = \"treatment\",\n  cluster_var = \"unit\")\nmodel\n\nOLS estimation, Dep. Var.: outcome\nObservations: 1,080\nStandard-errors: Custom \n                  Estimate   Std. Error  t value  Pr(&gt;|t|)    \ntreatment::0 -3.700000e-16 3.320000e-17 -11.1361 &lt; 2.2e-16 ***\ntreatment::1  4.848501e+00 4.512049e-01  10.7457 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 4.05175   Adj. R2: 0.212239\n\n\n\nOur ATT is the estimate for treatment::1, which in this case is 4.845.\n\nFor estimation of dynamic treatment effects, we do the following:\n\nmodel &lt;- did2s(\n  data = df,\n  yname = \"outcome\",\n  first_stage = ~ covariate | unit + time,\n      # if no covariate, should be ~ 0 | unit + time\n  second_stage = ~ i(rel.time, ref = c(-1, 10000)),\n      # the first number in ref= should always be -1\n      # the second number in ref= should be the value of rel.time for the never-treated\n  treatment = \"treatment\",\n  cluster_var = \"unit\")\n\niplot(\n  model,\n  drop = \"[[:digit:]]{2}\",\n      # this drops the time periods too far before and after treatment.\n  ref.line = 0)\n\n\n\n\n\n\n\n\n\nSee Kyle Butts’ github page for more info on this estimator. Asjad Naqvi also has a helpful page.",
    "crumbs": [
      "Difference-in-Differences",
      "2-stage Difference-in-Differences"
    ]
  },
  {
    "objectID": "did/twfe.html",
    "href": "did/twfe.html",
    "title": "Two-Way Fixed Effects Estimator",
    "section": "",
    "text": "# Packages Needed:\nlibrary(fixest)\n\nThe two-way fixed effects estimator is the standard estimator for DiD. The estimator with is a linear regression model specified as following:\n\\[\nY_{it} = \\underbrace{\\unit + \\time}_{\\text{fixed effects}} + D_{it}\\att + \\cov + \\eps_{it}\n\\]\n\nThis is the estimator for panel data. In repeated cross-sections, we have to replace the unit fixed effects \\(\\alpha_i\\) with group fixed effects, group being the level on which treatment is assigned (ex. if treatment is assigned at a state level, we use state fixed effects).\n\nThe fixed effects \\(\\alpha_i\\) and \\(\\gamma_t\\) are “special” intercepts. Unlike our normal intercepts in regression, these intercepts take different values depending on the unit and time.\n\nIntercept \\(\\alpha_i\\) has a different value for each unit \\(i\\).\nIntercept \\(\\gamma_t\\) has a different value for each time period \\(t\\).\n\n\nFor repeated cross section, \\(\\gamma_t\\) is the same. \\(\\alpha_g\\) will now have different values for each group \\(g\\).\n\n\nmodel &lt;- feols(\n  outcome ~ treatment + covariate | unit + time,\n      # the | separates the main variables from the fixed effects\n  data = df,\n  se = \"cluster\")\n      # we should always cluster standard errors by unit\nprint(model)\n\nOLS estimation, Dep. Var.: outcome\nObservations: 1,080\nFixed-effects: unit: 108,  time: 10\nStandard-errors: Clustered (unit) \n          Estimate Std. Error t value  Pr(&gt;|t|)    \ntreatment 4.850645   0.455222 10.6556 &lt; 2.2e-16 ***\ncovariate 0.972166   0.045972 21.1468 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 3.90173     Adj. R2: 0.474684\n                Within R2: 0.368706\n\n\n\nThe estimate for our treatment is the ATT. In this example, it is 4.850645.\n\nSince the differences between units \\(i\\) and time periods \\(t\\) are included in these fixed effects intercept terms, this allows us to account for differences between units and differences between time periods. Along with the parallel trends assumption, our treatment \\(D\\) should be strictly exogenous, allowing for the estimated \\(\\att\\) to be unbiased.\n\nThe unit fixed effects controls for between-unit differences. The time fixed effects controls for between-time differences. The only remaining differences are unit-time differences, or trends of units over time. Parallel trends (if met) controls for this.",
    "crumbs": [
      "Difference-in-Differences",
      "Two-way Fixed Effects"
    ]
  },
  {
    "objectID": "did/staggered.html",
    "href": "did/staggered.html",
    "title": "Stagged DiD and Issues with TWFE",
    "section": "",
    "text": "In the classical difference-in-differences design, all units in the treatment group receive treatment at the same time. However, in reality, this is not that common. It is very common that some units receive treatment before others.\n\nFor example, in the US, states often adopt different policies at different times - such as the staggered adoption of mail-in voting, or the staggered adoption of the common core curriculum.\n\nThe staggered difference-in-differences design is an extension on the classical design, which allows for staggered implementation of treatment.\nThe two-way fixed effects estimator is operates differently in a staggered setting. Let’s say we have 3 groups - a group treated early, a group that was treated late, and a untreated group.\n\n\n\n\n\nTwo-way fixed effects essentially estimates 4 different differences, shown below:\n\n\n\n\n\nThe actual TWFE estimate is a weighted average of these differences. The weights are a function of mainly the telative timing of treatment - groups treated more in the middle (so not too early or too late) are weighted higher. The weights are seen below in the figure.\n\n\n\n\n\nThere are two problems with these comparison conducted by TWFE:\n\nSome of these comparison (see comparison D above) may be “forbidden” - such that already-treated units are used as controls after they are treated.\nWeighting can result in weird weights - sometimes some groups treated early/late might receive negative weights (see figure above).\n\n\nThis site shows how you can explore the comparison and weighting issue in R. See Goodman-Bacon (2021) for a more technical overview of TWFE decomposition.\n\nThese problems with TWFE mean that TWFE is biased in estimating the ATT in staggered difference-in-differences when there is heterogenous treatment effects.\n\nTWFE is unbiased with homogeneity, but homogeneity is quite rare in scenarios we typically research.",
    "crumbs": [
      "Difference-in-Differences",
      "Staggered DiD and TWFE"
    ]
  },
  {
    "objectID": "did/did.html",
    "href": "did/did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "In April 1992, New Jersey increased its minimum wage. We might wonder how that caused changes in employment We have data from before and after the change:\n\n\n\n\n\n\n\nMar 1992 (Before Min. Wage Increase)\nDec 1992 (After Min. Wage Increase)\n\n\n\\(\\red{20.44}\\)\n\\(\\green{21.03}\\)\n\n\n\n\nRed indicates not treated, green indicates treated.\n\nOur treatment effect after the raise of minimum wage in December 1992 would be:\n\\[\n\\tau_\\text{Dec92} = \\green{Y_\\text{Dec92}^{(1)}} - \\red{Y_\\text{Dec92}^{(0)}}\n\\]\nWe don’t observe the counterfactual in December 1992 without the minimum wage increase.\n\nWe cannot use March 1992 because there could have been something that happened between March and December that changes the trend in employment (like a recession).\n\nDifference-in-differences estimates the hypothetical by looking at another state that did not receive a change in minimum wage during this time - like Pennsylvania:\n\n\n\n\n\n\n\nMar 1992 (Pennsylvania, No Min. Wage Increase)\nDec 1992 (Pennsylvania, No Min. Wage Increase)\n\n\n\\(\\red{23.33}\\)\n\\(\\red{21.17}\\)\n\n\n\n\nThese are employment values in Pennsylvania, where no mininum wage changes occured.\n\nPennsylvania tells us the trend in employment without a change in minimum wage (-2.16). This trend tells us that if New Jersey hadn’t received a minimum wage increase, their employment rate would be the March 1992 level minus 2.16.\n\n\n\n\n\n\n\nMar 92 (NJ, Not Minimum Wage Increase)\nDec 92 (NJ, Hypothetical No Minimum Wage Increase)\n\n\n\\(\\red{20.44}\\)\n\\(\\red{\\text{March 92} - 2.16}\\)\n\n\n\n\nThe March 1992 value is copied from the first table.\n\nNow, we have an approximation of New Jersey’s missing hypothetical outcome, so we can now estimate the causal effect.\n\\[\n\\begin{align}\n\\tau_\\text{Dec92} & = \\green{Y_\\text{Dec92}^{(1)}} - \\red{Y_\\text{Dec92}^{(0)}} \\\\\n& = \\green{Y_\\text{Dec92}^{(1)}} - \\left(\\red{Y_\\text{Mar92}^{(0)}} - 2.16 \\right)\n\\end{align}\n\\]\n\nThis entire idea of using another group’s trend to estimate counterfactuals depends on two assumptions:\n\n\n\n\n\n\n\nAssumption\nDescription\n\n\nParallel Trends\nThis means that had the treated group not received treatment, they would have followed the same trend as the control group.\n\n\nStable Unit Treatment Value Assumption (SUTVA)\nThis means that if Ava is treated, that does not affect Mia’s outcome (and for any other 2 individuals).\n\n\n\n\nI will show how to test for parallel trends later in Dynamic treatment effects.\n\nThere are two types of difference-in-differences designs - classical and staggered, which we will cover in the next few pages.",
    "crumbs": [
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did/etwfe.html",
    "href": "did/etwfe.html",
    "title": "Wooldridge (2021) Extended Two-Way Fixed Effects",
    "section": "",
    "text": "# Packages Needed\nlibrary(etwfe)\n\nThere are two issues with the two-way fixed effects estimator: forbidden comparisons and issues with weighting. Wooldridge (2021, 2023) proposes the extended two-way fixed effects (ETWFE) estimator.\nThe ETWFE estimator estimates all possible heterogeneous effects separately (by including a lot of interactions with initial treatment year, year, and covariates), using only valid comparisons, solving the forbidden comparison problem:\n\nmodel &lt;- etwfe(\n  fml = outcome ~ covariate,\n  tvar = time,\n  gvar = first.treated,\n  data = df,\n  vcov = ~unit) \n\nMany coefficients \\(\\blue{\\tau_{it}}\\) and \\(\\blue{\\tau_{itx}}\\) are estimated that capture different heterogeneity in treatment effects. These coefficients are manually aggregated together into one ATT using proper weighting based on how frequent each heterogeneous treatment effect occurs in our sample. This solves the weighting problem of TWFE.\n\nemfx(model)\n\n\n .Dtreat Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n    TRUE    -1.21      0.148 -8.23   &lt;0.001 52.2  -1.5 -0.925\n\nTerm: .Dtreat\nType:  response \nComparison: TRUE - FALSE\n\n\n\nWe can see our estimated ATT is -1.21.\n\nWe can also aggregate these coefficients into dynamic treatment effects. However, ETWFE only supports post-treatment dynamic treatment effects, not pre-treatment effects.\n\ndynamic &lt;- emfx(model, type = \"event\")\nplot(dynamic)\n\n\n\n\n\n\n\n\n\nSince ETWFE does not support pre-treatment effects, we cannot use it to test for parallel trends.\n\nWooldridge’s 2023 paper also extends this framework to work with logistic regression, poisson regression, and negative binomial regression, which is useful when we deal with non-continuous outcome variables. This can be accessed by adding an argument to the etwfe() function:\n\nfamily = c(\"logit\", \"poisson\", \"negbin\")\n\n\nOnly choose one of them - you cannot choose multiple families together.\n\n\nFor technical details, see Wooldridge (2021) and Wooldridge (2023).",
    "crumbs": [
      "Difference-in-Differences",
      "Extended Two-way Fixed Effects"
    ]
  },
  {
    "objectID": "did/cs.html",
    "href": "did/cs.html",
    "title": "Callaway and Sant’Anna Flexible Matching and Reweighting Estimator",
    "section": "",
    "text": "# Packages Needed\nlibrary(did)\n\nCallaway and Sant’Anna (2021) proposes to solve the issues with two-way fixed effects by carefully selecting the comparisons, and properly weighting the comparisons:\nWe first divide all units \\(i\\) into groups, based on their first initial year of receiving treatment. Each unit who was first treated in year \\(g\\) is assigned to group \\(g\\).\nThen, we define a new causal estimand of interest: what they call the group-time ATT. This is essentially the dynamic treatment effect of group \\(g\\) at time \\(t\\). We estimate the group-time ATT.\n\nset.seed(344) # for replication\nmodel &lt;- att_gt(\n  yname = \"outcome\", tname = \"time\", idname = \"unit\",\n  gname = \"first.treated\",\n  xformla = ~ covariate,\n      # you can delete argument xformla if no covariates\n  control_group = c(\"nevertreated\"), # don't change\n  est_method = 'dr', base_period = 'universal', #don't change\n  allow_unbalanced_panel = T, #don't change\n  data = df)\n\nNow, we can weight each group-time ATT together based on how frequent each group \\(g\\) is. By properly weighting, we solve the weighting issue of TWFE:\n\natt &lt;- did::aggte(\n  model,\n  type = \"simple\",\n  na.rm = TRUE)\nsummary(att)\n\n\nCall:\ndid::aggte(MP = model, type = \"simple\", na.rm = TRUE)\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\n     ATT    Std. Error     [ 95%  Conf. Int.]  \n -1.1237        0.5207    -2.1444     -0.1031 *\n\n\n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n\n\n\nWe can see the ATT estimate is -1.1237.\n\nWe can also see ATT’s weighted to show dynamic treatment effects:\n\natt_dynamic &lt;- did::aggte(\n  model,\n  type = \"dynamic\",\n  na.rm = TRUE)\nggdid(att_dynamic)",
    "crumbs": [
      "Difference-in-Differences",
      "Callaway and Sant'Anna (2021)"
    ]
  },
  {
    "objectID": "did/sunab.html",
    "href": "did/sunab.html",
    "title": "Sun and Abraham (2021) Estimator",
    "section": "",
    "text": "# Packages Needed:\nlibrary(fixest)\n\nThere are two issues with the two-way fixed effects estimator: forbidden comparisons and issues with weighting. Sun and Abraham (2021) proposes a new estimator to solve this issues.\nThe estimator divides all units into groups based on their initial treatment implementation. These groups are encoded into a new categorical variable \\(G_i\\). Then, the estimator calculates dynamic ATT’s separately for group, using only valid comparisons. This is done by interacting a relative-time variable \\(\\widetilde T_{it}\\) with \\(G_i\\).\nThis model will produce many coefficients \\(\\blue{\\tau_{it}}\\) (in fact, one for each post-treatment time period for each initial treatment year group). These are manually aggregated together into dynamic ATT’s by relative treatment time using proper weighting. This solves the weighting problem with TWFE.\n\nmodel &lt;- feols(\n  outcome ~ sunab(first.treated, time, att = T) + covariate | unit + time,\n      #first.treated for never-treated should be some value outside of time-range\n      #first.treated should not be NA.\n  data = df,\n  vcov = ~unit)\n\ntail(model$coeftable) # the ATT is listed at the end of the coef table\n\n                        Estimate Std. Error      t value     Pr(&gt;|t|)\ntime::-2:cohort::7  -0.558631418 0.87199275 -0.640637687 5.233168e-01\ntime::-2:cohort::8   0.429591471 0.30527023  1.407249792 1.626514e-01\ntime::-2:cohort::9   1.201899494 0.81918562  1.467188223 1.456627e-01\ntime::-2:cohort::10 -0.002429495 0.68208698 -0.003561854 9.971656e-01\nATT                 -1.133749449 0.20507049 -5.528583991 2.882038e-07\ncovariate            0.994678311 0.01837813 54.122927933 1.189553e-72\n\n\n\nThe ATT is given by the estimate of the ATT in the table. Here, we can see the ATT is -1.1337.\n\nWe can also calculate dynamic treatment effects and plot them:\n\nmodel &lt;- feols(\n  outcome ~ sunab(first.treated, time) + covariate | unit + time,\n      #att = T makes it calculate a singular ATT\n  data = df,\n  vcov = ~unit)\n\niplot(\n  model,\n  drop = \"[[:digit:]]{2}\",\n      # this drops the time periods too far before and after treatment.\n  ref.line = 0)\n\n\n\n\n\n\n\n\n\nFor technical details, see Sun and Abraham (2021) or this github page.",
    "crumbs": [
      "Difference-in-Differences",
      "Sun and Abraham (2021)"
    ]
  },
  {
    "objectID": "did/dynamic.html",
    "href": "did/dynamic.html",
    "title": "Dynamic Treatment Effects",
    "section": "",
    "text": "# Packages Needed:\nlibrary(fixest)\nlibrary(tidyverse)\n\nDynamic Treatment Effects calculate treatment effects for each year before treatment, and each year after treatment, instead of just one ATT.\n\nThis assumes we have more than just one pre-treatment and post-treatment period in our data. This is also called an event study or leads-and-lags.\n\nWe first create a new categorical variable, called relative-time \\(R_{it}\\). It measures the number of periods a certain time period \\(t\\) is before/after the initial treatment adoption, with \\(R_{it} = 0\\) being the initial treatment year.\n\ndf &lt;- df %&gt;%\n  mutate(rel.time = time - first.treated)\n\n\nFor example, if treatment starts in 2003, an observation in 2002 would get a relative-time of -1, and an observation in 2004 would get a relative-time of 1.\n\nThen, we run the following altered two-way fixed effects model:\n\\[\nY_{it} = \\underbrace{\\unit + \\time}_{\\text{fixed effects}} + R_{it}\\blue{\\tau_{r}}+ \\cov + \\eps_{it}\n\\]\n\n\\(R_{it}\\) is a categorical variable, with \\(R_{it} = -1\\) set as the reference category (since this is the last pre-treatment period). If you have repeated cross-sections, use group-fixed effects instead of unit.\n\n\nmodel &lt;- feols(\n  outcome ~ i(rel.time, -1) + covariate | unit + time,\n  data = df,\n  se = \"cluster\")\n\nThis will estimate a coefficient \\(\\blue{\\tau_{r}}\\) for each relative time period before/after treatment implementation. These are the treatment effects for each relative-time period \\(r\\). We can plot these coefficients:\n\niplot(\n  model,\n  drop = \"[[:digit:]]{2}\",\n      # this drops the time periods too far before and after treatment.\n  ref.line = 0)\n\n\n\n\n\n\n\n\n\nNegative rel.time are pre-treatment periods. 0 and positive rel.time are post-treatment periods. The dots and confidence intervals represent the estimated treatment effect at that time period relative to the treatment.\n\nIn the post-treatment periods (after treatment has started), these \\(\\blue{\\tau_r}\\) represent the treatment effect by year. In the pre-treatment periods, we shouldn’t expect any significant causal effect \\(\\blue{\\tau_r}\\), since treatment hasn’t started. If we do see any statistically significant effect pre-treatment, this is evidence for a violation of the parallel trends assumption.\n\nWe can see in our example above, that parallel trends is likely violated, because there are significant coefficients in the pre-treatment periods.\n\n\nSignficant coefficients can be indentified as ones whose confidence intervals do not contain 0. Note: this is not a definitive test for parallel trends. Even if there is 0 significance in pre-treatment periods, parallel trends could still be violated. There is no way to be 100% sure.",
    "crumbs": [
      "Difference-in-Differences",
      "Dynamic Treatment Effects"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Basics of Causality",
    "section": "",
    "text": "This page covers how confounders cause pre-existing differences between treated and untreated (selection bias), meaning correlation is not causation.\n\nLet us look at this causal question:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nGoing to the Hospital (D)\n\n\n\nY\n\nHealth Outcomes (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nWe have a group that went to hospital (we will call this group “went”), and a group that “did not go”. Using our potential outcomes framework, we can define the treatment effect of the group that went to the hospital:\n\\[\n\\tau_\\text{went} = \\textcolor{green}{{\\text{health}}_{\\text{went}}^{(1)}} - \\textcolor{red}{{\\text{health}}_{\\text{went}}^{(0)}}\n\\]\n\nIn red is the counterfactual we do not observe. This is because the individuals who went to the hospital were treated, so we cannot see the world where they are in control.\n\nNow compare the treatment effects above to correlation, which is defined as the difference in observed outcomes:\n\\[\n\\begin{align}\n\\text{correlation} & =  \\text{health}_\\text{went} - \\text{health}_\\text{did not go} \\\\\n& = \\textcolor{green}{\\text{health}_{\\text{went}}^{(1)}} - \\textcolor{red}{\\text{health}_\\text{did not go}^{(0)}}\n\\end{align}\n\\]\nIf we compare this correlation to our \\(\\tau_\\text{treated}\\), we see:\n\\[\n\\text{if  } \\textcolor{red}{\\text{health}_\\text{did not go}^{(0)}} ≠ \\textcolor{red}{\\text{health}_\\text{went}^{(0)}}\n\\]\nIf this is true (they are different), then correlation is not causation.\n\nThese two quantities are potential outcomes under control, or in another way to think of it, outcomes of the two groups prior to treatment happening.\n\nThus, if there is a difference between the average health outcomes between those who went to the hospital, and those who did not go to the hospital, before treatment is administered, then correlation is not equal to causation. This is because we cannot tell if the difference between the groups is due to treatment, or due to their pre-existing differences.\n\nWhat causes pre-existing differences? Confounders. For example, in our hospital-health example, a confounder could be smoking.\n\nSmoking is not the only possible confounder, we just use it as an example. Drinking, age, etc. are all other potential confounders.\n\nSmoking will worsen health outcomes. Someone who smokes is also more likely to visit the hospital with health complications. That means people who go to the hospital start out with (on average) worse health outcomes than people who did not go to the hospital.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nGoing to the Hospital (D)\n\n\n\nY\n\nHealth Outcomes (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmoking (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA confounder is a third variable that has the following characteristics:\n\nThe confounder is correlated (positive or negative) with the outcome variable.\nThe confounder causes who gets and doesn’t get the treatment.\nThe confounder is not itself caused by the treatment\n\n\n\n\n\nNote requirement 3 - it is a common mistake. Any result of the treatment \\(D\\) cannot be a confounder.\n\nConfounders cause pre-existing differences, which cause correlation to not equal causation. We must account for confounders to uncover causal effects.",
    "crumbs": [
      "Home",
      "Issue of Selection Bias"
    ]
  },
  {
    "objectID": "soo/distance.html",
    "href": "soo/distance.html",
    "title": "Distance Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red), since Mia receives the treatment.. However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Matched Individual}}\n\\]\nDistance matching matches an individual that is treated (like Mia) with one that is not treated based on how close their confounding values are. We define closeness by Mahalanobis distance:\n\\[\n\\text{distance}_{i, j} = \\sqrt{(\\b x_i - \\b x_j)' \\ \\b\\Sigma_x^{-1} (\\b x_i - \\b x_j)}\n\\]\n\nWhere \\(i\\) and \\(j\\) are two units we want to measure the distance between, \\(\\b x\\) is a vector of confounder values, and \\(\\b\\Sigma_x\\) is the covariance matrix of confounders.\n\nBecause distance matching depends on finding matches in a n-dimensional space, it is subject to the curse of dimensionality. This essentially means that the more confounders you have, the more dimensions you have to match over, and the harder it is to find good matches. So we typically do not use any more than 3-5 confounders with distance matching.\n\nBad matches means incorrectly using someone’s counterfactual, resulting in bad estimates.\n\n\nBefore you implement distance matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the Matching package.\n\nlibrary(Matching)\n\nNow, we can implement the matching as follows.\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,c(\"X1\",\"X2\", \"X3\")],\n             M = 1,\n             BiasAdjust = TRUE,\n             Weight = 2)\nsummary(att)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Distance Matching"
    ]
  },
  {
    "objectID": "soo/soo.html",
    "href": "soo/soo.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "Our issue in causal inference is that a confounder is causing pre-existing differences:\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nReceiving Scholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nIntellegence (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nBy definition, as the confounder changes, your likelihood of getting treatment changes. As the confounder changes, the outcome value will also change.\nThese issues occur when the confounder changes in value. So what if we hold the confounders constant? Then, there would be no changes in confounders - so the variation in treatment assignment and outcomes cannot be attributed to the confounder.\nFor example, let’s assume that intelligence has two values: smart and dumb. Let us calculate the treatment effect on university grades within each level of intelligence:\n\\[\n\\begin{align}\n\\tau_\\text{smart} & = \\green{Y_\\text{smart}^{(1)}} - \\red{Y_\\text{smart}^{(0)}}\\\\\n\\tau_\\text{dumb} & = \\green{Y_\\text{dumb}^{(1)}} - \\red{Y_\\text{dumb}^{(0)}}\n\\end{align}\n\\]\nThe confounder is constant here, so no selection bias. Thus, within each category, correlation is equal to causation. Our overall causal effect will be a weighted average of the categories:\n\\[\n\\tau = \\tau_\\text{smart} Pr(\\text{smart})  \\ + \\ \\tau_\\text{dumb} Pr(\\text{dumb})\n\\]\n\nThe weights of this weighted average are the probability/frequency of that value of the confounder.\n\nObviously, most confounders have more than 2 categories, and we often have more confounders. But the same intuition applies.\n\\[\n\\tau = \\sum_X \\tau_\\text{X} \\cdot Pr(X)\n\\]\n\nFor selection on observables to work, we need to meet 3 assumptions:\n\n\n\n\n\n\n\nAssumption\nDescription\n\n\nConditional Ignorability\nThis means that we must account for all possible confounders (cannot miss a single one).\n\n\nCommon Support\nThis means no one can have a 100% chance of being in treatment or control. They always have a chance to be in either, no matter their confounding values.\n\n\nStable Unit Treatment Value Assumption (SUTVA)\nThis means that if Ava is treated, that does not affect Mia’s outcome (and for any other 2 individuals).\n\n\n\n\nWe have a wide choice of estimators that we can use. Use the sidebar or links in the table to access each estimator’s page.",
    "crumbs": [
      "Selection on Observables"
    ]
  },
  {
    "objectID": "soo/pscore.html",
    "href": "soo/pscore.html",
    "title": "Propensity Score Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red), since Mia receives the treatment.. However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Matched Individual}}\n\\]\nPropensity Score Matching matches an individual that is treated (like Mia) with one that is not treated based on how similar their likelihoods of treatment are.\nWhat is a likelihood of treatment? Well we know confounders cause people to get the treatment or not treatment. Thus, using an individual’s confounder values, we can estimate their likelihood of getting treatment, called a propensity score.\n\\[\n\\text{propensity score } \\pi =Pr(\\green{D_i = 1})\n\\]\nPropensity scores are typically estimated with a logistic regression. This also means that propensity score matching shares the same weaknesses of logistic regression - including assuming linear relatinoships between confounders and propensities, and only being unbiased in large sample sizes.\n\nBefore you implement propensity score matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the Matching package.\n\nlibrary(Matching)\n\nFirst, we need to estimate the propensity scores with a logistic regression:\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity,\n                          type = \"response\")\n\n\nA random forest model is also possible, but less common.\n\nNow, we can implement the matching as follows.\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,\"pscore\"],\n             M = 1,\n             BiasAdjust = TRUE,\n             Weight = 2)\nsummary(att)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Propensity Score Matching"
    ]
  },
  {
    "objectID": "soo/genetic.html",
    "href": "soo/genetic.html",
    "title": "Genetic Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red), since Mia receives the treatment.. However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Matched Individual}}\n\\]\nLike distance matching, genetic matching matches an individual that is treated (like Mia) with one that is not treated based on how close their confounding values are. However, genetic matching uses a slightly different variation of mahalanobis distance:\n\\[\n\\text{distance}_{i, j}(\\b W) = \\sqrt{(\\b x_i - \\b x_j)' \\ (\\b\\Sigma_x^{-\\frac{1}{2}})' \\ \\b W \\ \\b\\Sigma_x^{-\\frac{1}{2}}  (\\b x_i - \\b x_j)}\n\\]\n\nWhere \\(i\\) and \\(j\\) are two units we want to measure the distance between, \\(\\b x\\) are their confounder values, and \\(\\b\\Sigma_x\\) is the covariance matrix of confounders. \\(\\b W\\) is a weights matrix.\n\nThe weights \\(\\b W\\) are estimated to make the treated and untreated groups as similar as possible. This balance between treated and untreated eliminates selection bias. Then, matching is done with the units that have the smallest distance.\n\nBefore you start genetic matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the Matching and MatchIt package.\n\nlibrary(Matching)\n\nFirst, we need to estimate the propensity scores with a logistic regression.\n\nIt is recommended to use the propensity score as one of the controls on which to genetic match on.\n\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity,\n                          type = \"response\")\n\nThen, we use the GenMatch() function to estimate a weights matrix \\(\\b W\\):\n\nset.seed(333) #any number works\ngen &lt;- GenMatch(Tr = my_data$D,\n                    X = my_data[,c(\"X1\",\"X2\",\"pscore\")],\n                    BalanceMatrix = my_data[,c(\"X1\",\"X2\")],   \n                    estimand = \"ATT\",\n                    M = 2,\n                    replace = TRUE,\n                    ties = FALSE,\n                    distance.tolerance = 0,\n                    print.level = 0,\n                    pop.size = 200)\n\n\nYou can increase pop.size to increase the accuracy - but it will increase the time and computational power needed.\n\nNow, let us conduct estimation with genetic matching:\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,c(\"X1\",\"X2\",\"pscore\")],\n             estimand = \"ATT\",\n             M = 2,\n             replace = TRUE,\n             ties = FALSE,\n             distance.tolerance = 0,\n             Weight.matrix = gen$Weight.matrix,\n             Weight = 3)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Genetic Matching"
    ]
  },
  {
    "objectID": "latent/sem.html",
    "href": "latent/sem.html",
    "title": "Structural Equation Models",
    "section": "",
    "text": "Structural Equation Models (SEMs) allow us to combine measurement models (ex. factor analysis) with linear regression models between the latent variables. We split our latent variables into latent explanatory variables \\(\\xi\\) and latent outcome variables \\(\\eta\\).\n\n\n\n\n\n\n\n\nexample2\n\n\nF1\n\nξ (Unobserved)\n\n\n\nX1\n\nX1\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3\n\n\n\nF1-&gt;X3\n\n\nλ\n\n\n\nF2\n\nη (Unobserved)\n\n\n\nF1-&gt;F2\n\n\nLinear Model\n\n\n\nX4\n\nY1\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\nX5\n\nY2\n\n\n\nF2-&gt;X5\n\n\nλ\n\n\n\nX6\n\nY3\n\n\n\nF2-&gt;X6\n\n\nλ\n\n\n\n\n\n\n\n\n\nThis structural equation model contains a linear regression between the latent input variable explaining the latent outcome variable.\n\\[\n\\eta = \\beta_0 + \\beta_1\\ \\xi + \\eps\n\\]\nThe latent outcome variable and input variable are unobserved, and we only observe items. We can use these observed items to create a factor analysis model for both the outcome variable, and the input variable.\nThe interpretations of the model are quite straight forward - the measurment models are interpreted in the same way as factor analysis, and the linear model between latent variable is interpreted the same way as a linear model between any other variables.\n\nEx: the linear model says that as explanatory increases by 1, response increases by an expected \\(\\beta\\) units.\n\nStructural models don’t have to have just one dependent and independent latent variable. We can have multiple dependent and independent variables, multiple linear models, and also measure correlations within the dependent and independent variables:\n\n\n\n\n\n\n\n\nexample2\n\n\nX1\n\nξ 1\n\n\n\nX2\n\nξ 2\n\n\n\nX1-&gt;X2\n\n\n\nCorrelation\n\n\n\nY1\n\nη 2\n\n\n\nX1-&gt;Y1\n\n\nβ1\n\n\n\nY2\n\nη 2\n\n\n\nX1-&gt;Y2\n\n\nγ1\n\n\n\nX2-&gt;Y1\n\n\nβ2\n\n\n\nX2-&gt;Y2\n\n\nγ2\n\n\n\nY1-&gt;Y2\n\n\n\nCorrelation\n\n\n\n\n\n\n\n\n\n\nNote: we cannot have linear models between two independent, or two dependent variables. They have to be seperate - within each group, only correlations are possible.\n\nIn this example, we are measuring the correlation between the independent variables, the correlation between the dependent variables, and we have two regression models:\n\\[\n\\begin{align}\n\\eta_1 & = \\beta_0 + \\beta_1\\ \\xi_1 + \\beta_2\\ \\xi_2 + \\eps_1 \\\\\n\\eta_2 & = \\gamma_0 + \\gamma_1\\ \\xi_1 + \\gamma_2\\ \\xi_2 + \\eps_2 \\\\\n\\end{align}\n\\]\n\nAnd each response and latent variable all have their own measurement models.\n\n\nTo implement structural equation models, we use the lavaan package:\n\nlibrary(lavaan)\n\nThen, we have to specify the relationships between variables we want to fit in our structural model - including measurement models, regression models, and correlations:\n\nmodel &lt;- '\n# Measurement models \n  input1 =~ NA*item1 + item2 + item3\n  input2 =~ NA*item4 + item5 + item6\n  outcome =~ item1 + item2 + item3\n  \n# Regressions\n  outcome ~ input1 + input2\n\n# Covariances\n  input1 ~~ input2\n\n# Fixing the variances of independent variables at 1\n  input1~~1*input1\n  input2~~1*input2\n'\n\n\nNote how outcome does not have NA*, and outcome doesn’t have its variance fixed at 1 at the end. Meanwhile, input1 and input2 do. This is standard to fix outcome variables with a variance of 1, and input variables are allowed to be “free”.\n\nNow, we estimate our specified model:\n\nsem &lt;- sem(model,\n           data = my_data,\n           missing=\"FIML\")\nsummary(sem)\n\n\nThe missing argument tells the software to keep observations with missing values (this is typically a good thing). But it can take longer, so you can delete this argument for it to use only complete observations.",
    "crumbs": [
      "Latent Variable Models",
      "Structural Equation Models"
    ]
  },
  {
    "objectID": "latent/multiplefac.html",
    "href": "latent/multiplefac.html",
    "title": "Multiple Latent Factors",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nFactor analysis can work with multiple latent factors \\(\\xi_1, \\xi_2, \\dots\\).\n\n\n\n\n\n\n\n\nexample2\n\n\nF1\n\nξ 1\n\n\n\nF2\n\nξ 1\n\n\n\nF1-&gt;F2\n\n\n\nCorrelation\n\n\n\nX1\n\nX1\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3\n\n\n\nF1-&gt;X3\n\n\nλ\n\n\n\nF2-&gt;X2\n\n\nλ\n\n\n\nF2-&gt;X3\n\n\nλ\n\n\n\nX4\n\nX4\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\n\n\n\n\n\n\nEach item can measure either only some or all of the factors. The factors can also both have the same items, but put emphasis on different items. The factors can also be correlated.\n\nWe can also have much more than just 2 factors. However, we typically need 3 items for the first factor, and 2 items for each additional factor, or else we won’t have enough degrees of freedom for estimation.\n\nJust like in single factor analysis, we use a linear regression to express the relationship between any item \\(i\\) and the factors. However, this time, each regression will relate each item to all factors:\n\\[\n\\begin{align}\nX_1 & = \\tau_1 + \\lambda_{1}^{(1)}\\ \\xi_1 + \\lambda_{1}^{(2)}\\ \\xi_2 + \\dots + \\delta_1 \\\\\nX_2 & = \\tau_2 + \\lambda_{2}^{(1)}\\ \\xi_1 + \\lambda_{2}^{(2)}\\ \\xi_2 + \\dots + \\delta_2 \\\\\n& \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nDetails: Assumptions of the Model\n\n\n\n\n\n\nWe still assume the factors are normally distributed:\n\\[\n\\sim \\mathcal N(\\b \\kappa, \\b\\Phi)\n\\]\nWhere \\(\\b\\kappa\\) is a vector of all the means of each factor, and \\(\\b\\Phi\\) is a variance-covariance matrix of all the factors.\nUsing our conventional identification assumption like in single factor analysis, we will assume each factor is a standard normal \\(\\mathcal N (0, 1)\\). This implies that \\(\\b\\kappa = 0\\).\nOur variance matrix \\(\\b\\Phi\\) is a little more complicated - the variances of each factor is 1 (as assumed in a standard normal), however, the matrix \\(\\b\\Phi\\) also includes the covariances between factors. This is an additional thing that we will need to estimate that was not present in one-factor models.\nFor example, factor 1 and 2 might be correlated with each other, which is reflected in \\(\\b\\Phi\\).\n\n\n\n\nThe factor loadings \\(\\lambda\\) are still the relationship between each factor and each item, and are interpreted in the same way as a 1-factor model. However, the the interpretation of communality and relaibility are no longer valid with more than 1-factor.\n\nWhen interpreting - do one factor at a time. Start with one, then go to the next. Interpreting each factor is the same as shown in the page on factor analysis.\n\nAs seen in the figure above, factors can also be correlated with each other. Our model estimation will also estimate the correlation between factors. Just like in single factor analysis, factor scores are also possible.\n\nTo implement multiple factor anlaysis, the procedure is quite similar to standard factor analysis. First, we will need the psych and GPArotation package:\n\nlibrary(psych)\nlibrary(GPArotation)\n\nFirst, we should get rid of missing observations:\n\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\nFor factor analysis with multiple factors, the notation is as follows:\n\nfa &lt;- fa(data[,items], nfactors=2, fm=\"ml\", rotate=\"oblimin\")\nprint(fa)\n\n\nChange nfactors=2 to however many factors you want. Note that you cannot have too many, this will be discussed in the next page on identification.\nThis code also uses the “oblimin” rotation, which will be discussed in the next page on identification and rotation.",
    "crumbs": [
      "Latent Variable Models",
      "Multiple Latent Factors"
    ]
  },
  {
    "objectID": "latent/latent.html",
    "href": "latent/latent.html",
    "title": "Latent Variable Models",
    "section": "",
    "text": "A latent variable model connects a unobserved variable (latent factor \\(\\xi\\)) with a few observed variables (items \\(X_1, X_2, \\dots\\)) that are considered imperfect measures of the latent factor.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\n\nFor example, maybe we want to measure the political ideology of a senator. We cannot directly observe the political ideology (the latent factor), but we can observe how they vote on different bills (the items)\n\nLatent variable models assume we can model the relationship between each observed item and the latent factor with some sort of regression model. The coefficient of each regression (often denoted \\(\\lambda\\)) is the relationship between each item and the factor.\n\nFor example, in the figure above, each item (X1, X2, X3) is related to the factor (F) with by a \\(\\lambda\\) coefficient.\n\nThese \\(\\lambda\\) are called factor loadings. We can interpret the estimated factor loadings \\(\\widehat\\lambda\\) to interpret what the unobserved factor actually is measuring.\n\nFor example, if a factor has a strong relationship with one item, and a weaker relationship with another item, we might conclude that the factor measures the first item more than the second item.\n\nWe can also use latent variable models to create factor scores \\(\\widehat\\xi\\), which are basically actual values of the latent variable for each individual \\(i\\) in our data. This allows us to use the latent variable in other statistical models.\nThe choice of latent variable model depends on the type of items/factors:\n\n\n\n\n\n\n\n\nModel\nFactor Type\nItem Type\n\n\nFactor Analysis\nContinuous\nContinuous\n\n\nItem Response Theory\nContinous\nCategorical/Binary\n\n\nStructural Class Models\nCategorical/Binary\nCategorical/Binary\n\n\n\n\nAll models allow for multiple factors \\(\\xi_1, \\xi_2, \\dots\\) as well. If we have many items, some items might only measure on factor, others both factors.\nEach latent model also has the ability to conduct confirmatory analysis. This is basically when we, based on theoretical reasons (such as reading the literature), impose certain restrictions on models, and test if this hypothesised model is good.\n\nThis typically takes the form of setting certain factor loadings \\(\\lambda\\) to 0, meaning a certain item does not measure one of the factors.\n\nWe can also combine different structural models together to form structural equation models (see the structural equation models part).",
    "crumbs": [
      "Latent Variable Models"
    ]
  },
  {
    "objectID": "latent/cluster.html",
    "href": "latent/cluster.html",
    "title": "Cluster Analysis",
    "section": "",
    "text": "Cluster analysis is a method of finding groups/clusters of similar individuals in our dataset, based on a set of variables. We choose the number of clusters we want to divide our data into, and each individual will be assigned to a specific cluster.\n\n\n\n\n\n\nAbove is an example of a cluster anlaysis with 2 clusters. Latent Class Models can also be considered to be a form of cluster analysis.\n\nCluster anlaysis is often conducted by K-means clustering. This form of clustering focuses on the distance between different individuals in our dataset. The idea is that a good clustering will have as small within-cluster variation as possible.\n\nIn other words, you want individuals in the same cluster to be similar.\n\nThus, K-means clustering tries to minimise the squared euclidean distance between all the points within a cluster and the centroid (“center”) of the cluster:\nHow do we minimise this distance? The K-means clustering algorithm works like this:\n\nChoose how many total clusters we want.\nRandomly assign each unit to one of the clusters.\nCalculate the centroids of each cluster.\nRe-assign each individual to the cluster whose centroid is closest to that individual.\nKeep repeating steps 3 to 4 until all units are in the cluster whose centroid is closest to that individual.\nRepeat the whole process a few times to find the global optimum.\n\n\nK-means clustering is algorithmic, not model based (unlike the very similar Latent Class models). This means that K-means does not have model summary statistics like AIC, or significance tests.\n\nTo interpret the clusters, we have to use our own field expertise. We look at the individuals in each cluster, as well as their mean variable values, and try to assign meaning to them.\n\n\n\n\n\n\nOther Clustering Methods\n\n\n\n\n\nK-means is the most common, but not the only way to cluster. Other methods include:\n\nLatent Class Models, which we covered previously. They technically count as cluster analysis, since we are assinging individuals to a category of a latent variable. This is a model-based method.\nGaussian Mixture Modelling: this is a variation of latent class models that deals with continuous observed items, but categorical variables.\nHierarchical Cluster Anlaysis: This basically starts with each unit in its own cluster, before merging clusters that are similar. Then it repeats that continuously. We can choose to analyse any specific cluster set within this process.\n\n\n\n\n\nTo implement cluster anlaysis, we should first start by standardising our variables.\n\nvars &lt;- c(\"X1\", \"X2\", \"X3\") #names of variables to cluster by\n\n# Standardise function\nstandard &lt;- function(x){\n  (x-mean(x))/sd(x)\n}\n\n# apply standardisation\nmy_data.z &lt;- global[,c(\"Unit Names Variable\", vars)]\nmy_data.z[,vars] &lt;- sapply(my_data.z[,vars], standard)\ncolnames(my_data.z)[-1] &lt;- vars.z\ndta &lt;- merge(my_data, my_data.z, by=\"Unit Names Variable\")\n\nThen, we can implement cluster anlaysis:\n\nset.seed(1236) #set seed\n\nres &lt;- kmeans(dta[,varsz],\n                  4, #change to number of clusters\n                  nstart = 10) #number of times to run algorithm\nprint(res)\n\nWe can access each individual’s assigned cluster with the following:\n\nres$cluster\n\n\nYou can save this vector back into your original dataset for further anlaysis.",
    "crumbs": [
      "Latent Variable Models",
      "Cluster Analysis"
    ]
  },
  {
    "objectID": "latent/group.html",
    "href": "latent/group.html",
    "title": "Multiple Group Models",
    "section": "",
    "text": "We often deal with data that is clustered or grouped. For example, if we are using European data, we might have observations from different countries. If we are using USA data, we might have observations from different states.\nHowever, there might be differences between these clusters. We might want to make our model be able to account for these differences between clusters. One way to account for differences between groups is to essentially run different models for different groups:\n\\[\n\\begin{align}\n\\eta^{(g=1)} & = \\beta_0^{(g=1)} + \\beta_1^{(g=1)} \\xi^{(g= 1)} + \\eps \\\\\n\\eta^{(g=2)} & = \\beta_0^{(g=2)} + \\beta_1^{(g=2)} \\xi^{(g= 2)} + \\eps \\\\\n\\eta^{(g=3)} & = \\beta_0^{(g=3)} + \\beta_1^{(g=3)} \\xi^{(g= 3)} + \\eps \\\\\n\\end{align}\n\\]\n\nWhere \\(g\\) stands for group.\n\nThis essentially means for each group, they have their own \\(\\beta_0\\) and \\(\\beta_1\\) estimates. Since we know \\(\\beta_1\\) is the relationship between input and output, this essentially allows the relationship between input and output to vary by group.\n\nFor example, this would allow the relationship between education and income to vary by country.\n\nIf our outcome and input variables are measurement models, we can also play around with these in a few ways.\n\nWe could assume the same measurement models across groups.\nOr, just like above, we could assume each group has its own measurement model, with its own \\(\\lambda\\) factor loading varying by group.\n\nWe noted in factor analysis that we typically assume the (outcome and input) factor is a standard normal distribution with mean of 0 and variance of 1.\nIn clustered analysis, we can’t actually assume this. Why? Well, if we allow each group to have a different relationship between input and outcome, by definition, the outcome’s mean will differ by country. So instead, we fix one group (usually randomly) to be standard normal with mean of 0 and variance of 1, and allow the variances of all the other groups be freely estimated.\n\nTo implement structural equation models, we use the lavaan package:\n\nlibrary(lavaan)\n\nThen, we have to specify the relationships between variables we want to fit in our structural model - including measurement models, regression models, and correlations:\n\nmodel &lt;- '\n# Measurement models \n  input1 =~ NA*item1 + item2 + item3\n  input2 =~ NA*item4 + item5 + item6\n  outcome =~ item1 + item2 + item3\n  \n# Regressions\n  outcome ~ input1 + input2\n\n# Covariances\n  input1 ~~ input2\n\n# Fixing the variances of independent variables at 1\n  input1~~c(1,NA)*input1\n  input2~~c(1,NA)*input2\n  outcome~~c(1,NA)*outcome\n'\n\nNow, we estimate our specified model:\n\nsem &lt;- sem(model,\n           data = my_data,\n           group = \"groupvariable\",\n           group.equal=c(\"intercepts\",\"loadings\",\"residuals\"),\n           missing=\"FIML\")\nsummary(sem)\n\n\nThe missing argument tells the software to keep observations with missing values (this is typically a good thing). But it can take longer, so you can delete this argument for it to use only complete observations.",
    "crumbs": [
      "Latent Variable Models",
      "Multiple Group Models"
    ]
  },
  {
    "objectID": "latent/confirmatory.html",
    "href": "latent/confirmatory.html",
    "title": "Confirmatory Analysis",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nSometimes, we already have some ideas about our latent variables - some hypotheses on how different items are related to the latent variable. This is what we can confirmatory analysis, where we study how well a hypothesised model fits the data.\nThe basic approach of confirmatory analysis is to set some factor loadings \\(\\lambda\\) for certain variables to 0. This essentially means that based on our preconceived theories, establish that some items do not measure a certain factor.\n\nBy setting enough loadings to 0, we no longer have to worry about rotation issues discussed previously.\n\nFor example, perhaps theoretically we believe that items 1 and 2 only explain factor 1, and only items 3 and 4 explain latent factor 2. We can set the items we believe to not explain each factor to 0.\n\n\n\n\n\n\n\n\nexample2\n\n\nF1\n\nLatent Factor 1\n\n\n\nX1\n\nItem 1\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3\n\n\n\nF1-&gt;X3\n\n\n0\n\n\n\nF2\n\nLatent Factor 2\n\n\n\nF2-&gt;X2\n\n\n0\n\n\n\nF2-&gt;X3\n\n\nλ\n\n\n\nX4\n\nItem 4\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe estimation and interpretation of confirmatory analysis are essentially identical to that of exploratory analysis we have previously looked at.\n\n\n\n\n\n\nDetails: Identification Assumptions\n\n\n\n\n\n\nRecall that we previously assumed the latent factors are standardly normally distributed. In confirmatory analysis, we still assume the factor is normally distributed, but we allow for the mean and variance of the distribution to be estimated:\nInstead, we fix each factor (if we have multiple) to the scale of one of the items, with each factor fixed to a different item. Basically, this means that the factor will take the same scale/measurement characteristics as one of the items. This is done by fixing that item’s intercept \\(\\tau_i = 0\\), and the same item’s factor loading for that specific factor at \\(\\lambda = 1\\).\n\n\n\n\nThere is a special set of tests to see if the parameters we set equal to 0 actually make sense. Modification indicies are a sort of hypothesis test for this - larger values indicate parameters that if were not 0, could improve the fit of the model.\nThere are also expected parameter changes (EPC), which basically estimate what parameters set to 0 would actually be equal to, if they were added to the model.\n\nBelow in the R-code I provide more ways of choosing between confirmatory models and exploratory models (and really any factor models).\n\n\nFor confirmatory analysis, we will need the lavaan package:\n\nlibrary(lavaan)\n\nWe fit a confirmatory model in the following way:\n\n# specify formula\nformula &lt;- '\nf1 =~ X1 + X2 + 0*X3 \nf2 =~ 0*X2 + X3 + X4\n'\n\n# estimate model\nmodel &lt;- sem(formula,\n           data = my_data,\n           std.lv = TRUE,\n           missing = \"fiml\")\nsummary(model)\n\n\nWe put a 0* before any item for that factor we want to set equal to 0. You can also use this command to estimate exploratory models.\n\nIf we have theoretical reasons, we can also force different factor load gins to be equal by including the same labels before as follows:\n\n# specify formula\nformula &lt;- '\nf1 =~ c1*X1 + c1*X2 + X3 \nf2 =~ X2 + X3 + X4\n'\n\n\nSince label c1 appears before X1 and X2 for factor f1, that means their factor loadings will be forced to be equal. You can include more labels as well.\n\nTo create factor scores, we simply use the predict function:\n\npredict(model)\n\n\n\n\n\n\n\nMore on Choosing Between Models\n\n\n\n\n\nTo compare and choose between different models, we have a few ways.\n\nWe can use a likelihood ratio test for nested models (see the regression section for more details).\n\n\nlavTestLRT(model1, model2)\n\n\nWe can use a global goodness of fit test - essentially a likelihood ratio but with the full sample covariance matrix as the null model. We want to fail to reject the null, because we want our model to be as close to the sample covariance matrix as possible.\n\n\nlavTestLRT(model)\n\n\nWe can use AIC and BIC to compare models since factor models are estimated with MLE. These are included in the output.\n\nThere are also a series of fit indicies made for factor analysis. These are included in the output.\n\nRoot Mean Square Error of Approximation (RMSEA): 0 is a perfect fit, 0.05 or smaller is a good fit, and anything above 0.1 is a poor fit.\nStandard Root Mean Square Residual (SRMR): smaller values the better, anything below 0.08 is a good fit.\nTucker and Lewis Index (TLI): 1 is the best fit, anything below 0.9 is a poor fit, and anything above 1 may indicate overfitting.\nComparative Fit Index: values between 0 and 1, anything close to 1 is a good fit.",
    "crumbs": [
      "Latent Variable Models",
      "Confirmatory Analysis"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Additional Resources",
    "section": "",
    "text": "Here are a list of additional resources that are useful for causal inference and social science statistics.\n\nDr. Asjad Naqvi has a repository of the modern advancements going on in the recent difference-in-differences revolution.\nYiqing Xu also has a page on modern advancements in difference-in-differences.\nCausal Inference: The Mixtape as a great resource for details on causal inference methodology.\nCausal Inference for the Brave and True is another great resource of causal inference, with application in Python.\nFixest (an R package used for causal inference and regression) is a very useful thing to be familiar with.\nBen Lambert is a terrific resource for econometrics. He also has a lot of resources on general statistics, including factor analysis and bayesian statistics.\nGreat textbooks for causal inference: Angrist and Pischke have two books - Mostly Harmless Econometrics (more advanced) and Mastering Metrics (more simple). A very technical book is Imbens and Rubin’s Causal Inference for Statistics, Social, and Biomedical Sciences.\nGreat textbooks for more theoretical econometrics/statistics include Wooldridge’s Introductory Econometrics.",
    "crumbs": [
      "Home",
      "Additional Resources"
    ]
  }
]