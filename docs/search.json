[
  {
    "objectID": "randomisation.html",
    "href": "randomisation.html",
    "title": "The Magic of Randomisation",
    "section": "",
    "text": "This chapter covers how randomisation solves the problem of selection bias, and why randomisation is considered the “gold standard” of causal inference.\n\nLet us say we are interested in this question:\n\n\n\n\n\n\n\n\nexample1\n\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Performance (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nOur concern is a confounder. For example, smartness of an individual could mean they are more likely to get a scholarship. Since smart people tend to perform well at university, that means the people who get treated are different from those who don’t get treated.\n\n\n\n\n\n\n\n\nexample2\n\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Performance (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmartness is not the only confounder. Other confounders could be family income, athletic ability, etc.\n\nBut what if randomness (like flipping a coin) controls who gets the treatment or not. A coin will be flipped to decide if every person in our study will get the scholarship. This means that the randomness (the coin), and not the confounder, are causing selection into treatment:\n\n\n\n\n\n\n\n\nexample2\n\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Performance (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;Y\n\n\n\n\n\n\nO\n\nCoin\n\n\n\nO-&gt;D\n\n\n\n\n\n\n\n\n\n\n\nSince the confounder is no longer causing who gets the treatment and who doesn’t, that means there is no more concern of selection bias.\n\nRandomisation also means that every individual has the same chance of being treated or untreated, so the two groups will, on average, the same as each other. That means:\n\\[\n\\textcolor{purple}{\\mean Y_\\text{untreated}^{(0)}} = \\textcolor{red}{\\mean Y_\\text{treated}^{(0)}}, \\text{ so correlation} = \\text{causation}\n\\]\n\nThis is established by the law of large numbers, but it is a little technical for here.\n\nSo if our treatment is randomly assigned (individuals randomly assigned to treatment or control), correlation does equal causation.\n\nRandomisation is the gold standard of causal inference. There is no better method.\n\nRandomisation is possible if you are running your own experiment: you can use a random number generator to assign treatment.\nRandomisation is also possible if there is something that is being randomly assigned in the real world. For example, the US green card lottery randomly chooses who gets accepted.\n\nHowever, randomisation is not always possible to due to cost of running experiments, non-compliance of individuals within experiments, and impracticality.\n\nNon-compliance is an issue that can be solved pretty easily with an instrumental variable, given a few assumptions about the non-compliance.",
    "crumbs": [
      "Home",
      "Magic of Randomisation"
    ]
  },
  {
    "objectID": "latent/confirmatory.html",
    "href": "latent/confirmatory.html",
    "title": "Confirmatory Analysis",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nIn the past few chapters, we have focused on exploratory analysis, essentially allowing the data to “speak for itself” and estimate all of the model’s parameters to measure the latent variable.\nHowever, what if we already have some ideas about our latent variables - some hypotheses on how different items are related to the latent variable. This is what we can confirmatory analysis, where we study how well a hypothesised model fits the data.\nThe basic approach of confirmatory analysis is to set some factor loadings \\(\\lambda\\) for certain variables to 0. This essentially means that based on our preconceived theories, establish that some items do not measure a certain factor.\n\nBy setting enough loadings to 0, we no longer have to worry about rotation issues discussed previously.\n\nFor example, perhaps theoretically we believe that items 1 and 2 only explain factor 1, and only items 3 and 4 explain latent factor 2. We can set the items we believe to not explain each factor to 0.\n\n\n\n\n\n\n\n\nexample2\n\n\n\nF1\n\nLatent Factor (F1)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF1-&gt;X3\n\n\n0\n\n\n\nF2\n\nLatent Factor (F2)\n\n\n\nF2-&gt;X2\n\n\n0\n\n\n\nF2-&gt;X3\n\n\nλ\n\n\n\nX4\n\nItem 4 (X4)\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\n\n\n\n\n\n\nAnother difference is in the assumptions we make on the actual latent variables. Recall that we previously assumed the latent factors are standardly normally distributed:\n\\[\nF \\sim \\mathcal N(0, 1)\n\\]\nIn confirmatory analysis, we still assume the factor is normally distributed, but we allow for the mean and variance of the distirbution to be estimated:\n\\[\nF \\sim \\mathcal N(\\kappa, \\phi)\n\\]\nInstead, we fix each factor (if we have multiple) to the scale of one of the items, with each factor fixed to a different item. Basically, this means that the factor will take the same scale/measurement characteristics as one of the items. This is done by fixing that item’s intercept \\(\\tau_i = 0\\), and the same item’s factor loading for that specific factor at \\(\\lambda = 1\\).\nThe estimation and interpretation of confirmatory analysis are essentially identical to that of exploratory analysis we have previously looked at.\nThere is a special set of tests to see if the parameters we set equal to 0 actually make sense. Modification indicies are a sort of hypothesis test for this - larger values indicate parameters that if were not 0, could improve the fit of the model.\n\nThere are also expected parameter changes (EPC), which basically estimate what parameters set to 0 would actually be equal to, if they were added to the model.\n\n\nBelow in the R-code I providem more ways of choosing between confirmatory models and exploratory models (and really any factor models).\n\n\nFor confirmatory analysis, we will need the lavaan package:\n\nlibrary(lavaan)\n\nWe fit a confirmatory model in the following way:\n\n# specify formula\nformula &lt;- '\nf1 =~ X1 + X2 + 0*X3 \nf2 =~ 0*X2 + X3 + X4\n'\n\n# estimate model\nmodel &lt;- sem(formula,\n           data = my_data,\n           std.lv = TRUE,\n           missing = \"fiml\")\nsummary(model)\n\n\nWe put a 0* before any item for that factor we want to set equal to 0. You can also use this command to estimate exploratory models.\n\nIf we have theoretical reasons, we can also force different factor load gins to be equal by including the same labels before as follows:\n\n# specify formula\nformula &lt;- '\nf1 =~ c1*X1 + c1*X2 + X3 \nf2 =~ X2 + X3 + X4\n'\n\n\nSince label c1 appears before X1 and X2 for factor f1, that means their factor loadings will be forced to be equal. You can include more labels as well.\n\nTo create factor scores, we simply use the predict function:\n\npredict(model)\n\n\n\n\n\n\n\nMore on Choosing Between Models\n\n\n\n\n\nTo compare and choose between different models, we have a few ways.\n\nWe can use a likelihood ratio test for nested models (see the regression section for more details).\n\n\nlavTestLRT(model1, model2)\n\n\nWe can use a global goodness of fit test - essentially a likelihood ratio but with the full sample covariance matrix as the null model. We want to fail to reject the null, because we want our model to be as close to the sample covariance matrix as possible.\n\n\nlavTestLRT(model)\n\n\nWe can use AIC and BIC to compare models since factor models are estimated with MLE. These are included in the output.\n\nThere are also a series of fit indicies made for factor analysis. These are included in the output.\n\nRoot Mean Square Error of Approximation (RMSEA): 0 is a perfect fit, 0.05 or smaller is a good fit, and anything above 0.1 is a poor fit.\nStandard Root Mean Square Residual (SRMR): smaller values the better, anything below 0.08 is a good fit.\nTucker and Lewis Index (TLI): 1 is the best fit, anything below 0.9 is a poor fit, and anything above 1 may indicate overfitting.\nComparative Fit Index: values between 0 and 1, anything close to 1 is a good fit.",
    "crumbs": [
      "Latent Variable Models",
      "Confirmatory Analysis"
    ]
  },
  {
    "objectID": "latent/class.html",
    "href": "latent/class.html",
    "title": "Latent Class Model",
    "section": "",
    "text": "Latent class models are an extension of factor analysis, that deals with cases of a categorical latent variable with categorical/binary observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\n\nF\n\nLatent Factor (F)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe factor \\(F\\) is a categorical variable with \\(C\\) number of categories, which are also called latent classes. These categories are treated as if they have no inherent order, and we can choose the number of categories.\nWe also have categorical observed items \\(X_1, X_2, \\dots, X_p\\), with each item \\(X_i\\) having \\(K_i\\) number of categories. The latent class models connects each item with a factor through a item response probability:\n\\[\n\\begin{align}\nPr(X_1 = k \\ | \\ F = c) & = \\pi_{1,kc} \\\\\nPr(X_2 = k \\ | \\ F = c) & = \\pi_{2,kc} \\\\\n\\vdots \\qquad \\qquad  \\vdots \\qquad & \\qquad \\vdots \\\\\nPr(X_p = k \\ | \\ F = c) & = \\pi_{p,kc} \\\\\n\\end{align}\n\\]\n\nEach \\(\\pi_{i, kc}\\) is the probability of any item \\(X_i\\) being in category \\(k\\), given the factor \\(F\\) is category \\(c\\).\n\nWe also have another part of the measurement mode, the structural model, which determines the probability of each category in the factor:\n\\[\n\\alpha_c = Pr(F=c)\n\\]\nInterpretation of the latent factor \\(F\\) depends on these item response probabilities \\(\\pi_{i, kc}\\). An example is provided below, because it can be a little confusing.\n\n\n\n\n\n\nExample of Interpretation\n\n\n\n\n\nBelow, the columns are the different classes/categories of the factors, and the big rows are each item.\n\nThe first class (the first column) has the highest probabilities if individuals never worry about crime, no real effect on quality of life, never worry about burglary, and no real effect on quality of life. Thus, we can conclude this first category of the latent variable is something like - not worried about crime.\nThe second class (the 2nd column), where the top responses have the highest probabilities except for the frequency of worry about burglaries - where the probabilities are highest for some of the time and just occasionally. This suggests that this second category is measuring something like - only worried about burglary, and no other crime.\n\n\n\nWe can also create factor scores - which is a little different, because now we are basically assigning every unit in our data to a category of the latent factor. This is done by calculating the posterior probability of being in each class:\n\\[\n\\widehat{Pr}(F = c | X_1 = k_1, X_2 = k_2, \\dots ) \\quad \\text{for all }c\n\\]\nWhichever category \\(c\\) of the latent variable has the highest probability, is the category a unit is assigned to.\n\nThis can be considered quite similar to that of cluster analysis, which will be introduced later.\n\n\nTo implement latent class models, we will need the polLCA package:\n\nlibrary(poLCA)\n\nThis package requires that our categories of items are labelled starting with 1. This means if you have a binary variable of 0 and 1, you will need to change it to 1 and 2.\nTo begin, we will first need to create a vector of our item names:\n\nvars &lt;- c(\"X1\",\"X2\",\"X3\",\"X4\")\n\nThen, let us fit our model as follows:\n\nform &lt;- cbind(X1, X2, X3, X4) ~1\nmodel &lt;- poLCA(form,\n              my_data[,vars],\n              nclass=2, #number of categories for factor\n              na.rm=F,\n              nrep=10) \n\n\nna.rm = F means to include missing values when estimating (which is recommended). nrep = 10 indicates how many times to run the gradient descent algorithm - more is better, but will take longer.\n\nThe traditional output is hard to read, so we will use a function:\n\n# function\nLCA.probs &lt;- function(res){\n  probs &lt;- res$probs\n  item.p &lt;- NULL\n  for(i in seq_along(probs)){\n        m.tmp &lt;- t(probs[[i]])\n        rownames(m.tmp) &lt;- paste(names(probs)[i],colnames(probs[[i]]),sep=\".\")\n        item.p &lt;- rbind(item.p,m.tmp)\n  }\n  item.p &lt;- round(item.p,3)\n  class.p &lt;- res$P\n  names(class.p) &lt;- colnames(item.p)\n  list(item.probabilities=item.p,class.probabilities=class.p)\n}\n\n# output results\nLCA.probs(model)\n\nWe can calculate factor scores/classification as follows:\n\nmodel$predclass\n\nWe can choose our model based on the AIC or BIC score.\n\nmodel$aic\nmodel$bic",
    "crumbs": [
      "Latent Variable Models",
      "Latent Class Models"
    ]
  },
  {
    "objectID": "latent/irt.html",
    "href": "latent/irt.html",
    "title": "Item Response Theory",
    "section": "",
    "text": "Item Response Theory (IRT), also called Latent Trait Models, is an extension of factor anlaysis, that deals with cases with a continuous latent factor but a binary/categorical set of observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\n\nF\n\nLatent Factor (F)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe factor (notated \\(F\\) or \\(\\xi\\)), is the unobserved latent variable we want to measure. We assume that the factor \\(F\\) is continuous, and normally distributed:\n\\[\nF \\sim \\mathcal N(0, 1)\n\\]\nWe must have at least 3 observed items for estimation purposes. The items \\(X_1, \\dots, X_p\\) are associated with the latent factor with a binomial logistic model:\n\\[\n\\begin{align}\n\\log \\left( \\frac{Pr(X_1 =1|F)}{1 - Pr(X_1 =1|F)}\\right) & = \\tau_1 + \\lambda_1 F \\\\\n\\log \\left( \\frac{Pr(X_2 =1|F)}{1 - Pr(X_2 =1|F)}\\right) & = \\tau_2 + \\lambda_2 F \\\\\n\\vdots \\qquad \\qquad \\vdots \\qquad & \\qquad \\vdots \\\\\n\\log \\left( \\frac{Pr(X_p =1|F)}{1 - Pr(X_p=1|F)}\\right) & = \\tau_p + \\lambda_p F \\\\\n\\end{align}\n\\]\n\nFor categorical (more than 2 categories) items, a multinomial or ordinal logistic model is used. But this is pretty rare for someone to actually fit a model of this type.\n\n\\(\\tau_i\\) is the intercept of the model, called the difficulty. \\(\\lambda_i\\) is the coefficeint that describes the relationship between any item \\(X_i\\) and the factor \\(F\\). These are called factor loadings. It is also called the discrimination parameter.\nThese factor loadings are interepreted in a very similar way to factor analysis. The sign of the factor loading tells us the direction in which our latent variable is measuring. The absolute size of the factor loading tells us how important that item is to the factor.\nThe intercepts \\(\\tau_i\\) also tell us how “common” a value of 1 for the item is.\n\n\n\n\n\n\nExample of Interpretation\n\n\n\n\n\nBelow is a example of an interpretation - the items are binary (things that politicians want/don’t want).\n\nWe can first look at the signs:\n\nThe latent factor increases with agreeing (a value of 1) with PartialBirth, CapitalGains, and CAFTA.\nThe latent factor decreases with agreeing (a value of 1) with StemCell, IraqWithdraw, Immigration, and Minimum Wage.\n\nFrom this, we can kind of get a sense that latent variable is measuring right-left political affiliation, with higher values indicating right-wing, since agreeing left wing policies like IraqWithdraw, Immigration, and Minimum Wage are negative.\nThe absolute size of the factor loadings are quite similar - we can see that CAFTA is the least important to the factor.\nThe \\(d\\) (which in this output are the intercepts) show the average agreement with each item - we see the most people agree with MinimumWage and StemCell, and the least people agree with CAFTA and Immigration.\n\n\n\nThe factor scores are also very similar to factor analysis - they are a linear combination, with weights determined by how large a loading is for a certain item.\nThe models can also accommodate confirmatory analysis and multiple factors, just like factor analysis can.\n\nTo run an item response theory mode, we need the mirt package:\n\nlibrary(mirt)\n\nThen, we run the model as follows (make sure to subset your data to only include the items).\n\nmodel &lt;- mirt(data = my_data, model = 1, SE = TRUE)\ncoef(result)\n\n\nmodel = 1 indicates how many factors you want to include.\n\nTo get factor scores, we do the following:\n\nscores &lt;- as.vector(fscores(model, method = \"EAP\"))\n\n\n\n\n\n\n\nMore on Choosing Between Models\n\n\n\n\n\nTo compare and choose between different models, we have a few ways.\n\nWe can use a likelihood ratio test for nested models (see the regression section for more details).\n\n\nlavTestLRT(model1, model2)\n\n\nWe can use a global goodness of fit test - essentially a likelihood ratio but with the full sample covariance matrix as the null model. We want to fail to reject the null, because we want our model to be as close to the sample covariance matrix as possible.\n\n\nlavTestLRT(model)\n\n\nWe can use AIC and BIC to compare models since factor models are estimated with MLE. These are included in the output.\n\nUnfortunately, fit indicies like RMSEA do not work on IRT models.",
    "crumbs": [
      "Latent Variable Models",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "latent/factor.html",
    "href": "latent/factor.html",
    "title": "Factor Analysis Model",
    "section": "",
    "text": "Factor analysis is a way to model a continuous latent variable, based on a bunch of continuous observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\n\nF\n\nLatent Factor (F)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe factor (notated \\(F\\) or \\(\\xi\\)), is the unobserved latent variable we want to measure. We assume that the factor \\(F\\) is continuous, and normally distributed:\n\\[\nF \\sim \\mathcal N(0, 1)\n\\]\n\nWhere \\(\\kappa = 0\\) is the mean of the factor, and \\(\\phi =1\\) is the variance of the factor. Technically, the factor only needs to be normally distributed - we can have different means and variances. However, for identification reasons, we typically fix the factor to a standard normal.\n\nThe items, notated \\(X_1, X_2, \\dots, X_p\\), are the variables we observe, that we believe are a measurement of the latent factor \\(F\\). Factor analysis assumes that each observed item is associated with the latent factor with a linear regression model:\n\\[\n\\begin{align}\nX_1 & = \\tau_1 + \\lambda_1F + \\delta_1 \\\\\nX_2 & = \\tau_2 + \\lambda_2 F + \\delta_2 \\\\\n& \\vdots \\qquad \\qquad \\vdots \\\\\nX_p & = \\tau_p + \\lambda_p F + \\delta_p\n\\end{align}\n\\]\n\nWe assume that the error terms are normally distributed \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\), that the different error terms \\(\\delta_1, \\dots, \\delta_p\\) are uncorrelated with each other, and \\(F\\) is uncorrelated with the errors (exogeneity).\n\n\n\\(\\tau_i\\) is the intercept of the model.\n\\(\\lambda_i\\) is the coefficient that describes the relationship between any item \\(X_i\\) and the factor \\(F\\). These are called factor loadings.\n\\(\\delta_i\\) is the error term - the part of an item \\(X_i\\) not explained by the factor \\(F\\). They are called the unique factors.\n\nThe factor loadings \\(\\lambda_i\\) represent the relationship/covariance between any item \\(X_i\\) and a factor \\(F\\). If the items \\(X_i\\) have been standardised to a standard normal, then \\(\\lambda_i\\) is also the correlation coefficient between items and factor.\n\nNote: this isn’t entirely true for multiple factors, as we will discuss in a later page.\n\nThese factor loadings help us interpret our latent variable \\(F\\). The sign of the factor loading tells us the direction in which our latent variable is measuring. The absolute size of the factor loading tells us how important that item is to the factor.\n\n\n\n\n\n\nInterpretation Example\n\n\n\n\n\nThis is a typical output of factor anlaysis. ML1 and ML2 are the two factors (let us focus on just ML1), and the rows are items (which are trust in different institutions).\n\nFor factor 1 (ML1), we can see that pol_parties, politicians have very large loadings. EP, parliament, and UN have moderate loadings. Police and legal have almost 0 loadings. Almost all loadings are positive.\nThis tells us that factor 1 is a latent variable that measures mostly trust in politicians, rather than the legal/policing system. Since the loadings are almost all positive, we can conclude that higher values of factor 1 mean higher levels of trust in legal/policing systems.\n\n\n\nWe can conduct hypothesis testing with each factor loading \\(\\lambda\\) with a z-test. The null hypothesis is that \\(\\lambda = 0\\), and the alternate hypothesis is that \\(\\lambda ≠ 0\\). If the p-value is less than 0.05, we know that there is a relationship between that item and the factor.\n\nTo implement factor analysis, we will need the psych and GPArotation package:\n\nlibrary(psych)\nlibrary(GPArotation)\n\nFirst, we should get rid of missing observations:\n\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\nFor factor analysis with one factor, we use the syntax:\n\nfa &lt;- fa(data[,items], nfactors=1, fm=\"ml\")\nprint(fa1)",
    "crumbs": [
      "Latent Variable Models",
      "Factor Analysis Models"
    ]
  },
  {
    "objectID": "soo/genetic.html",
    "href": "soo/genetic.html",
    "title": "Genetic Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red). However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{purple}{Y^{(0)}_\\text{Matched individual}}\n\\]\nLike distance matching, genetic matching matches an individual that is treated (like Mia) with one that is not treated based on how close their confounding values are. However, genetic matching uses a slightly different variation of mahalanobis distance:\n\\[\n\\delta_{i, j}(\\b W) = \\sqrt{(\\b x_i - \\b x_j)' \\ (\\b\\Sigma_x^{-\\frac{1}{2}})' \\ \\b W \\ \\b\\Sigma_x^{-\\frac{1}{2}}  (\\b x_i - \\b x_j)}\n\\]\n\nWhere \\(i\\) and \\(j\\) are two units we want to measure the distance between, \\(\\b x\\) are their confounder values, and \\(\\b\\Sigma_x\\) is the covariance matrix of confounders. \\(\\b W\\) is a weights matrix.\n\nThe weights \\(\\b W\\) are estimated to make the treated and untreated groups as similar as possible. This balance between treated and untreated eliminates selection bias. Then, matching is done with the units that have the smallest distance.\n\nBefore you start genetic matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the Matching and MatchIt package.\n\nlibrary(Matching)\n\nFirst, we need to estimate the propensity scores with a logistic regression.\n\nIt is recommended to use the propensity score as one of the controls on which to genetic match on.\n\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity,\n                          type = \"response\")\n\nThen, we use the GenMatch() function to estimate a weights matrix \\(\\b W\\):\n\nset.seed(333) #any number works\ngen &lt;- GenMatch(Tr = my_data$D,\n                    X = my_data[,c(\"X1\",\"X2\",\"pscore\")],\n                    BalanceMatrix = my_data[,c(\"X1\",\"X2\")],   \n                    estimand = \"ATT\",\n                    M = 2,\n                    replace = TRUE,\n                    ties = FALSE,\n                    distance.tolerance = 0,\n                    print.level = 0,\n                    pop.size = 200)\n\n\nYou can increase pop.size to increase the accuracy - but it will increase the time and computational power needed.\n\nNow, let us conduct estimation with genetic matching:\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,c(\"X1\",\"X2\",\"pscore\")],\n             estimand = \"ATT\",\n             M = 2,\n             replace = TRUE,\n             ties = FALSE,\n             distance.tolerance = 0,\n             Weight.matrix = gen$Weight.matrix,\n             Weight = 3)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Genetic Matching"
    ]
  },
  {
    "objectID": "soo/pscore.html",
    "href": "soo/pscore.html",
    "title": "Propensity Score Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red). However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{purple}{Y^{(0)}_\\text{Matched individual}}\n\\]\nPropensity Score Matching matches an individual that is treated (like Mia) with one that is not treated based on how similar their likelihoods of treatment are.\nWhat is a likelihood of treatment? Well we know confounders cause people to get the treatment or not treatment. Thus, using an individual’s confounder values, we can estimate their likelihood of getting treatment, called a propensity score.\n\\[\n\\text{propensity score } \\pi =Pr(\\text{you get treated})\n\\]\nPropensity scores are typically estimated with a logistic regression:\n\\[\n\\log\\left( \\frac{\\pi(\\b X_i)}{1- \\pi(\\b X_i)}\\right) = \\alpha + \\b X_i' \\b\\beta\n\\]\nThis also means that propensity score matching shares the same weaknesses of logistic regression - including assuming linear relatinoships between confounders and propensities, and only being unbiased in large sample sizes.\n\nBefore you implement propensity score matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the Matching package.\n\nlibrary(Matching)\n\nFirst, we need to estimate the propensity scores with a logistic regression:\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity,\n                          type = \"response\")\n\n\nA random forest model is also possible, but less common.\n\nNow, we can implement the matching as follows.\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,\"pscore\"],\n             M = 1,\n             BiasAdjust = TRUE,\n             Weight = 2)\nsummary(att)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Propensity Score Matching"
    ]
  },
  {
    "objectID": "soo/soo.html",
    "href": "soo/soo.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "Our issue in causal inference is that a confounder is causing pre-existing differences:\n\n\n\n\n\n\n\n\nexample2\n\n\n\nD\n\nReceiving Scholarship\n\n\n\nY\n\nUniversity Grades\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nIntellegence (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nBy definition, as the confounder changes, your likelihood of getting treatment changes. As the confounder changes, the outcome value will also change.\nThese issues occur when the confounder changes in value. So what if we hold the confounders constant? Then, there would be no changes in confounders - so the variation in treatment assignment and outcomes cannot be attributed to the confounder.\nFor example, let’s assume that intellegence has two values: smart and dumb. Let us calculate the treatment effect within each level of intellegence:\n\\[\n\\tau_\\text{smart} = \\purple{\\mean Y_\\text{smart}^{(1)}} - \\purple{\\mean Y_\\text{smart}^{(0)}}\n, \\quad \\tau_\\text{dumb} = \\purple{\\mean Y_\\text{dumb}^{(1)}} - \\purple{\\mean Y_\\text{dumb}^{(0)}}\n\\]\nThe confounder is constant here, so no selection bias. Thus, within each category, correlation is equal to causation. Our overall causal effect will be a weighted average of the categories:\n\\[\n\\tau = \\tau_\\text{smart} Pr(\\text{smart})  \\ + \\ \\tau_\\text{dumb} Pr(\\text{dumb})\n\\]\n\nThe weights of this weighted average are the probability/frequency of that value of the confounder.\n\nObviously, most confounders have more than 2 categories, and we often have more confounders. But the same intuition applies. Given a set of confounders \\(\\set X\\), we first calculate the treatment effect within a specific vector of confounder values \\(\\b x\\):\n\\[\n\\tau(\\b x) = (\\T - \\C)| \\b x\n\\]\nAnd then, we find the weighted average for the total effect, with the weights being the frequency of that specific vector of confounder values:\n\\[\n\\tau_{ATE} = \\sum \\tau(\\b x) \\cdot Pr(\\b x)\n\\]\nFor selection on observables to work, we need to meet 3 assumptions:\n\n\n\n\n\n\n\nAssumption\nDescription\n\n\nConditional Ignorability\nThis means that we must account for all possible confounders (cannot miss a single one).\n\n\nCommon Support\nThis means no one can have a 100% chance of being in treatment or control. They always have a chance to be in either, no matter their confounding values.\n\n\nStable Unit Treatment Value Assumption (SUTVA)\nThis means that if Ava is treated, that does not affect Mia’s outcome (and for any other 2 individuals).\n\n\n\n\nWe have a wide choice of estimators that we can use. Use the sidebar or links in the table to access each estimator’s page.",
    "crumbs": [
      "Selection on Observables"
    ]
  },
  {
    "objectID": "soo/distance.html",
    "href": "soo/distance.html",
    "title": "Distance Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red). However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{purple}{Y^{(0)}_\\text{Matched individual}}\n\\]\nDistance matching matches an individual that is treated (like Mia) with one that is not treated based on how close their confounding values are. We define closeness by Mahalanobis distance:\n\\[\n\\delta_{i, j} = \\sqrt{(\\b x_i - \\b x_j)' \\ \\b\\Sigma_x^{-1} (\\b x_i - \\b x_j)}\n\\]\n\nWhere \\(i\\) and \\(j\\) are two units we want to measure the distance between, \\(\\b x\\) is a vector of confounder values, and \\(\\b\\Sigma_x\\) is the covariance matrix of confounders.\n\nBecause distance matching depends on finding matches in a n-dimensional space, it is subject to the curse of dimensionality. This essentially means that the more confounders you have, the more dimensions you have to match over, and the harder it is to find good matches. So we typically do not use any more than 3-5 confounders with distance matching.\n\nBad matches means incorrectly using someone’s counterfactual, resulting in bad estimates.\n\n\nBefore you implement distance matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the Matching package.\n\nlibrary(Matching)\n\nNow, we can implement the matching as follows.\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,c(\"X1\",\"X2\", \"X3\")],\n             M = 1,\n             BiasAdjust = TRUE,\n             Weight = 2)\nsummary(att)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Distance Matching"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Basics of Causality",
    "section": "",
    "text": "This page covers how confounders cause pre-existing differences between treated and untreated (selection bias), meaning correlation is not causation.\n\nLet us look at this causal question:\n\n\n\n\n\n\n\n\nexample1\n\n\n\nD\n\nGoing to the Hospital (D)\n\n\n\nY\n\nHealth Outcomes (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nWe have a treated group (went to hospital), and an untreated group. Using our potential outcomes framework, we can define the treatment effect of the treated group:\n\\[\n\\tau_\\text{treated} = \\textcolor{purple}{\\mean Y_{\\text{treated}}^{(1)}} - \\textcolor{red}{\\mean Y_{\\text{treated}}^{(0)}}\n\\]\n\nIn red is the counterfactual we do not observe.\n\nNow compare the treatment effects above to correlation, which is defined as the difference in observed outcomes:\n\\[\n\\begin{align}\n\\text{correlation} & = \\mean Y_\\text{treated} - \\mean Y_\\text{untreated} \\\\\n& = \\textcolor{purple}{\\mean Y_{\\text{treated}}^{(1)}} - \\textcolor{purple}{\\mean Y_\\text{untreated}^{(0)}}\n\\end{align}\n\\]\nIf we compare this correlation to our \\(\\tau_\\text{treated}\\), we see:\n\\[\n\\text{if  } \\textcolor{purple}{\\mean Y_\\text{untreated}^{(0)}} ≠ \\textcolor{red}{\\mean Y_\\text{treated}^{(0)}}, \\text{ then }  \\tau_\\text{treated} ≠ \\text{correlation}\n\\]\n\nThese two quantities are potential outcomes under control, or in another way to think of it, outcomes of the two groups prior to treatment happening.\n\nThus, if there is a difference between the average outcomes between treated and untreated before treatment is administered, then correlation is not equal to causation. This is because we cannot tell if the difference between the groups is due to treatment, or due to their pre-existing differences.\n\nWhat causes pre-existing differences? Confounders. For example, in our hospital-health example, a confounder could be smoking.\n\nSmoking is not the only possible confounder, we just use it as an example. Drinking, age, etc. are all other potential confounders.\n\nSmoking will worsen health outcomes. Someone who smokes is also more likely to visit the hospital with health complications. That means people who go to the hospital start out with (on average) worse health outcomes than people who did not go to the hospital.\n\n\n\n\n\n\n\n\nexample2\n\n\n\nD\n\nGoing to the Hospital (D)\n\n\n\nY\n\nHealth Outcomes (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmoking (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA confounder is a third variable that has the following characteristics:\n\nThe confounder is correlated (positive or negative) with the outcome variable.\nThe confounder causes who gets and doesn’t get the treatment.\nThe confounder is not itself caused by the treatment\n\n\n\n\n\nNote requirement 3 - it is a common mistake. Any result of the treatment \\(D\\) cannot be a confounder.\n\nConfounders cause pre-existing differences, which cause correlation to not equal causation. We must account for confounders to uncover causal effects.",
    "crumbs": [
      "Home",
      "Issue of Selection Bias"
    ]
  },
  {
    "objectID": "frameworks.html",
    "href": "frameworks.html",
    "title": "Basics of Causality",
    "section": "",
    "text": "This page covers the potential outcomes framework and the causal estimands.\n\nIn causal inference, we are interested in causal questions:\n\n\n\n\n\n\n\n\nexample1\n\n\n\nD\n\nTreatment (D)\n\n\n\nY\n\nOutcome (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\n\nWe generally assume the treatment \\(D\\) is binary.\n\nImagine we have two hypothetical parallel worlds that are copies of each other. Both of these worlds are identical except for one aspect: the treatment:\n\n\n\n\n\n\n\n\nParallel World\nTreatment\nPotential Outcome\n\n\nKevin does not Receive Treatment\n\\(D_\\text{Kevin} = 0\\)\n\\(\\purple{Y_\\text{Kevin}^{(0)}}\\)\n\n\nKevin Receives Treatment\n\\(D_\\text{Kevin} = 1\\)\n\\(\\purple{Y_\\text{Kevin}^{(1)}}\\)\n\n\n\n\nThe only difference between the two worlds is the treatment. Thus, any difference in outcomes between the two worlds must be the causal effect of the treatment.\n\\[\n\\tau_\\text{Kevin} = \\purple{Y_\\text{Kevin}^{(1)}} - \\purple{Y_\\text{Kevin}^{(0)}}\n\\]\n\nTechnically, we need another assumption, SUTVA, for this to be true. I will explain this assumption as part of the identification assumptions.\n\nHowever, in reality, we do not have two parallel worlds. Thus, by definition, one of the potential outcomes is not observed in our real world - the counterfactual.\n\n\n\n\n\n\n\n\nIn the Real World\nObserved Outcome\nCounterfactual\n\n\nKevin receives treatment (treated)\n\\(Y_\\text{Kevin}= \\purple{Y_\\text{Kevin}^{(1)}}\\)\n\\(\\red{Y_\\text{Kevin}^{(0)}}\\)\n\n\nKevin did not receive treatment (untreated)\n\\(Y_\\text{Kevin} = \\purple{Y_\\text{Kevin}^{(0)}}\\)\n\\(\\red{Y_\\text{Kevin}^{(1)}}\\)\n\n\n\n\nThe fundamental problem of causal inference is that in order to calculate our individual treatment effect \\(\\tau\\), we need both potential outcomes. Our goal is to estimate causal effects without observing counterfactuals. This is difficult at the individual level, so instead, we focus on average treatment effects for groups:\n\n\n\n\n\n\n\n\nGroup Effects\nNotation\nDefinition\n\n\nAverage Treatment Effect (ATE)\n\\(\\tau_\\text{ATE}\\)\nThe average treatment effects for all individuals in our study (treated and untreated).\n\n\nAverage Treatment Effect on the Treated (ATT)\n\\(\\tau_\\text{ATT}\\)\nThe average treatment effect but only for individuals who receive the treatment in our study.\n\n\nLocal Average Treatment Effect (LATE)\n\\(\\tau_\\text{LATE}\\)\nThe average treatment effect but only for a specific (local) group of individuals in a study.",
    "crumbs": [
      "Home",
      "Basics of Causality"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resources on Causal Inference and Social Science Statistics",
    "section": "",
    "text": "This repository contains resources on causal inference and social statistics, specifically focused on applications to politics and economics.\n\nUse the top navigation bar to navigate between different methods.\nUse the side navigation bar to navigate within a method.\nR-code for implementation is included.\nI have tried to keep up with modern advancements, including the recent difference-in-differences rennaisance.\n\nThese resources have been collected throughout my postgraduate degree at the London School of Economics. I hope these resources can be useful for my future self and others. I try to ensure everything in this repository is accurate. If there are any issues with the repository, let me know.\nThere are some stuff that doesn’t fit into any category, which I have included in the home page sidebar.\nFor my personal website, see http://kevinli03.github.io",
    "crumbs": [
      "Home",
      "Homepage"
    ]
  },
  {
    "objectID": "soo/soochoose.html",
    "href": "soo/soochoose.html",
    "title": "Choosing an Estimator",
    "section": "",
    "text": "We know how selection on observables works now. But how do we actually implement selection on observables? Below are a list of estimators and their strengths/weaknesses. You can use multiple simultaneously for robustness.\n\n\n\n\n\n\nLinear Regression Estimator\n\nEstimand: ATE\nPros: very simple, works well with small datasets.\nCons: 1) requires linear relationship between confounders and outcome, 2) does not work when there are heterogenous treatment effects.\n\n\nKevin’s Comments: since heterogeneity is so common in almost everything, I would recommend against using this estimator unless nothing else is possible. The fully interacted estimator (below) is just a better version of this.\n\n\n\nFully Interacted Estimator\n\nEstimand: ATE\nPros: 1) modified regression to allow for heterogenous effects, 2) still relatively simple.\nCons: requires linear relationship between confounders and outcome\n\n\n\nDistance Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome. 2) relatively intuitive idea.\nCons: 1) can be badly biased when more than 3-5 confounders, 2) throws out unmatched data so wastes data.\n\n\nKevin’s Comments: there is little reason to use distance matching over genetic matching, unless your machine physically cannot estimate genetic matching.\n\n\n\nPropensity Score Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) can handle larger amounts of confounders than distance matching.\nCons: 1) needs a large sample size to not be biased, 2) throws out unmatched data so wastes data.\n\n\nKevin’s Comments: there is little reason to use propensity score matching over genetic matching, unless your machine physically cannot estimate genetic matching.\n\n\n\nGenetic Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) shown to be the best matching estimator\nCons: 1) throws out unmatched data so wastes data, 2) can be computationally taxing.\n\n\n\nInverse Probability Weighting\n\nEstimand: ATE\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) does not waste data like matching methods do.\nCons: requires a large sample size to be unbiased.",
    "crumbs": [
      "Selection on Observables",
      "Choosing an Estimator"
    ]
  },
  {
    "objectID": "soo/regress.html",
    "href": "soo/regress.html",
    "title": "Linear Regression Estimator",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nSelection on observables is about controlling for confounders. Linear regression is a very natural way to control for confounders.\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2 X_{i2} + \\eps_i\n\\]\nWe know that in the above equation, \\(\\beta_1\\) is the relationship between \\(X_i\\) and \\(Y_i\\), while controllling (holding constant) \\(X_2\\).\nUsing this idea, we can implement causal inference with regression, with one of the explanatory variables being our treatment variable, and the rest of the explanatory variables being control variables.\n\\[\nY_i = \\alpha + D_i\\tau + \\b X_i' \\b\\beta + \\eps_i\n\\]\n\nWhere \\(\\alpha\\) is the intercept and \\(\\b X_i\\) is a vector of confounder values for individual \\(i\\).\n\nOur ordinary least squares (OLS) estimate \\(\\widehat\\tau\\) is an unbiased estimator of the true \\(\\tau_{ATE}\\) given three conditions are met:\n\nWe meet the selection on observables assumption of conditional ignorability. Conditional ignorability implies exogeneity, which means the estimate is unbiased.\nThe relationship between our continuous confounders and outcome variable is linear. This is because if the true relationship between these two is not linear, then our linear model is wrong, so it is not properly controlling for confounders.\nThere is no heterogeneity in treatment effects. Angrist (1998), Lin (2013), and Słoczyński (2022) have proven that when there is heterogeneity, OLS is estimating another quantity that is not the ATE.\n\n\nHeterogeneity means that different individuals have different individual treament effects \\(\\tau_i\\). OLS only estimates the ATE if there is homogeneity - all \\(\\tau_i\\) are equal.\n\n\nBefore you implement the estimator, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the fixest package:\n\nlibrary(fixest)\n\nTo run a regression estimator, we do:\n\nfeols(Y ~ D + X1 + X2 + X3,\n      data = data,\n      se = \"hetero\")\n\n\nWe typically assume heteroscedasticity, so we use heteroscedasticity-robust standard errors. If you can prove homoscedasticity, then you can use normal standard errors.\n\nThe coefficient for the treatment variable will be the ATE - the average treatment effect for all units in the study.\n\nAssuming you have met all the assumptions of selection on observables, and the special assumptions for the linear regression estimator shown above.",
    "crumbs": [
      "Selection on Observables",
      "Linear Regression Estimator"
    ]
  },
  {
    "objectID": "soo/ipw.html",
    "href": "soo/ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nLet us look at this example, with a confounder.\n\n\n\n\n\n\n\n\nexample2\n\n\n\nD\n\nReceiving Scholarship\n\n\n\nY\n\nUniversity Grades\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s pretend there are only dumb and smart people (for simplicity). Our treated and control groups might be:\n\n\n\n\n\n\n\nTreated (Got Scholarship)\nUntreated (Did not get scholarship)\n\n\nSmart Students (x4)\nSmart Students (x1)\n\n\nDumb Students (x1)\nDumb Students (x4)\n\n\n\nOur two groups have pre-existing differences. However, by emphasising certain individuals, we can make it seem like there are no more imbalances. For example, weighting might make our above table become:\n\n\n\n\n\n\n\nTreated (Got Scholarship)\nUntreated (Did not get scholarship)\n\n\nSmart Students (x4)\nSmart Students (emphasise to x4)\n\n\nDumb Students (emphasise to x4)\nDumb Students (x4)\n\n\n\n\nSee how the underrepresented individuals in each group (treated/untreated) were weighted upwards. More technically, inverse probability weighting emphasises/weights an individual by the inverse of their likelihood to receive treatment.\n\nWe can see there is no more pre-existing differences after weighting. Thus, selection bias has been solved.\n\nBefore you inverse probability weighting, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the estimatr package:\n\nlibrary(estimatr)\n\nTo estimate the propensity scores and weights, we can use the glm() command:\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity, type = \"response\")\nmy_data$ipw &lt;- ifelse(mydata$D == 1,\n                      1/my_data$pscore,\n                      1/(1-my_data$pscore))\n\nFinally, we need to use the lm_robust() command to estimate our causal effects:\n\nate &lt;- estimatr::lm_robust(Y ~ D, data = my_data, weights = ipw)\nsummary(ate)\n\nThe output will be the ATE - the average treatment effect for all units in the study.",
    "crumbs": [
      "Selection on Observables",
      "Inverse Probability Weighting"
    ]
  },
  {
    "objectID": "soo/interact.html",
    "href": "soo/interact.html",
    "title": "Fully Interacted Estimator",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nFrom selection on observables, we know that our causal effect is a weighted average:\n\\[\n\\tau_\\text{ATE} = \\sum\\tau (\\b x) Pr(\\b x)\n\\]\nNotice how the weights are the probability of the confounder values of \\(\\b x\\). With some complex math (Angrist 1998), we can actually show that OLS actually estimates:\n\\[\n\\hat\\beta_\\text{OLS} = \\sum \\tau(\\b x) \\frac{Var(D_i | \\b x)Pr(\\b x)}{\\sum Var(D_i | \\b x^c)Pr(\\b x^c)}\n\\]\n\nWhere \\(\\b x^c\\) is the complement (not \\(\\b x\\)).\n\nThese weights are not equivalent to the selection on observables \\(\\tau_{ATE}\\). Thus, if not all \\(\\tau(\\b x)\\) are exactly the same (which implies heterogeneity), then our linear regression estimator will incorrectly estimate the ATE.\nHeterogeneity is present in almost all situations we are interested in. Lin (2013) proposes the fully interacted estimator, which allows for consistent estimation of the ATE even with heterogeneity:\n\\[\nY_i = \\alpha + D_i \\tau + (\\b{X}_i - \\mean{\\b X})' \\b\\beta \\ + D_i (\\b X_i - \\mean{\\b X})' \\b\\gamma \\  + \\eps_i\n\\]\n\n\\(\\mean{\\b X}\\) is a vector of the means of each confounder. \\(\\tau\\) is the estimate of the ATE. See Lin (2013) for proofs.\n\nThe new OLS estimate of \\(\\hat\\tau\\) in this estimator will technically still be a biased estimator of \\(\\tau_{ATE}\\), but the bias is negligible.\n\nBefore you implement the estimator, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the estimatr package:\n\nlibrary(estimatr)\n\nThen, we can use the lm_lin() function to estimate:\n\nate &lt;- estimatr::lm_lin(Y ~ D,\n                        covariates = ~ X1 + X2 + X3,\n                        data = my_data)\nsummary(ate)\n\nThe output will be the ATE - the average treatment effect for all units in the study.",
    "crumbs": [
      "Selection on Observables",
      "Fully Interacted Estimator"
    ]
  },
  {
    "objectID": "latent/scores.html",
    "href": "latent/scores.html",
    "title": "Reliability and Factor Scores",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nRecall our measurement models linking each item \\(X_1, X_2, \\dots, X_p\\) to our factor:\n\\[\nX_i = \\tau_i + \\lambda_i F + \\delta_i\n\\]\nOne of the assumptions of this model is the the error term \\(\\delta_i \\sim \\mathcal N(0, \\theta_i)\\), or in other words, the error term has a mean of 0 and a variance of \\(\\theta_i\\). Through some complex math, we can show that the variance of each item \\(X_i\\) is as follows:\n\\[\nVar(X_i) = \\lambda_i^2 + \\theta_i\n\\]\nThus, this allows us to essentially “split” the variance in any item \\(X_i\\) into two parts:\n\n\\(\\lambda_i^2\\) is the part of the variance in the item explained by the latent factor. We call this the communality of \\(X_i\\).\n\\(\\theta_i\\) is the residual variance, the part of the variance not explained by our factor.\n\nWe can also calculate the percentage/proportion of variance in \\(X_i\\) that our factor explains, called the reliability of \\(X_i\\):\n\\[\n\\text{Reliability} = \\frac{\\lambda_i^2}{Var(X_i)} = \\frac{\\lambda_i^2}{\\lambda_i^2+\\theta_i}\n\\]\n\nIf all items \\(X_i\\) are standarised to a standard normal, then \\(\\lambda_i^2\\) is equal to the reliability.\n\nCommunality and relaibility is important for two reasons:\n\nItems with higher reliability are considered more “accurate” measures of the latent variable \\(F\\). They thus ensure better model estimation - and when a factor has low relaibility, we will often drop it.\nThese allow us to calculate factor scores.\n\nFactor scores \\(\\widetilde F\\) are essentially values of the latent variable for individuals in our study. This allows us to use our observed items \\(X_i\\) to calculate the latent variable value that any individual should have, which we can then put into another statistical model.\nWe often conduct factor anlaysis for the sole purpose of getting factor scores. Factor scores are calculated as a weighted linear combination of all items:\n\\[\n\\widetilde F = w_0 + w_1 X_1 + w_2X_2 + \\dots\n\\]\nThe weights are calculated based on the communalities. The items with the highest communalities tend to get the strongest weights, while the items with the least communalities get the smallest weights.\n\nTo calculate factor analysis, we have to first run a factor analysis model in the same way we did in the last page.\n\nlibrary(psych)\nlibrary(GPArotation)\n\n# eliminate missing observations\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\n# factor model\nfa &lt;- fa(data[,items], nfactors=1, fm=\"ml\")\n\nR will automatically calculate factor scores in the estimation process, so all we have to do is access it within our output object:\n\nfa$scores\n\nYou can save this into your dataset, and use for other purposes.",
    "crumbs": [
      "Latent Variable Models",
      "Reliability and Factor Scores"
    ]
  },
  {
    "objectID": "latent/multiplefac.html",
    "href": "latent/multiplefac.html",
    "title": "Multiple Latent Factors",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nSo far, we have covered factor analysis for a single factor. However, factor analysis can work with multiple latent factors.\n\n\n\n\n\n\n\n\nexample2\n\n\n\nF1\n\nLatent Factor (F1)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF1-&gt;X3\n\n\nλ\n\n\n\nF2\n\nLatent Factor (F2)\n\n\n\nF2-&gt;X2\n\n\nλ\n\n\n\nF2-&gt;X3\n\n\nλ\n\n\n\nX4\n\nItem 4 (X4)\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\n\n\n\n\n\n\nWe can see in the above figure, an example of two latent factors. Each item can measure either only one or two of the factors. The factors can also both have the same items, but put emphasis on different items.\nLet us say we have multiple factors \\(F_1, F_2, \\dots, F_q\\). Let us use vector \\(\\b F\\) to reperesent the mall. We still assume the factors are normally distributed:\n\\[\n\\b F \\sim \\mathcal N(\\b \\kappa, \\b\\Phi)\n\\]\nUsing our conventional identification assumption like in single factor analysis, we will assume each factor is a standard normal \\(F \\sim \\mathcal N (0, 1)\\). This implies that \\(\\b\\kappa = 0\\).\nOur variance matrix \\(\\b\\Phi\\) is a little more complicated - the variances of each factor is 1 (as assumed in a standard normal), however, the matrix \\(\\b\\Phi\\) also includes the covariances between factors. This is an additional thing that we will need to estimate that was not present in one-factor models.\n\nFor example, factor \\(F_1\\) and \\(F_2\\) might be correlated with each other, which is reflected in \\(\\b\\Phi\\).\n\nNow on to our measurement models. Just like in single factor anlaysis, we use a linear regression to express the relationship between any item \\(X_i\\) and the factors. However, this time, each regression will relate each \\(X_i\\) to all factors \\(F_1, F_2, \\dots, F_q\\):\n\\[\n\\begin{align}\nX_1 & = \\tau_1 + \\lambda_{11}F_1 + \\lambda_{12}F_2 + \\dots + \\lambda_{1q}F_q + \\delta_1 \\\\\nX_2 & = \\tau_2 + \\lambda_{21} F_1 + \\lambda_{22} F_2 + \\dots + \\lambda_{2q}F_q + \\delta_2 \\\\\n& \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\\\\nX_p & = \\tau_p + \\lambda_{p1} F_1 + \\lambda_{p2}F_2+ \\dots + \\lambda_{pq}F_1 + \\delta_p\n\\end{align}\n\\]\nThe factor loadings \\(\\lambda\\) are still the relationship between each factor and each item, and are interpreted in the same way as a 1-factor model. However, the the interpretation of communality and relaibility are no longer valid with more than 1-factor.\n\nWhen interpreting - do one factor at a time. Start with one, then go to the next. Interpreting each factor is the same as shown in the page on factor analysis.\n\n\nTo implement multiple factor anlaysis, the procedure is quite similar to standard factor analysis. First, we will need the psych and GPArotation package:\n\nlibrary(psych)\nlibrary(GPArotation)\n\nFirst, we should get rid of missing observations:\n\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\nFor factor analysis with multiple factors, the notation is as follows:\n\nfa &lt;- fa(data[,items], nfactors=2, fm=\"ml\", rotate=\"oblimin\")\nprint(fa)\n\n\nChange nfactors=2 to however many factors you want. Note that you cannot have too many, this will be discussed in the next page on identification.\nThis code also uses the “oblimin” rotation, which will be discussed in the next page on identification and rotation.",
    "crumbs": [
      "Latent Variable Models",
      "Multiple Latent Factors"
    ]
  },
  {
    "objectID": "latent/latent.html",
    "href": "latent/latent.html",
    "title": "Latent Variable Models",
    "section": "",
    "text": "A latent variable model connects a unobserved variable (latent factor) with a few observed variables (items) that are considered imperfect measures of the latent factor.\n\n\n\n\n\n\n\n\nexample2\n\n\n\nF\n\nLatent Factor (F)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\n\nFor example, maybe we want to measure the political ideology of a senator. We cannot directly observe the political ideology (the latent factor), but we can observe how they vote on different bills (the items)\n\nLatent variable models assume we can model the relationship between each observed item and the latent factor with some sort of regression model. The coefficient of each regression (often denoted \\(\\lambda\\)) is the relationship between each item and the factor.\n\nFor example, in the figure above, each item (X1, X2, X3) is related to the factor (F) with by a \\(\\lambda\\) coefficient.\n\nThese \\(\\lambda\\) are called factor loadings. We can interpret the estimated factor loadings \\(\\widehat\\lambda\\) to interpret what the unobserved factor actually is measuring.\n\nFor example, if a factor has a strong relationship with one item, and a weaker relationship with another item, we might conclude that the factor measures the first item more than the second item.\n\nWe can also use latent variable models to create factor scores \\(\\widetilde F_i\\), which are basically actual values of the latent variable for each individual \\(i\\) in our data. This allows us to use the latent variable in other statistical models.\nThe choice of latent variable model depends on the type of items/factors:\n\n\n\n\n\n\n\n\nModel\nFactor Type\nItem Type\n\n\nFactor Analysis\nContinuous\nContinuous\n\n\nItem Response Theory\nContinous\nCategorical/Binary\n\n\nStructural Class Models\nCategorical/Binary\nCategorical/Binary\n\n\n\n\nSo far, this introduction has assumed just one latent variable factor. However, all models allow for multiple factors as well. If we have many items, some items might only measure on factor, others both factors.\nEach latent model also has the ability to conduct confirmatory analysis. This is basically when we, based on theoretical reasons (such as reading the literature), impose certain restrictions on models, and test if this hypothesised model is good.\n\nThis typically takes the form of setting certain factor loadings \\(\\lambda\\) to 0, meaning a certain item does not measure one of the factors.\n\nWe can also combine different structural models together to form structural equation models (see the structural equation models part).",
    "crumbs": [
      "Latent Variable Models"
    ]
  },
  {
    "objectID": "latent/identify.html",
    "href": "latent/identify.html",
    "title": "Identification and Rotation",
    "section": "",
    "text": "Make sure you have read all the previous pages on factor analysis before this.\n\nWhen estimating factor analysis models, we encounter many identification issues. An identification issue is basically an inability to calculate a solution. There are two types of identification issues:\n\nWe have too many possible solutions, and we cannot determine which one (a unique identification issue).\nWe don’t have enough data to compute any solutions.\n\nOne identification issue is rotation. For example, imagine our latent variable is left-right political spectrum leaning. Our latent variable could use positive values for right-wing and negative values for left-wing. Or, it could do the opposite. There is no real substantive difference.\nThus, in 1-factor models, most software will arbitrarily choose a rotation, as it does not really make a huge difference.\nIn multiple factor models, rotation becomes more complex. Because of some linear algebra, you can have almost infinite rotations of factors that make no difference. Just like in the 1-factor model, software will arbitrarily choose one rotation.\nHowever, in multiple factor models, the chosen rotation can make a big difference in how easy it is to interpret our solutions.\n\nThe default rotation is often an orthogonal rotation. This means the factors (if visualised in space) are perpendicular to each other, with zero correlation.\nAn oblique rotation is one where the factors are allowed to be correlated with each other (non-perpindicular).\n\n\n\n\n\n\nThe oblique rotations are generally easier to interpret. This is because they make some items have close to 0 factor loadings, which means we can easily say that factor is not measuring that specific item.\nRotations are a unique identification issue. Factor analysis also has some lack-of-parameters identification issues that result in no possible solutions. To avoid this, we should generally have at least 3 items for a 1-factor model, and 2 items per factor for a multiple-factor model.",
    "crumbs": [
      "Latent Variable Models",
      "Identification and Rotation"
    ]
  },
  {
    "objectID": "latent/pca.html",
    "href": "latent/pca.html",
    "title": "Principle Components Analysis",
    "section": "",
    "text": "Note: PCA is not a latent variable model itself, but can be used to approximate a latent variable model.\n\nPrinciple components analysis (PCA) takes a set of observed variables \\(X_1, X_2, \\dots, X_p\\) (called features), and change them into a set of new variables \\(Y_1, Y_2, \\dots Y_p\\) called principle components (PC), without losing any variance/information.\n\nWithout losing information/variance means the total variance of features equals total variance of PCs\n\nEssentially, what PCA does is it takes our features, and finds the “axis” in which there is the most variation, and makes that the 1st PC. Then it finds the “axis” with the 2nd most variation, and makes that the 2nd PC, and so on.\n\n\n\n\n\n\nNote that each principle component is orthogonal/uncorrelated with each other by design.\n\nThe 1st PC explains the most variance in our features, then the 2nd PC, and so on.\n\\[\nVar(Y_1) ≥ Var(Y_2) ≥ \\dots &gt; Var(Y_p)\n\\]\nEach new principle component is a weighted average of the observed variables:\n\\[\n\\begin{align}\nY_1 & = a_{11}X_1 + a_{21} X_2 + \\dots + a_{p1}X_p \\\\\nY_2 & = a_{12}X_1 + a_{22} X_2 + \\dots + a_{p2}X_p \\\\\n& \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\\\\nY_p & = a_{1p}X_1 + a_{2p} X_2 + \\dots + a_{pp}X_p \\\\\n\\end{align}\n\\]\nThe \\(a\\)’s are the weights of each observed feature in creating a principle component, and are determined by the correlation matrix of the observed features. For interpreting a principle component, we often “normalise” the weights \\(a\\) to get the correlation between a observed feature and a principle component:\n\\[\nCorr(X_i, Y_j) = \\sqrt{Var(Y_j)} \\cdot a_{ij}\n\\]\n\nThis value is also called a component loading. Note that the right side is only equal to the correlation given we perform PCA on the correlation matrix (which is standard, but you can use a covariance matrix).\n\nInterpreting the principle components is identical to that of factor analysis - just using these correlations rather than factor loadings. Just like factor analysis, we can also calculate principle component scores, which are the individual values of each principle component for each individual in our data.\n\n\n\n\n\n\nExample of Interpretation\n\n\n\n\n\nBelow, the rows are observed features describing how much an individual trusts different institutions on a scale of 1-10. The columns are the principle components.\n\nIn component 1, we see that the correlation is pretty high for all of the variables, and positive. We might conclude component 1 measures general trust in institutions.\nIn component 2, we can see that the loadgins for legal and police are the highest (in absolute terms) and negative. Meanwhile, politicians, pol_parties, and EP are positive and still not too small. The other loadings are quite small. We might interpret this component as sort of a tradeoff between political trust and legal/law enforcement trust, with higher values indicating more political trust, and lower values indicating more trust in legal/police.\nIn component 3, we see positive loadings for everything but EP and UN. This might measure the tradeoff between trust in national and international institutions, with higher values for trust in national institutions, and lower values indicating more trust in international institutions.\n\n\n\n\nTo implement principle components analysis, we do the following:\n\npca &lt;- princomp(~ X1 + X2 + X3,\n                data = my_data,\n                cor = TRUE, #use correlation matrix\n                scores = TRUE, #calculate pc scores\n                na.action=na.exclude)\nsummary(pca)\n\nNow, to get the component loadings/corelation, we do the following:\n\n# grab the weights\nweights &lt;- loadings(pca)\n\n# grab the sqrt of variance\nsqrt.var &lt;- pca$sdev\n\n# calculate component loadings/correlation\nprint(t(t(weights)*sqrt.var), cutoff = 0, digits=4)\n\nTo access principle component scores, we do:\n\npca$score\n\nIf we are performing other statistical analysis, we might want to choose how many of the new principle components we want to use. A scree-plot shows the percentage of variance each component explains.\n\nscreeplot(pca, type='l', main=\"\")\n\n\nTo choose the amount of components to use, we look for the “elbow” in the plot - basically when adding another additional PC does not really increase the amount of variance explained significantly anymore.",
    "crumbs": [
      "Latent Variable Models",
      "Principle Components Analysis"
    ]
  }
]