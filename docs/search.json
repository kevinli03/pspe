[
  {
    "objectID": "randomisation.html",
    "href": "randomisation.html",
    "title": "The Magic of Randomisation",
    "section": "",
    "text": "This chapter covers how randomisation solves the problem of selection bias, and why randomisation is considered the “gold standard” of causal inference.\n\nLet us say we are interested in this question:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Performance (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nOur concern is a confounder. For example, smartness of an individual could mean they are more likely to get a scholarship. Since smart people tend to perform well at university, that means the people who get treated are different from those who don’t get treated.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Performance (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmartness is not the only confounder. Other confounders could be family income, athletic ability, etc.\n\nBut what if randomness (like flipping a coin) controls who gets the treatment or not. A coin will be flipped to decide if every person in our study will get the scholarship. This means that the randomness (the coin), and not the confounder, are causing selection into treatment:\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Performance (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;Y\n\n\n\n\n\n\nO\n\nCoin\n\n\n\nO-&gt;D\n\n\n\n\n\n\n\n\n\n\n\nSince the confounder is no longer causing who gets the treatment and who doesn’t, that means there is no more concern of selection bias.\n\nRandomisation also means that every individual has the same chance of being treated or untreated, so the two groups will, on average, the same as each other. That means:\n\\[\n\\textcolor{purple}{\\mean Y_\\text{untreated}^{(0)}} = \\textcolor{red}{\\mean Y_\\text{treated}^{(0)}}, \\text{ so correlation} = \\text{causation}\n\\]\n\nThis is established by the law of large numbers, but it is a little technical for here.\n\nSo if our treatment is randomly assigned (individuals randomly assigned to treatment or control), correlation does equal causation.\n\nRandomisation is the gold standard of causal inference. There is no better method.\n\nRandomisation is possible if you are running your own experiment: you can use a random number generator to assign treatment.\nRandomisation is also possible if there is something that is being randomly assigned in the real world. For example, the US green card lottery randomly chooses who gets accepted.\n\nHowever, randomisation is not always possible to due to cost of running experiments, non-compliance of individuals within experiments, and impracticality.\n\nNon-compliance is an issue that can be solved pretty easily with an instrumental variable, given a few assumptions about the non-compliance.",
    "crumbs": [
      "Home",
      "Magic of Randomisation"
    ]
  },
  {
    "objectID": "latent/pca.html",
    "href": "latent/pca.html",
    "title": "Principle Components Analysis",
    "section": "",
    "text": "Note: PCA is not a latent variable model itself, but can be used to approximate a latent variable model.\n\nPrinciple components analysis (PCA) takes \\(p\\) number of observed variables (called features), and change them into a \\(p\\) number of new variables called principle components (PC), without losing any variance/information.\n\nWithout losing information/variance means the total variance of features equals total variance of PCs\n\nEssentially, what PCA does is it takes our features, and finds the “axis” in which there is the most variation, and makes that the 1st PC. Then it finds the “axis” with the 2nd most variation, and makes that the 2nd PC, and so on.\n\n\n\n\n\n\nNote that each principle component is orthogonal/uncorrelated with each other by design.\n\nThe 1st PC explains the most variance in our features, then the 2nd PC, and so on.\n\\[\nVar(\\text{PC}_1) ≥ Var(\\text{PC}_2) ≥ \\dots &gt; Var(\\text{PC}_p)\n\\]\nEach new principle component is a weighted average of the observed variables:\n\\[\n\\begin{align}\n\\text{PC}_1 & = a_{11}\\text{feature}_1 + a_{21}\\text{feature}_2 + \\dots + a_{p1}\\text{feature}_p \\\\\n\\text{PC}_2 & = a_{12}\\text{feature}_1 + a_{22} \\text{feature}_2 + \\dots + a_{p2}\\text{feature}_p \\\\\n& \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots\\\\\n\\text{PC}_p & = a_{1p}\\text{feature}_1 + a_{2p} \\text{feature}_2 + \\dots + a_{pp}\\text{feature}_p \\\\\n\\end{align}\n\\]\nThe \\(a\\)’s are the weights of each observed feature in creating a principle component, and are determined by the correlation matrix of the observed features. For interpreting a principle component, we often “normalise” the weights \\(a\\) to get the correlation between a observed feature and a principle component:\n\\[\nCorr(\\text{feature}_i, \\text{PC}_j) = \\sqrt{Var(\\text{PC}_j)} \\cdot a_{ij}\n\\]\n\nThis value is also called a component loading. Note that the right side is only equal to the correlation given we perform PCA on the correlation matrix (which is standard, but you can use a covariance matrix).\n\nInterpreting the principle components is identical to that of factor analysis - just using these correlations rather than factor loadings. Just like factor analysis, we can also calculate principle component scores, which are the individual values of each principle component for each individual in our data.\n\n\n\n\n\n\nExample of Interpretation\n\n\n\n\n\nBelow, the rows are observed features describing how much an individual trusts different institutions on a scale of 1-10. The columns are the principle components.\n\nIn component 1, we see that the correlation is pretty high for all of the variables, and positive. We might conclude component 1 measures general trust in institutions.\nIn component 2, we can see that the loadgins for legal and police are the highest (in absolute terms) and negative. Meanwhile, politicians, pol_parties, and EP are positive and still not too small. The other loadings are quite small. We might interpret this component as sort of a tradeoff between political trust and legal/law enforcement trust, with higher values indicating more political trust, and lower values indicating more trust in legal/police.\nIn component 3, we see positive loadings for everything but EP and UN. This might measure the tradeoff between trust in national and international institutions, with higher values for trust in national institutions, and lower values indicating more trust in international institutions.\n\n\n\n\nTo implement principle components analysis, we do the following:\n\npca &lt;- princomp(~ X1 + X2 + X3,\n                data = my_data,\n                cor = TRUE, #use correlation matrix\n                scores = TRUE, #calculate pc scores\n                na.action=na.exclude)\nsummary(pca)\n\nNow, to get the component loadings/corelation, we do the following:\n\n# grab the weights\nweights &lt;- loadings(pca)\n\n# grab the sqrt of variance\nsqrt.var &lt;- pca$sdev\n\n# calculate component loadings/correlation\nprint(t(t(weights)*sqrt.var), cutoff = 0, digits=4)\n\nTo access principle component scores, we do:\n\npca$score\n\nIf we are performing other statistical analysis, we might want to choose how many of the new principle components we want to use. A scree-plot shows the percentage of variance each component explains.\n\nscreeplot(pca, type='l', main=\"\")\n\n\nTo choose the amount of components to use, we look for the “elbow” in the plot - basically when adding another additional PC does not really increase the amount of variance explained significantly anymore.",
    "crumbs": [
      "Latent Variable Models",
      "Principle Components Analysis"
    ]
  },
  {
    "objectID": "latent/identify.html",
    "href": "latent/identify.html",
    "title": "Identification and Rotation",
    "section": "",
    "text": "Make sure you have read all the previous pages on factor analysis before this.\n\nWhen estimating factor analysis models, we encounter many identification issues. An identification issue is basically an inability to calculate a solution. There are two types of identification issues:\n\nWe have too many possible solutions, and we cannot determine which one (a unique identification issue).\nWe don’t have enough data to compute any solutions.\n\nOne identification issue is rotation. For example, imagine our latent variable is left-right political spectrum leaning. Our latent variable could use positive values for right-wing and negative values for left-wing. Or, it could do the opposite. There is no real substantive difference.\nThus, in 1-factor models, most software will arbitrarily choose a rotation, as it does not really make a huge difference.\nIn multiple factor models, rotation becomes more complex. Because of some linear algebra, you can have almost infinite rotations of factors that make no difference. Just like in the 1-factor model, software will arbitrarily choose one rotation.\nHowever, in multiple factor models, the chosen rotation can make a big difference in how easy it is to interpret our solutions.\n\nThe default rotation is often an orthogonal rotation. This means the factors (if visualised in space) are perpendicular to each other, with zero correlation.\nAn oblique rotation is one where the factors are allowed to be correlated with each other (non-perpindicular).\n\n\n\n\n\n\nThe oblique rotations are generally easier to interpret. This is because they make some items have close to 0 factor loadings, which means we can easily say that factor is not measuring that specific item.\nRotations are a unique identification issue. Factor analysis also has some lack-of-parameters identification issues that result in no possible solutions. To avoid this, we should generally have at least 3 items for a 1-factor model, and 2 items per factor for a multiple-factor model.",
    "crumbs": [
      "Latent Variable Models",
      "Identification and Rotation"
    ]
  },
  {
    "objectID": "latent/cluster.html",
    "href": "latent/cluster.html",
    "title": "Cluster Analysis",
    "section": "",
    "text": "Cluster analysis is a method of finding groups/clusters of similar individuals in our dataset, based on a set of variables. We choose the number of clusters we want to divide our data into, and each individual will be assigned to a specific cluster.\n\n\n\n\n\n\nAbove is an example of a cluster anlaysis with 2 clusters. Latent Class Models can also be considered to be a form of cluster analysis.\n\nThe primary way in which cluster anlaysis is conducted is by K-means clustering. This form of clustering focuses on the distance between different individuals in our dataset. The idea is that a good clustering will have as small within-cluster variation as possible.\n\nIn other words, you want individuals in the same cluster to be similar.\n\nThus, K-means clustering tries to minimise the squared euclidean distance between all the points within a cluster and the centroid (“center”) of the cluster:\n\\[\n\\text{distance} (\\text{cluster}_k) = \\sum\\limits_{i \\in \\text{cluster}_k} || \\b x_i - \\mean{\\b x}_k||^2\n\\]\n\nWhere \\(\\b x_i\\) is the vector of variable values of any specific unit within a cluster \\(k\\), \\(\\mean{\\b x}_k\\) is a vector of the mean variable values for all units in cluster \\(k\\), and the two vertical bars indicate the euclidean distance. We generally want to standardise all of our variables before starting to avoid measurement scale issues.\n\nHow do we minimise \\(\\text{distance}(\\text{cluster}_k)\\)? The K-means clustering algorithm works like this:\n\nChoose how many total clusters we want.\nRandomly assign each unit to one of the clusters.\nCalculate the centroids of each cluster.\nRe-assign each individual to the cluster whose centroid is closest to that individual.\nKeep repeating steps 3 to 4 until all units are in the cluster whose centroid is closest to that individual.\nRepeat the whole process a few times to find the global optimum.\n\n\nK-means clustering is algorithmic, not model based (unlike the very similar Latent Class models). This means that K-means does not have model summary statistics like AIC, or significance tests.\n\nTo interpret the clusters, we have to use our own field expertise. We look at the individuals in each cluster, as well as their mean variable values, and try to assign meaning to them.\n\n\n\n\n\n\nOther Clustering Methods\n\n\n\n\n\nK-means is the most common, but not the only way to cluster. Other methods include:\n\nLatent Class Models, which we covered previously. They technically count as cluster analysis, since we are assinging individuals to a category of a latent variable. This is a model-based method.\nGaussian Mixture Modelling: this is a variation of latent class models that deals with continuous observed items, but categorical variables.\nHierarchical Cluster Anlaysis: This basically starts with each unit in its own cluster, before merging clusters that are similar. Then it repeats that continuously. We can choose to analyse any specific cluster set within this process.\n\n\n\n\n\nTo implement cluster anlaysis, we should first start by standardising our variables.\n\nvars &lt;- c(\"X1\", \"X2\", \"X3\") #names of variables to cluster by\n\n# Standardise function\nstandard &lt;- function(x){\n  (x-mean(x))/sd(x)\n}\n\n# apply standardisation\nmy_data.z &lt;- global[,c(\"Unit Names Variable\", vars)]\nmy_data.z[,vars] &lt;- sapply(my_data.z[,vars], standard)\ncolnames(my_data.z)[-1] &lt;- vars.z\ndta &lt;- merge(my_data, my_data.z, by=\"Unit Names Variable\")\n\nThen, we can implement cluster anlaysis:\n\nset.seed(1236) #set seed\n\nres &lt;- kmeans(dta[,varsz],\n                  4, #change to number of clusters\n                  nstart = 10) #number of times to run algorithm\nprint(res)\n\nWe can access each individual’s assigned cluster with the following:\n\nres$cluster\n\n\nYou can save this vector back into your original dataset for further anlaysis.",
    "crumbs": [
      "Latent Variable Models",
      "Cluster Analysis"
    ]
  },
  {
    "objectID": "latent/latent.html",
    "href": "latent/latent.html",
    "title": "Latent Variable Models",
    "section": "",
    "text": "A latent variable model connects a unobserved variable (latent factor) with a few observed variables (items) that are considered imperfect measures of the latent factor.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nLatent Factor (F)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\n\nFor example, maybe we want to measure the political ideology of a senator. We cannot directly observe the political ideology (the latent factor), but we can observe how they vote on different bills (the items)\n\nLatent variable models assume we can model the relationship between each observed item and the latent factor with some sort of regression model. The coefficient of each regression (often denoted \\(\\lambda\\)) is the relationship between each item and the factor.\n\nFor example, in the figure above, each item (X1, X2, X3) is related to the factor (F) with by a \\(\\lambda\\) coefficient.\n\nThese \\(\\lambda\\) are called factor loadings. We can interpret the estimated factor loadings \\(\\widehat\\lambda\\) to interpret what the unobserved factor actually is measuring.\n\nFor example, if a factor has a strong relationship with one item, and a weaker relationship with another item, we might conclude that the factor measures the first item more than the second item.\n\nWe can also use latent variable models to create factor scores \\(\\widetilde F_i\\), which are basically actual values of the latent variable for each individual \\(i\\) in our data. This allows us to use the latent variable in other statistical models.\nThe choice of latent variable model depends on the type of items/factors:\n\n\n\n\n\n\n\n\nModel\nFactor Type\nItem Type\n\n\nFactor Analysis\nContinuous\nContinuous\n\n\nItem Response Theory\nContinous\nCategorical/Binary\n\n\nStructural Class Models\nCategorical/Binary\nCategorical/Binary\n\n\n\n\nSo far, this introduction has assumed just one latent variable factor. However, all models allow for multiple factors as well. If we have many items, some items might only measure on factor, others both factors.\nEach latent model also has the ability to conduct confirmatory analysis. This is basically when we, based on theoretical reasons (such as reading the literature), impose certain restrictions on models, and test if this hypothesised model is good.\n\nThis typically takes the form of setting certain factor loadings \\(\\lambda\\) to 0, meaning a certain item does not measure one of the factors.\n\nWe can also combine different structural models together to form structural equation models (see the structural equation models part).",
    "crumbs": [
      "Latent Variable Models"
    ]
  },
  {
    "objectID": "latent/multiplefac.html",
    "href": "latent/multiplefac.html",
    "title": "Multiple Latent Factors",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nSo far, we have covered factor analysis for a single factor. However, factor analysis can work with multiple latent factors.\n\n\n\n\n\n\n\n\nexample2\n\n\nF1\n\nLatent Factor (F1)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF1-&gt;X3\n\n\nλ\n\n\n\nF2\n\nLatent Factor (F2)\n\n\n\nF2-&gt;X2\n\n\nλ\n\n\n\nF2-&gt;X3\n\n\nλ\n\n\n\nX4\n\nItem 4 (X4)\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\n\n\n\n\n\n\nWe can see in the above figure, an example of two latent factors. Each item can measure either only one or two of the factors. The factors can also both have the same items, but put emphasis on different items.\nLet us say we have multiple factors. We still assume the factors are normally distributed:\n\\[\n\\text{factors} \\sim \\mathcal N(\\b \\kappa, \\b\\Phi)\n\\]\n\nWhere \\(\\b\\kappa\\) is a vector of all the means of each factor, and \\(\\b\\Phi\\) is a variance-covariance matrix of all the factors.\n\nUsing our conventional identification assumption like in single factor analysis, we will assume each factor is a standard normal \\(\\mathcal N (0, 1)\\). This implies that \\(\\b\\kappa = 0\\).\nOur variance matrix \\(\\b\\Phi\\) is a little more complicated - the variances of each factor is 1 (as assumed in a standard normal), however, the matrix \\(\\b\\Phi\\) also includes the covariances between factors. This is an additional thing that we will need to estimate that was not present in one-factor models.\n\nFor example, factor 1 and 2 might be correlated with each other, which is reflected in \\(\\b\\Phi\\).\n\nNow on to our measurement models. Just like in single factor analysis, we use a linear regression to express the relationship between any item \\(i\\) and the factors. However, this time, each regression will relate each item to all factors:\n\\[\n\\begin{align}\n\\text{item}_1 & = \\tau_1 + \\lambda_{11}\\ \\text{factor}_1 + \\lambda_{12}\\ \\text{factor}_2 + \\dots + \\delta_1 \\\\\n\\text{item}_2 & = \\tau_2 + \\lambda_{21}\\ \\text{factor}_1 + \\lambda_{22}\\ \\text{factor}_2 + \\dots + \\delta_2 \\\\\n& \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\\\\n\\text{item}_p & = \\tau_p + \\lambda_{p1}\\ \\text{factor}_1 + \\lambda_{p2}\\ \\text{factor}_2+ \\dots + \\delta_p\n\\end{align}\n\\]\nThe factor loadings \\(\\lambda\\) are still the relationship between each factor and each item, and are interpreted in the same way as a 1-factor model. However, the the interpretation of communality and relaibility are no longer valid with more than 1-factor.\n\nWhen interpreting - do one factor at a time. Start with one, then go to the next. Interpreting each factor is the same as shown in the page on factor analysis.\n\n\nTo implement multiple factor anlaysis, the procedure is quite similar to standard factor analysis. First, we will need the psych and GPArotation package:\n\nlibrary(psych)\nlibrary(GPArotation)\n\nFirst, we should get rid of missing observations:\n\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\nFor factor analysis with multiple factors, the notation is as follows:\n\nfa &lt;- fa(data[,items], nfactors=2, fm=\"ml\", rotate=\"oblimin\")\nprint(fa)\n\n\nChange nfactors=2 to however many factors you want. Note that you cannot have too many, this will be discussed in the next page on identification.\nThis code also uses the “oblimin” rotation, which will be discussed in the next page on identification and rotation.",
    "crumbs": [
      "Latent Variable Models",
      "Multiple Latent Factors"
    ]
  },
  {
    "objectID": "latent/sem.html",
    "href": "latent/sem.html",
    "title": "Structural Equation Models",
    "section": "",
    "text": "Structural Equation Models (SEM) allow us to combine measurement models (ex. factor anlaysis) with Linear regression models between the latent variables.\n\n\n\n\n\n\n\n\nexample2\n\n\nF1\n\nLatent Explanatory Variable\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF1-&gt;X3\n\n\nλ\n\n\n\nF2\n\nLatent Response Variable\n\n\n\nF1-&gt;F2\n\n\nLinear Model\n\n\n\nX4\n\nItem (Y1)\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\nX5\n\nItem (Y2)\n\n\n\nF2-&gt;X5\n\n\nλ\n\n\n\nX6\n\nItem (Y3)\n\n\n\nF2-&gt;X6\n\n\nλ\n\n\n\n\n\n\n\n\n\n\nThis is an example of a very simple structural equation model.\n\nThis structural equation model contains a linear regression between the independent latent variable explaining the dependent latent variable.\n\\[\n\\text{response} = \\alpha +  \\beta\\ \\text{explanatory} + \\eps\n\\]\nThe independent latent variable \\(\\xi\\) and dependent variable \\(\\eta\\) explained by measurement models (typically a factor analysis model):\n\\[\n\\begin{align}\n\\b{\\text{items}}_x & = \\b\\tau_x + \\b\\lambda_x \\ \\text{explanatory} + \\b\\eps_x \\\\\n\\b{\\text{items}}_y & = \\b\\tau_y + \\b\\lambda_y \\ \\text{response} + \\b\\eps_y\n\\end{align}\n\\]\n\nWhere \\(\\b{\\text{items}}_x\\) are a vector of items the are observed for the latent explanatory variable, and \\(\\b{\\text{items}}_x\\) are a vector of items the are observed for the latent response variable. Vectors \\(\\b\\tau\\) and \\(\\b\\lambda_x\\) are the coefficients for each item’s latent measurement model.\n\nThe interpretations of the model are quite straight forward - the measurment models are interpreted in the same way as factor analysis, and the linear model between latent variable is interpreted the same way as a linear model between any other variables.\n\nFor example, the linear model says that as explanatory increases by 1, response increases by an expected \\(\\beta\\) units.\n\nStructural models don’t have to have just one dependent and independent latent variable. We can have multiple dependent and independent variables, multiple linear models, and also measure correlations within the dependent and independent variables:\n\n\n\n\n\n\n\n\nexample2\n\n\nX1\n\nLatent Independent Variable 1\n\n\n\nX2\n\nLatent Independent Variable 2\n\n\n\nX1-&gt;X2\n\n\n\nCorrelation\n\n\n\nY1\n\nLatent Dependent Variable 1\n\n\n\nX1-&gt;Y1\n\n\nModel\n\n\n\nY2\n\nLatent Dependent Variable 2\n\n\n\nX1-&gt;Y2\n\n\nModel\n\n\n\nX2-&gt;Y1\n\n\nModel\n\n\n\nX2-&gt;Y2\n\n\nModel\n\n\n\nY1-&gt;Y2\n\n\n\nCorrelation\n\n\n\n\n\n\n\n\n\n\nNote: we cannot have linear models between two independent, or two dependent variables. They have to be seperate - within each group, only correlations are possible.\n\nIn this example, we are measuring the correlation between the independent variables, the correlation between the dependent variables, and we have two regression models:\n\\[\n\\begin{align}\n\\text{response}_1 & = \\beta_0 + \\beta_1\\ \\text{explanatory}_1 + \\beta_2\\ \\text{explanatory}_2 + \\eps_1 \\\\\n\\text{response}_2 & = \\gamma_0 + \\gamma_1\\ \\text{explanatory}_1 + \\gamma_2\\ \\text{explanatory}_2 + \\eps_2 \\\\\n\\end{align}\n\\]\n\nAnd each response and latent variable all have their own measurement models.\n\n\nTo implement structural equation models, we use the lavaan package:\n\nlibrary(lavaan)\n\nThen, we have to specify the relationships between variables we want to fit in our structural model - including measurement models, regression models, and correlations:\n\nmodel &lt;- '\n# Measurement models \n  F1 =~ NA*X1 + X2 + X3\n  F2 =~ NA*X4 + X5 + X6\n    F3 =~ Y1 + Y2 + Y3\n# Regressions\n  F3 ~ F1 + F2\n\n# Covariances\n  F1 ~~ F2\n\n# Fixing the variances of independent variables at 1\n  F1~~1*F1\n  F2~~1*D2\n'\n\n\nNote how F3 does not have NA*, and F3 doesn’t have its variance fixed at 1 at the end. Meanwhile, F1 and F2 do. This is standard to fix independent variables with a variance of 1, and dependent variables are allowed to be “free”.\n\nNow, we estimate our specified model:\n\nsem &lt;- sem(model,\n           data = my_data,\n           missing=\"FIML\")\nsummary(sem)\n\n\nThe missing argument tells the software to keep observations with missing values (this is typically a good thing). But it can take longer, so you can delete this argument for it to use only complete observations.",
    "crumbs": [
      "Latent Variable Models",
      "Structural Equation Models"
    ]
  },
  {
    "objectID": "soo/genetic.html",
    "href": "soo/genetic.html",
    "title": "Genetic Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red). However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{purple}{Y^{(0)}_\\text{Matched individual}}\n\\]\nLike distance matching, genetic matching matches an individual that is treated (like Mia) with one that is not treated based on how close their confounding values are. However, genetic matching uses a slightly different variation of mahalanobis distance:\n\\[\n\\text{distance}_{i, j}(\\b W) = \\sqrt{(\\b x_i - \\b x_j)' \\ (\\b\\Sigma_x^{-\\frac{1}{2}})' \\ \\b W \\ \\b\\Sigma_x^{-\\frac{1}{2}}  (\\b x_i - \\b x_j)}\n\\]\n\nWhere \\(i\\) and \\(j\\) are two units we want to measure the distance between, \\(\\b x\\) are their confounder values, and \\(\\b\\Sigma_x\\) is the covariance matrix of confounders. \\(\\b W\\) is a weights matrix.\n\nThe weights \\(\\b W\\) are estimated to make the treated and untreated groups as similar as possible. This balance between treated and untreated eliminates selection bias. Then, matching is done with the units that have the smallest distance.\n\nBefore you start genetic matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the Matching and MatchIt package.\n\nlibrary(Matching)\n\nFirst, we need to estimate the propensity scores with a logistic regression.\n\nIt is recommended to use the propensity score as one of the controls on which to genetic match on.\n\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity,\n                          type = \"response\")\n\nThen, we use the GenMatch() function to estimate a weights matrix \\(\\b W\\):\n\nset.seed(333) #any number works\ngen &lt;- GenMatch(Tr = my_data$D,\n                    X = my_data[,c(\"X1\",\"X2\",\"pscore\")],\n                    BalanceMatrix = my_data[,c(\"X1\",\"X2\")],   \n                    estimand = \"ATT\",\n                    M = 2,\n                    replace = TRUE,\n                    ties = FALSE,\n                    distance.tolerance = 0,\n                    print.level = 0,\n                    pop.size = 200)\n\n\nYou can increase pop.size to increase the accuracy - but it will increase the time and computational power needed.\n\nNow, let us conduct estimation with genetic matching:\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,c(\"X1\",\"X2\",\"pscore\")],\n             estimand = \"ATT\",\n             M = 2,\n             replace = TRUE,\n             ties = FALSE,\n             distance.tolerance = 0,\n             Weight.matrix = gen$Weight.matrix,\n             Weight = 3)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Genetic Matching"
    ]
  },
  {
    "objectID": "soo/pscore.html",
    "href": "soo/pscore.html",
    "title": "Propensity Score Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red). However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{purple}{Y^{(0)}_\\text{Matched individual}}\n\\]\nPropensity Score Matching matches an individual that is treated (like Mia) with one that is not treated based on how similar their likelihoods of treatment are.\nWhat is a likelihood of treatment? Well we know confounders cause people to get the treatment or not treatment. Thus, using an individual’s confounder values, we can estimate their likelihood of getting treatment, called a propensity score.\n\\[\n\\text{propensity score } \\pi =Pr(\\text{you get treated})\n\\]\nPropensity scores are typically estimated with a logistic regression. This also means that propensity score matching shares the same weaknesses of logistic regression - including assuming linear relatinoships between confounders and propensities, and only being unbiased in large sample sizes.\n\nBefore you implement propensity score matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the Matching package.\n\nlibrary(Matching)\n\nFirst, we need to estimate the propensity scores with a logistic regression:\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity,\n                          type = \"response\")\n\n\nA random forest model is also possible, but less common.\n\nNow, we can implement the matching as follows.\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,\"pscore\"],\n             M = 1,\n             BiasAdjust = TRUE,\n             Weight = 2)\nsummary(att)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Propensity Score Matching"
    ]
  },
  {
    "objectID": "soo/soo.html",
    "href": "soo/soo.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "Our issue in causal inference is that a confounder is causing pre-existing differences:\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nReceiving Scholarship\n\n\n\nY\n\nUniversity Grades\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nIntellegence (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nBy definition, as the confounder changes, your likelihood of getting treatment changes. As the confounder changes, the outcome value will also change.\nThese issues occur when the confounder changes in value. So what if we hold the confounders constant? Then, there would be no changes in confounders - so the variation in treatment assignment and outcomes cannot be attributed to the confounder.\nFor example, let’s assume that intellegence has two values: smart and dumb. Let us calculate the treatment effect within each level of intellegence:\n\\[\n\\tau_\\text{smart} = \\purple{\\mean Y_\\text{smart}^{(1)}} - \\purple{\\mean Y_\\text{smart}^{(0)}}\n, \\quad \\tau_\\text{dumb} = \\purple{\\mean Y_\\text{dumb}^{(1)}} - \\purple{\\mean Y_\\text{dumb}^{(0)}}\n\\]\nThe confounder is constant here, so no selection bias. Thus, within each category, correlation is equal to causation. Our overall causal effect will be a weighted average of the categories:\n\\[\n\\tau = \\tau_\\text{smart} Pr(\\text{smart})  \\ + \\ \\tau_\\text{dumb} Pr(\\text{dumb})\n\\]\n\nThe weights of this weighted average are the probability/frequency of that value of the confounder.\n\nObviously, most confounders have more than 2 categories, and we often have more confounders. But the same intuition applies. Given a set of confounders \\(\\set X\\), we first calculate the treatment effect within a specific vector of confounder values \\(\\b x\\):\n\\[\n\\tau(\\b x) = (\\T - \\C)| \\b x\n\\]\nAnd then, we find the weighted average for the total effect, with the weights being the frequency of that specific vector of confounder values:\n\\[\n\\tau_{ATE} = \\sum \\tau(\\b x) \\cdot Pr(\\b x)\n\\]\nFor selection on observables to work, we need to meet 3 assumptions:\n\n\n\n\n\n\n\nAssumption\nDescription\n\n\nConditional Ignorability\nThis means that we must account for all possible confounders (cannot miss a single one).\n\n\nCommon Support\nThis means no one can have a 100% chance of being in treatment or control. They always have a chance to be in either, no matter their confounding values.\n\n\nStable Unit Treatment Value Assumption (SUTVA)\nThis means that if Ava is treated, that does not affect Mia’s outcome (and for any other 2 individuals).\n\n\n\n\nWe have a wide choice of estimators that we can use. Use the sidebar or links in the table to access each estimator’s page.",
    "crumbs": [
      "Selection on Observables"
    ]
  },
  {
    "objectID": "soo/distance.html",
    "href": "soo/distance.html",
    "title": "Distance Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red). However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{purple}{Y^{(1)}_\\text{Mia}} - \\textcolor{purple}{Y^{(0)}_\\text{Matched individual}}\n\\]\nDistance matching matches an individual that is treated (like Mia) with one that is not treated based on how close their confounding values are. We define closeness by Mahalanobis distance:\n\\[\n\\text{distance}_{i, j} = \\sqrt{(\\b x_i - \\b x_j)' \\ \\b\\Sigma_x^{-1} (\\b x_i - \\b x_j)}\n\\]\n\nWhere \\(i\\) and \\(j\\) are two units we want to measure the distance between, \\(\\b x\\) is a vector of confounder values, and \\(\\b\\Sigma_x\\) is the covariance matrix of confounders.\n\nBecause distance matching depends on finding matches in a n-dimensional space, it is subject to the curse of dimensionality. This essentially means that the more confounders you have, the more dimensions you have to match over, and the harder it is to find good matches. So we typically do not use any more than 3-5 confounders with distance matching.\n\nBad matches means incorrectly using someone’s counterfactual, resulting in bad estimates.\n\n\nBefore you implement distance matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the Matching package.\n\nlibrary(Matching)\n\nNow, we can implement the matching as follows.\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,c(\"X1\",\"X2\", \"X3\")],\n             M = 1,\n             BiasAdjust = TRUE,\n             Weight = 2)\nsummary(att)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Distance Matching"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Basics of Causality",
    "section": "",
    "text": "This page covers how confounders cause pre-existing differences between treated and untreated (selection bias), meaning correlation is not causation.\n\nLet us look at this causal question:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nGoing to the Hospital (D)\n\n\n\nY\n\nHealth Outcomes (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nWe have a treated group (went to hospital), and an untreated group. Using our potential outcomes framework, we can define the treatment effect of the treated group:\n\\[\n\\tau_\\text{treated} = \\textcolor{purple}{\\mean Y_{\\text{treated}}^{(1)}} - \\textcolor{red}{\\mean Y_{\\text{treated}}^{(0)}}\n\\]\n\nIn red is the counterfactual we do not observe.\n\nNow compare the treatment effects above to correlation, which is defined as the difference in observed outcomes:\n\\[\n\\begin{align}\n\\text{correlation} & = \\mean Y_\\text{treated} - \\mean Y_\\text{untreated} \\\\\n& = \\textcolor{purple}{\\mean Y_{\\text{treated}}^{(1)}} - \\textcolor{purple}{\\mean Y_\\text{untreated}^{(0)}}\n\\end{align}\n\\]\nIf we compare this correlation to our \\(\\tau_\\text{treated}\\), we see:\n\\[\n\\text{if  } \\textcolor{purple}{\\mean Y_\\text{untreated}^{(0)}} ≠ \\textcolor{red}{\\mean Y_\\text{treated}^{(0)}}, \\text{ then }  \\tau_\\text{treated} ≠ \\text{correlation}\n\\]\n\nThese two quantities are potential outcomes under control, or in another way to think of it, outcomes of the two groups prior to treatment happening.\n\nThus, if there is a difference between the average outcomes between treated and untreated before treatment is administered, then correlation is not equal to causation. This is because we cannot tell if the difference between the groups is due to treatment, or due to their pre-existing differences.\n\nWhat causes pre-existing differences? Confounders. For example, in our hospital-health example, a confounder could be smoking.\n\nSmoking is not the only possible confounder, we just use it as an example. Drinking, age, etc. are all other potential confounders.\n\nSmoking will worsen health outcomes. Someone who smokes is also more likely to visit the hospital with health complications. That means people who go to the hospital start out with (on average) worse health outcomes than people who did not go to the hospital.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nGoing to the Hospital (D)\n\n\n\nY\n\nHealth Outcomes (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmoking (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA confounder is a third variable that has the following characteristics:\n\nThe confounder is correlated (positive or negative) with the outcome variable.\nThe confounder causes who gets and doesn’t get the treatment.\nThe confounder is not itself caused by the treatment\n\n\n\n\n\nNote requirement 3 - it is a common mistake. Any result of the treatment \\(D\\) cannot be a confounder.\n\nConfounders cause pre-existing differences, which cause correlation to not equal causation. We must account for confounders to uncover causal effects.",
    "crumbs": [
      "Home",
      "Issue of Selection Bias"
    ]
  },
  {
    "objectID": "frameworks.html",
    "href": "frameworks.html",
    "title": "Basics of Causality",
    "section": "",
    "text": "This page covers the potential outcomes framework and the causal estimands.\n\nIn causal inference, we are interested in causal questions:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nTreatment (D)\n\n\n\nY\n\nOutcome (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\n\nWe generally assume the treatment \\(D\\) is binary.\n\nImagine we have two hypothetical parallel worlds that are copies of each other. Both of these worlds are identical except for one aspect: the treatment:\n\n\n\n\n\n\n\n\nParallel World\nTreatment\nPotential Outcome\n\n\nKevin does not Receive Treatment\n\\(D_\\text{Kevin} = 0\\)\n\\(\\purple{Y_\\text{Kevin}^{(0)}}\\)\n\n\nKevin Receives Treatment\n\\(D_\\text{Kevin} = 1\\)\n\\(\\purple{Y_\\text{Kevin}^{(1)}}\\)\n\n\n\n\nThe only difference between the two worlds is the treatment. Thus, any difference in outcomes between the two worlds must be the causal effect of the treatment.\n\\[\n\\tau_\\text{Kevin} = \\purple{Y_\\text{Kevin}^{(1)}} - \\purple{Y_\\text{Kevin}^{(0)}}\n\\]\n\nTechnically, we need another assumption, SUTVA, for this to be true. I will explain this assumption as part of the identification assumptions.\n\nHowever, in reality, we do not have two parallel worlds. Thus, by definition, one of the potential outcomes is not observed in our real world - the counterfactual.\n\n\n\n\n\n\n\n\nIn the Real World\nObserved Outcome\nCounterfactual\n\n\nKevin receives treatment (treated)\n\\(Y_\\text{Kevin}= \\purple{Y_\\text{Kevin}^{(1)}}\\)\n\\(\\red{Y_\\text{Kevin}^{(0)}}\\)\n\n\nKevin did not receive treatment (untreated)\n\\(Y_\\text{Kevin} = \\purple{Y_\\text{Kevin}^{(0)}}\\)\n\\(\\red{Y_\\text{Kevin}^{(1)}}\\)\n\n\n\n\nThe fundamental problem of causal inference is that in order to calculate our individual treatment effect \\(\\tau\\), we need both potential outcomes. Our goal is to estimate causal effects without observing counterfactuals. This is difficult at the individual level, so instead, we focus on average treatment effects for groups:\n\n\n\n\n\n\n\n\nGroup Effects\nNotation\nDefinition\n\n\nAverage Treatment Effect (ATE)\n\\(\\tau_\\text{ATE}\\)\nThe average treatment effects for all individuals in our study (treated and untreated).\n\n\nAverage Treatment Effect on the Treated (ATT)\n\\(\\tau_\\text{ATT}\\)\nThe average treatment effect but only for individuals who receive the treatment in our study.\n\n\nLocal Average Treatment Effect (LATE)\n\\(\\tau_\\text{LATE}\\)\nThe average treatment effect but only for a specific (local) group of individuals in a study.",
    "crumbs": [
      "Home",
      "Basics of Causality"
    ]
  },
  {
    "objectID": "did/did2s.html",
    "href": "did/did2s.html",
    "title": "Gardner (2021) Two-Stage Difference-in-Differences",
    "section": "",
    "text": "See Kyle Butts’ github page for more info on this estimator. Asjad Naqvi also has a helpful page.\n\nRecall our two-way fixed effects model:\n\\[\nY_{it} = \\alpha_i + \\gamma_t + D_{it}\\tau + \\b X_{it}' \\b\\beta + \\eps_{it}\n\\]\nThis also implies a model for the potential outcomes - the potential outcome under control is when \\(D=0\\), and the potential outcome under treatment is \\(D=1\\):\n\\[\n\\begin{align}\n\\purple{Y_{it}^{(1)}} & = \\alpha_i + \\gamma_t + \\blue\\tau + \\b X_{it}' \\b\\beta + \\eps_{it} \\\\\n\\purple{Y_{it}^{(0)}} & = \\alpha_i + \\gamma_t  + \\b X_{it}' \\b\\beta + \\eps_{it}\n\\end{align}\n\\]\n\nThese are obtained by simply plugging in \\(D=0,1\\) into the TWFE equation.\n\nGardner (2021) proposes to solve the issues with two-way fixed effects regarding “forbidden comparisons” and “weighting” problems by simply estimating the counterfactuals \\(\\red{Y_{it}^{(0)}}\\) for treated units in the post-period. With these counterfactuals, we can calculate the treatment effect directly without weighting or forbidden comparisons.\nTo estimate the counterfactuals, we first run a regression with only untreated/not-yet treated observations \\(D=0\\):\n\\[\nY_{it|D=0} = \\alpha_i + \\gamma_t + \\b X_{it}' \\b\\beta + \\eps_{it}\n\\]\nThis regression will provide estimates for \\(\\widehat\\alpha_i\\), \\(\\widehat\\gamma_t\\), and \\(\\widehat{\\b\\beta}\\) using only untreated data. This will allow us to estimate the \\(Y_{it}^{(0)}\\) potential outcomes for all units, including the counterfactuals for the treated units and the observed potential outcomes for untreated units.\n\\[\n\\blue{\\widehat Y_{it}^{(0)}} = \\widehat\\alpha_i + \\widehat\\gamma_t + \\b X_{it}' \\widehat{\\b\\beta}\n\\]\nNow, let us find the difference between our observed \\(Y_{it}\\) values in our dataset and our estimated \\(Y_{it}^{(0)}\\) potential outcomes:\n\\[\n\\begin{align}\n\\widetilde Y_{it} & = Y_{it} - \\blue{\\widehat Y_{it}^{(0)}} \\\\\n& = Y_{it} - \\left(\\widehat\\alpha_i + \\widehat\\gamma_t + \\b X_{it}' \\widehat{\\b\\beta} \\right)\n\\end{align}\n\\]\n\nFor untreated units \\(D=0\\), their observed \\(Y_{it}\\) is \\(\\purple{Y_{it}^{(0)}}\\), so \\(\\widetilde Y_{it}\\) is essentially measuring \\(\\purple{Y_{it}^{(0)}}- \\purple{Y_{it}^{(0)}}\\). This should be approximately 0.\nFor treated units \\(D = 1\\), their observed \\(Y_{it}\\) is \\(\\purple{Y_{it}^{(1)}}\\), so \\(\\widetilde Y_{it}\\) is essentially measuring \\(\\purple{Y_{it}^{(1)}}- \\red{Y_{it}^{(0)}}\\), or the individual causal effect.\n\nThus, with the full dataset, we run the following regression to obtain the causal effect:\n\\[\n\\widetilde Y_{it} = \\delta + D_{it}\\tau + \\epsilon_{it}\n\\]\n\nThis model implies that for \\(D = 0\\) units, \\(\\widetilde Y_{it} = \\delta \\approx 0\\). For \\(D = 1\\), then \\(\\widetilde Y_{it} = \\tau\\). Thus, with our intuition of \\(\\widetilde Y_{it}\\) from above, we can see that \\(\\tau\\) is measuring the average difference \\(\\purple{Y_{it}^{(1)}}- \\red{Y_{it}^{(0)}}\\) for treated units.\n\nThe estimated \\(\\widehat\\tau\\) is the estimate of the ATT, that should be free of the issues that TWFE faces in staggered difference-in-differences. The same mechanics can be used to calculate dynamic treatment effects for each time period.\n\nThe standard errors will be incorrect, so Gardner (2021) uses a generalised method of moments estimator to correct the standard errors.\n\n\nTo implement 2-stage difference-in-differences, we need the did2s package:\n\nlibrary(did2s)\n\nFor the estimation of the ATT, we do the following:\n\nmodel &lt;- did2s(data = my_data,\n               yname = \"Y\",\n               first_stage = ~ X1 + X2 | unit + period, #if no covariates, put \"0\"\n               second_stage = ~i(D), #i() makes it a factor variable\n               treatment = \"D\",\n               cluster_var = \"unit\")\nmodel\n\nFor estimation of dynamic treatment effects (and testing of parallel trends), we do the following:\n\nmodel &lt;- did2s(data = my_data,\n               yname = \"Y\",\n               first_stage = ~ X1 + X2 | unit + period, #if no covariates, put \"0\"\n               second_stage = ~i(time_to_treat, ref = c(-1, Inf)),\n               treatment = \"D\",\n               cluster_var = \"unit\")\nmodel\niplot(model) #to view dynamic treatment effects plot\n\n\nNote: the inf is the reference for the never-treated control group, for the variable time_to_treat. If it is NA’s, then you do not need to include it, just put ref = -1.",
    "crumbs": [
      "Difference-in-Differences",
      "Gardner (2021) 2-stage DiD"
    ]
  },
  {
    "objectID": "did/etwfe.html",
    "href": "did/etwfe.html",
    "title": "Wooldridge (2021) Extended Two-Way Fixed Effects",
    "section": "",
    "text": "There are two issues with the two-way fixed effects estimator: forbidden comparisons and issues with weighting. Wooldridge (2021, 2023) proposes a new estimator, the extended two-way fixed effects (ETWFE) estimator, that solves both of these issues.\n\nFirst, the ETWFE estimator estimates all possible heterogeneous effects separately, using only valid comparisons, solving the forbidden comparison problem.\nThen, these separate heterogeneous effects are manually aggregated together into one ATT using proper weighting based on how frequent each heterogeneous treatment effect occurs in our sample.\n\nWooldridge proposes to estimate all of the possible heterogenous treatment effects by simply adding a ton of interaction terms to the TWFE estimator to account for every possible heterogeneity. The two main forms of heterogeneity are heterogeneity between different initial treatment years, and heterogeneity between different levels of confounders.\n\\[\n\\begin{align}\nY_{it} = \\sum\\limits_{g} & \\sum\\limits_t \\blue{\\tau_{g,t}} \\cdot 1\\{i \\in g\\} \\cdot D_{it} \\\\\n+ & \\sum\\limits_g \\sum\\limits_t \\blue{\\tau_{g,t,j}} \\cdot 1\\{i \\in g\\} \\cdot D_{it} \\cdot X_{j, it} \\\\ + & \\  \\alpha_i + \\gamma_t + \\b X_{it} \\b\\beta + \\eps_{it}\n\\end{align}\n\\]\n\nThe summations basically mean include one interaction for each \\(g\\) (initial treatment year group), and each \\(t\\) (time periods) within each \\(g\\). The \\(1\\{\\}\\) are indicator functions that output a value of 1 if true, and value of 0 if not true. \\(i \\in g\\) means unit \\(i\\) is in the initial treatment year group of \\(g\\). The first line interacts treatment with initial group. The second line interacts treatment with initial group and covariates.\n\nThis model will produce many coefficients \\(\\tau_{g, t}\\) and \\(\\tau_{g, t, j}\\) that capture different heterogeneity in treatment effects. To obtain a single ATT, we will want to aggregate them together in a weighted average.\nWe can also aggregate them in terms of relative years to initial treatment, which allows us to compute dynamic treatment effects. However, this estimator can only compute post-treatment dynamic treatment effects.\nWooldridge’s 2023 paper also extends this framework to work with logistic regression, poisson regression, and negative binomial regression, which is useful when we deal with non-continuous outcome variables.\nWooldridge’s extended two-way fixed effects estimator is very similar to Sun and Abraham’s estimator. The three main differences are:\n\nSun and Abraham can calculate dynamic treatment effects for every time period (both pre and post treatment). Wooldridge’s can only calculate post-treatment. Thus, only Sun and Abraham can test for parallel trends.\nSun and Abraham is more strict with covariates for parallel trends, while Wooldridge’s allows for more flexibility and robustness to violations of parallel trends. This is because Wooldridge’s estimator contains far more interactions with covariates.\nWooldridge’s estimator allows for non-linear models (poisson, logistic, etc.).\n\n\nTo implement extended two-way fixed effects, we will need the etwfe package:\n\nlibrary(etwfe)\n\nTo estimate all the group-time ATTs, we do the following:\n\nmodel &lt;- etwfe(fml = Y ~ X1 + X2 + X3, #if no controls, put 0 after ~\n              tvar = period, #time period variable\n              gvar = initial_treat, #initial period of treatment for unit\n              data = my_data,\n              vcov = ~unit, # cluster se by unit\n              family = NULL) \n\n\nYou can delete the family argument if you are just using a standard linear model. You can also replace NULL with “logit”, “poisson”, and “negbin”.\n\nWe can then aggregate our group-time ATT’s. To obtain one single ATT, we can use the following code:\n\nemfx(model)\n\nWe can also get dynamic ATT’s for post-treatment periods:\n\ndynamic &lt;- emfx(model, type = \"event\")\nplot(dynamic)",
    "crumbs": [
      "Difference-in-Differences",
      "Wooldridge (2021) ETWFE"
    ]
  },
  {
    "objectID": "did/did.html",
    "href": "did/did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "In April 1992, New Jersey increased its minimum wage. We want to know how this affected employment rates. We have data from before and after the change:\n\n\n\n\n\n\n\nMar 1992 (Before)\nDec 1992 (After)\n\n\n20.44\n21.03\n\n\n\n\nSpecifically, these are average employment numbers in fast-food restaurants.\n\nWith this data, we cannot find the causal effect. Why? Well recall our definition of the causal effect - our treatment effect after the treatment in December 1992 would be:\n\\[\n\\tau_\\text{Dec92} = \\purple{Y^{(1)}_\\text{Dec92}} - \\red{Y^{(0)}_\\text{Dec92}}\n\\]\nWe don’t observe the counterfactual in December 1992 without the minimum wage increase. We need some way to estimate this.\nWhy can’t we pretend that the numbers from March 1992 are our counterfactual, since the treatment hadn’t occurred yet? Well, there could have been something that happened between these two times that changes the trend in employment (like a recession).\nDifference-in-differences estimates the trend by looking at another state that did not receive a change in minimum wage during this time - like Pennsylvania:\n\n\n\n\n\n\n\nMar 1992 (Pennsylvania)\nDec 1992 (Pennsylvania)\n\n\n23.33\n21.17\n\n\n\n\nThese are employment values in Pennsylvania, where no mininum wage changes occured.\n\nPennsylvania tells us the trend in employment without a change in minimum wage (-2.16). This trend tells us that if New Jersey hadn’t received a minimum wage increase, their employment rate would be the March 1992 level minus 2.16.\n\n\n\n\n\n\n\nMar 92 (NJ, Not Treated)\nDec 92 (NJ, Hypothetical Not Treated)\n\n\n20.44\n20.44 - 2.16\n\n\n\n\nThe March 1992 value is copied from the first table.\n\nNow, we have an approximation of New Jersey’s missing counterfactual, so we can now estimate the causal effect:\n\\[\n\\begin{align}\n\\tau_\\text{Dec92} &  = \\purple{Y^{(1)}_\\text{Dec92}} - \\red{Y^{(0)}_\\text{Dec92}} \\\\\n& \\approx \\purple{Y^{(1)}_\\text{Dec92}} - \\left( \\purple{Y^{(0)}_\\text{Mar92}}  - 2.16\\right)\n\\end{align}\n\\]\nThis entire idea of using another group’s trend to estimate counterfactuals depends on two assumptions:\n\n\n\n\n\n\n\nAssumption\nDescription\n\n\nParallel Trends\nThis means that had the treated group not received treatment, they would have followed the same trend as the control group.\n\n\nStable Unit Treatment Value Assumption (SUTVA)\nThis means that if Ava is treated, that does not affect Mia’s outcome (and for any other 2 individuals).\n\n\n\n\nI will show how to test for parallel trends later in Dynamic treatment effects.\n\nThere are two types of difference-in-differences designs - classical and staggered, which we will cover in the next few pages.",
    "crumbs": [
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did/staggered.html",
    "href": "did/staggered.html",
    "title": "Stagged Difference-in-Differences",
    "section": "",
    "text": "Ensure you understand the classical difference-in-differences design before starting this.\n\nIn the classical difference-in-differences design, all units in the treatment group receive treatment at the same time. However, in reality, this is not that common. It is very common that some units receive treatment before others.\n\nFor example, in the US, states often adopt different policies at different times - such as the staggered adoption of mail-in voting, or the staggered adoption of the common core curriculum.\n\nThe staggered difference-in-differences design is an extension on the classical design, which allows for staggered implementation of treatment.\nEssentially, this design is a combination of a bunch of smaller classical difference-in-differences designs.\n\nFor example, consider that we have 4 groups. Group 1 receives the treatment in period 1, group 2 receives the treatment in period 2, and group 3 receives the treatment in period 3. Group 4 never receives the treatment.\nIn this scenario, we essentially have 3 smaller classical difference-in-differences combined into one larger study: one for group 1, one for group 2, and one for group 3.\n\nJust like in the classical design, the staggered design can acommodate panel data (more common) or repeated cross-sections. Unlike the classical design however, there are far more estimators to choose from.\n\n\n\nTwo-Way Fixed Effects Estimator\nThe TWFE estimator is possible in staggered DiD just like in the classical design - and is quite simple. However, there are a variety of issues outlined here, that make it more dubious.\n\n\nGardner (2021) 2-Stage DiD\nThis estimator accounts for the issues with TWFE by estimating the missing counterfactuals to find the treatment effect.\n\nKevin’s Notes: very simple and intuitive, however, not very flexible or robust. If you are interested in a better counterfactual estimator, use Liu et al (2024).\n\n\n\nWooldridge (2021) Extended Two-Way Fixed Effects\nThis estimator accounts for the issues with TWFE by adding interaction terms to model heterogeneity.\n\nKevin’s Notes: Can handle non-linear models like poisson and logistic. Very similar to Sun and Abraham. It handles conditional parallel trends more flexibly than Sun and Abraham. However, it cannot estimate pre-treatment dynamic effects, so if you need to test parallel trends use Sun and Abraham.\n\n\n\nSun and Abraham (2021)\nThis estimator accounts for the issues with TWFE by adding interaction terms to model heterogeneity.\n\nKevin’s Notes: Very similar to Wooldridge. It can estimate pre-treatment dynamic treatment effects when Wooldridge cannot. However, it is less flexible and robust when dealing with conditional parallel trends.\n\n\n\nCallaway and Sant’Anna (2021)\nThis estimator accounts for the issues with TWFE through matching and reweighting.\n\nKevin’s Notes: Probably the most well regarded in econometrics. Intuitively, it might be the simplest, however, technically, it is very complex. It is semi-parametric, making it more flexible than regression based methods.\n\n\n\nLiu et al (2022) Counterfactual\nThis estimator accounts for the issues with TWFE by estimating the missing counterfactuals to find the treatment effect.\n\nKevin’s Notes: This is a more complex estimator that (debatedly) might not even be a difference-in-differences estimator, as it draws a lot from synthetic controls. It is quite flexible and preferable to Gardner (2021), but is more complex and hard to explain.",
    "crumbs": [
      "Difference-in-Differences",
      "Staggered DiD Design"
    ]
  },
  {
    "objectID": "did/twfe.html",
    "href": "did/twfe.html",
    "title": "Two-Way Fixed Effects Estimator",
    "section": "",
    "text": "Note: if you have staggered treatment implementation, TWFE can produce incorrect results.\n\nThe two-way fixed effects estimator is the most commonly used estimator in difference-in-differences. The estimator is a linear regression model specified as following:\n\\[\nY_{it} = \\alpha_i + \\gamma_t + D_{it}\\tau + \\b X_{it}' \\b\\beta + \\eps_{it}\n\\]\n\n\\(\\alpha_i\\) and \\(\\gamma_t\\) are country and year fixed effects, \\(\\b X_{it}\\) is a vector of covariates for conditional parallel trends.\n\nWhat are fixed effects, and what do they do? Let us take a look at this example:\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nRaising Minimum Wage in a State\n\n\n\nY\n\nUnemployment in a State\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nState\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\nQ\n\nYear\n\n\n\nQ-&gt;D\n\n\n\n\n\nQ-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see in this diagram, we have two potential confounders:\n\nStates have systematic differences between them. These differences between states may change the likelihood of adopting a minimum wage increase, and may be linked with unemployment.\nYears have differences between them. Some years have recessions, which might affect both the likelihood of a minimum wage increase, and may be linked with unemployment.\n\nThus, two-way fixed effects are mechnically control variables for units (in this example, countries) and time-periods (in this example, years). Fixed effects \\(\\alpha_i\\) and \\(\\gamma_t\\) get rid of these two potential sources of confounding.\n\nYou can also think of fixed effects as essentially de-meaning observations by group. For example, it calculates the average unemployment in every state over time, and subtracts that for each observation within that state. This “subtracts” away the differences between states.\n\nThe estimated \\(\\widehat\\tau\\) of the two-way fixed effects estimator is the ATT estimate of difference-in-differences. This estimate is unbiased, assuming our assumptions (most importantly, parallel trends) are met.\nThe reason why two-way fixed effects works lies in the mechanics of the Ordinary Least Squares (OLS) estimator. Under the condition of strict exogeneity, OLS is an unbiased estimator. Or in other words, if all confounders are controlled for, OLS is unbiased.\nThe unit fixed effects \\(\\alpha_i\\) control for between-unit differences. The time fixed effects \\(\\gamma_t\\) control for between-period differences. Thus, the only remaining “confounders” are the difference in trends over time. If parallel trends assumption is met, then we have no “confounders”, and thus, we have strict exogneity and a unbiased estimate.\nTwo additional notes regarding fixed effects:\n\nWe should cluster standard errors by unit.\nFor staggered treatment (see later pages), TWFE is flawed.\n\n\nTo implement two-way fixed effects, we need the fixest package:\n\nlibrary(fixest)\n\nWe implement two-way fixed effects as follows:\n\nmodel &lt;- feols(Y ~ D + X1 + X2 + X3 | unit + period,\n               data = my_data,\n               se = \"cluster\")",
    "crumbs": [
      "Difference-in-Differences",
      "Two-way Fixed Effects"
    ]
  },
  {
    "objectID": "did/twfestaggered.html",
    "href": "did/twfestaggered.html",
    "title": "Issues with Two Way Fixed Effects in Staggered DiD",
    "section": "",
    "text": "See Goodman-Bacon (2021) for a more technical overview of TWFE decomposition.\n\nThe two-way fixed effects estimator is possible with staggered difference-in-differences:\n\\[\nY_{it} = \\alpha_i + \\gamma_t + D_{it}\\tau + \\b X_{it}'\\b\\beta + \\eps_{it}\n\\]\n\nFor more info on two-way fixed effects, see the previous page in classical DiD.\n\nBut what is two-way fixed effects estimating in a staggered difference-in-differences setting? Goodman-Bacon decompose the estimate of TWFE in a staggered setting. Let us assume we have 3 groups - a group treated early, a group that was treated late, and a untreated group.\n\n\n\n\n\nTwo-way fixed effects essentially estimates 4 different differences, shown below:\n\n\n\n\n\nThe actual TWFE result is a weighted average of these differences. The weights are a function of three things:\n\nThe sample size of each comparison. The higher sample size, the more weight.\nThe ratio of treated to control - the closer the ratio to 0.5, the higher weight.\nRelative timing of treatment - groups treated more in the middle (so not too early or too late) are weighted higher. The weights are seen below in the figure.\n\n\n\n\n\n\nThere are two problems with these comparison conducted by TWFE:\n\nSome of these comparison (see comparison D above) may be “forbidden” - such that already-treated units are used as controls after they are treated.\nWeighting can result in weird weights - sometimes some groups treated early/late might receive negative weights (see figure above).\n\n\nThis site shows how you can explore the comparison and weighting issue in R.\n\nThese problems with TWFE mean that TWFE is biased in estimating the ATT in staggered difference-in-differences when there is heterogenous treatment effects.\n\nTWFE is unbiased with homogeneity, but homogeneity is quite rare in scenarios we typically research.\n\nThus, researchers have tried to solve these issues by either removing forbidden comparisons, or fixing the weighting (or both). This research took a huge leap since 2020 during the difference-in-differences revolution, which has lead to a series of new estimators that solve these issues.",
    "crumbs": [
      "Difference-in-Differences",
      "Issues with TWFE"
    ]
  },
  {
    "objectID": "did/sunab.html",
    "href": "did/sunab.html",
    "title": "Sun and Abraham (2021) Estimator",
    "section": "",
    "text": "There are two issues with the two-way fixed effects estimator: forbidden comparisons and issues with weighting. Sun and Abraham (2021) proposes a new estimator that solves both of these issues.\n\nFirst, the estimator divides all units into groups based on their initial treatment implementation.\nThen, the estimator calculates dynamic ATT’s separately for group. This captures heterogeneity of treatment between different initial treatment periods.\nThen, these separate heterogeneous effects are manually aggregated together into one ATT using proper weighting based on how frequent each initial treatment period group appears in our sample.\n\nSun and Abraham estimate dynamic ATT’s for each group by including interaction terms for each initial treatment period grouping and time period in a regression model.\n\\[\n\\begin{align}\nY_{it} = \\sum\\limits_g & \\sum\\limits_{k ≠ -1} \\blue{\\tau_{g, k}} \\cdot 1\\{i \\in g\\} \\cdot 1\\{t-g=k\\} \\\\\n+ & \\ \\alpha_i + \\gamma_t + \\b X_{it}'\\b\\beta + \\eps_{it}\n\\end{align}\n\\]\n\nThe summations mean include one interaction for each \\(g\\) (initial treatment year group), and each \\(t\\) (time periods) within each \\(g\\). The \\(1\\{\\}\\) is an indicator function that takes a value of 1 if true, and 0 if false. \\(i \\in g\\) means unit \\(i\\) initially recieved treatment in period \\(g\\). \\(t-g\\) represents the relative year of treatment, with the year treatment initiates is year 0. The first line interacts relative treatment period with the group indicator.\n\nThe reason for \\(k ≠ -1\\) is because time period -1 relative to treatment (so the year before treatment occurs) is set as the reference category. This is similar to dynamic treatment effects we saw previously.\nThis model will produce many coefficients \\(\\tau_{g, t}\\) (in fact, one for each post-treatment time period for each initial treatment year group). We can then properly weight each of them to create dynamic treatment effects. Sun and Abraham is primarily used to calculate dynamic treatment effects, but we can aggregate them into a single ATT if we need to.\nSun and Abraham’s estimator is very similar to Wooldridge’s extended two-way fixed effects estimator. The three main differences are:\n\nSun and Abraham can calculate dynamic treatment effects for every time period (both pre and post treatment). Wooldridge’s can only calculate post-treatment. Thus, only Sun and Abraham can test for parallel trends.\nSun and Abraham is more strict with covariates for parallel trends, while Wooldridge’s allows for more flexibility and robustness to violations of parallel trends. This is because Wooldridge’s estimator contains far more interactions with covariates.\nWooldridge’s estimator allows for non-linear models (poisson, logistic, etc.).\n\n\nTo implement this estimator, we will need the fixest package:\n\nlibrary(fixest)\n\nWe implement Sun and Abraham’s estimator as follows:\n\nmodel &lt;- feols(Y ~ sunab(initial_treat_year, period) | unit + period,\n               data = my_data,\n               vcov = ~unit)\niplot(model)",
    "crumbs": [
      "Difference-in-Differences",
      "Sun and Abraham (2021)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resources on Causal Inference and Social Science Statistics",
    "section": "",
    "text": "This repository contains resources on causal inference and social statistics, specifically focused on applications to politics and economics.\n\nUse the top navigation bar to navigate between different methods.\nUse the side navigation bar to navigate within a method.\nR-code for implementation is included.\nI have tried to keep up with modern advancements, including the recent difference-in-differences rennaisance.\n\nThese resources have been collected throughout my postgraduate degree at the London School of Economics. I hope these resources can be useful for my future self and others. I try to ensure everything in this repository is accurate. If there are any issues with the repository, let me know.\nThere are some stuff that doesn’t fit into any category, which I have included in the home page sidebar.\nFor my personal website, see http://kevinli03.github.io",
    "crumbs": [
      "Home",
      "Homepage"
    ]
  },
  {
    "objectID": "soo/soochoose.html",
    "href": "soo/soochoose.html",
    "title": "Choosing an Estimator",
    "section": "",
    "text": "We know how selection on observables works now. But how do we actually implement selection on observables? Below are a list of estimators and their strengths/weaknesses. You can use multiple simultaneously for robustness.\n\n\n\n\n\n\nLinear Regression Estimator\n\nEstimand: ATE\nPros: very simple, works well with small datasets.\nCons: 1) requires linear relationship between confounders and outcome, 2) does not work when there are heterogenous treatment effects.\n\n\nKevin’s Comments: since heterogeneity is so common in almost everything, I would recommend against using this estimator unless nothing else is possible. The fully interacted estimator (below) is just a better version of this.\n\n\n\nFully Interacted Estimator\n\nEstimand: ATE\nPros: 1) modified regression to allow for heterogenous effects, 2) still relatively simple.\nCons: requires linear relationship between confounders and outcome\n\n\n\nDistance Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome. 2) relatively intuitive idea.\nCons: 1) can be badly biased when more than 3-5 confounders, 2) throws out unmatched data so wastes data.\n\n\nKevin’s Comments: there is little reason to use distance matching over genetic matching, unless your machine physically cannot estimate genetic matching.\n\n\n\nPropensity Score Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) can handle larger amounts of confounders than distance matching.\nCons: 1) needs a large sample size to not be biased, 2) throws out unmatched data so wastes data.\n\n\nKevin’s Comments: there is little reason to use propensity score matching over genetic matching, unless your machine physically cannot estimate genetic matching.\n\n\n\nGenetic Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) shown to be the best matching estimator\nCons: 1) throws out unmatched data so wastes data, 2) can be computationally taxing.\n\n\n\nInverse Probability Weighting\n\nEstimand: ATE\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) does not waste data like matching methods do.\nCons: requires a large sample size to be unbiased.",
    "crumbs": [
      "Selection on Observables",
      "Choosing an Estimator"
    ]
  },
  {
    "objectID": "soo/regress.html",
    "href": "soo/regress.html",
    "title": "Linear Regression Estimator",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nSelection on observables is about controlling for confounders. Linear regression is a very natural way to control for confounders.\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2 X_{i2} + \\eps_i\n\\]\nWe know that in the above equation, \\(\\beta_1\\) is the relationship between \\(X_i\\) and \\(Y_i\\), while controllling (holding constant) \\(X_2\\).\nUsing this idea, we can implement causal inference with regression, with one of the explanatory variables being our treatment variable, and the rest of the explanatory variables being control variables.\n\\[\nY_i = \\alpha + D_i\\tau + \\b X_i' \\b\\beta + \\eps_i\n\\]\n\nWhere \\(\\alpha\\) is the intercept and \\(\\b X_i\\) is a vector of confounder values for individual \\(i\\).\n\nOur ordinary least squares (OLS) estimate \\(\\widehat\\tau\\) is an unbiased estimator of the true \\(\\tau_{ATE}\\) given three conditions are met:\n\nWe meet the selection on observables assumption of conditional ignorability. Conditional ignorability implies exogeneity, which means the estimate is unbiased.\nThe relationship between our continuous confounders and outcome variable is linear. This is because if the true relationship between these two is not linear, then our linear model is wrong, so it is not properly controlling for confounders.\nThere is no heterogeneity in treatment effects. Angrist (1998), Lin (2013), and Słoczyński (2022) have proven that when there is heterogeneity, OLS is estimating another quantity that is not the ATE.\n\n\nHeterogeneity means that different individuals have different individual treament effects \\(\\tau_i\\). OLS only estimates the ATE if there is homogeneity - all \\(\\tau_i\\) are equal.\n\n\nBefore you implement the estimator, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the fixest package:\n\nlibrary(fixest)\n\nTo run a regression estimator, we do:\n\nfeols(Y ~ D + X1 + X2 + X3,\n      data = data,\n      se = \"hetero\")\n\n\nWe typically assume heteroscedasticity, so we use heteroscedasticity-robust standard errors. If you can prove homoscedasticity, then you can use normal standard errors.\n\nThe coefficient for the treatment variable will be the ATE - the average treatment effect for all units in the study.\n\nAssuming you have met all the assumptions of selection on observables, and the special assumptions for the linear regression estimator shown above.",
    "crumbs": [
      "Selection on Observables",
      "Linear Regression Estimator"
    ]
  },
  {
    "objectID": "soo/ipw.html",
    "href": "soo/ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nLet us look at this example, with a confounder.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nReceiving Scholarship\n\n\n\nY\n\nUniversity Grades\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s pretend there are only dumb and smart people (for simplicity). Our treated and control groups might be:\n\n\n\n\n\n\n\nTreated (Got Scholarship)\nUntreated (Did not get scholarship)\n\n\nSmart Students (x4)\nSmart Students (x1)\n\n\nDumb Students (x1)\nDumb Students (x4)\n\n\n\nOur two groups have pre-existing differences. However, by emphasising certain individuals, we can make it seem like there are no more imbalances. For example, weighting might make our above table become:\n\n\n\n\n\n\n\nTreated (Got Scholarship)\nUntreated (Did not get scholarship)\n\n\nSmart Students (x4)\nSmart Students (emphasise to x4)\n\n\nDumb Students (emphasise to x4)\nDumb Students (x4)\n\n\n\n\nSee how the underrepresented individuals in each group (treated/untreated) were weighted upwards. More technically, inverse probability weighting emphasises/weights an individual by the inverse of their likelihood to receive treatment.\n\nWe can see there is no more pre-existing differences after weighting. Thus, selection bias has been solved.\n\nBefore you inverse probability weighting, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the estimatr package:\n\nlibrary(estimatr)\n\nTo estimate the propensity scores and weights, we can use the glm() command:\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity, type = \"response\")\nmy_data$ipw &lt;- ifelse(mydata$D == 1,\n                      1/my_data$pscore,\n                      1/(1-my_data$pscore))\n\nFinally, we need to use the lm_robust() command to estimate our causal effects:\n\nate &lt;- estimatr::lm_robust(Y ~ D, data = my_data, weights = ipw)\nsummary(ate)\n\nThe output will be the ATE - the average treatment effect for all units in the study.",
    "crumbs": [
      "Selection on Observables",
      "Inverse Probability Weighting"
    ]
  },
  {
    "objectID": "soo/interact.html",
    "href": "soo/interact.html",
    "title": "Fully Interacted Estimator",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nFrom selection on observables, we know that our causal effect is a weighted average:\n\\[\n\\tau_\\text{ATE} = \\sum\\tau (\\b x) Pr(\\b x)\n\\]\nNotice how the weights are the probability of the confounder values of \\(\\b x\\). With some complex math (Angrist 1998), we can actually show that OLS actually estimates:\n\\[\n\\hat\\beta_\\text{OLS} = \\sum \\tau(\\b x) \\underbrace{\\frac{Var(D_i | \\b x)Pr(\\b x)}{\\sum Var(D_i | \\b x^c)Pr(\\b x^c)}}_{\\text{weight}}\n\\]\n\nWhere \\(\\b x^c\\) is the complement (not \\(\\b x\\)).\n\nThese weights are not equivalent to the selection on observables \\(\\tau_{ATE}\\). Thus, if not all \\(\\tau(\\b x)\\) are exactly the same (which implies heterogeneity), then our linear regression estimator will incorrectly estimate the ATE.\nHeterogeneity is present in almost all situations we are interested in. Lin (2013) proposes the fully interacted estimator, which allows for consistent estimation of the ATE even with heterogeneity:\n\\[\nY_i = \\alpha + D_i \\tau + \\underbrace{(\\b{X}_i - \\mean{\\b X})' \\b\\beta}_{\\text{confounders}} \\ + \\underbrace{D_i (\\b X_i - \\mean{\\b X})' \\b\\gamma}_{\\text{interaction}} \\  + \\eps_i\n\\]\n\n\\(\\mean{\\b X}\\) is a vector of the means of each confounder. \\(\\tau\\) is the estimate of the ATE. See Lin (2013) for proofs.\n\nThe new OLS estimate of \\(\\hat\\tau\\) in this estimator will technically still be a biased estimator of \\(\\tau_{ATE}\\), but the bias is negligible.\n\nBefore you implement the estimator, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the estimatr package:\n\nlibrary(estimatr)\n\nThen, we can use the lm_lin() function to estimate:\n\nate &lt;- estimatr::lm_lin(Y ~ D,\n                        covariates = ~ X1 + X2 + X3,\n                        data = my_data)\nsummary(ate)\n\nThe output will be the ATE - the average treatment effect for all units in the study.",
    "crumbs": [
      "Selection on Observables",
      "Fully Interacted Estimator"
    ]
  },
  {
    "objectID": "latent/scores.html",
    "href": "latent/scores.html",
    "title": "Reliability and Factor Scores",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nRecall our measurement models linking each item \\(i\\) to our factor:\n\\[\n\\text{item}_i = \\tau_i + \\lambda_i \\ \\text{factor} + \\delta_i\n\\]\nOne of the assumptions of this model is the the error term \\(\\delta_i \\sim \\mathcal N(0, \\theta_i)\\), or in other words, the error term has a mean of 0 and a variance of \\(\\theta_i\\). Through some complex math, we can show that the variance of each item \\(X_i\\) is as follows:\n\\[\nVar(\\text{item}_i) = \\lambda_i^2 + \\theta_i\n\\]\nThus, this allows us to essentially “split” the variance in any item \\(X_i\\) into two parts:\n\n\\(\\lambda_i^2\\) is the part of the variance in the item explained by the latent factor. We call this the communality of the item \\(i\\).\n\\(\\theta_i\\) is the residual variance, the part of the variance not explained by our factor.\n\nWe can also calculate the percentage/proportion of variance in item \\(i\\) that our factor explains, called the reliability:\n\\[\n\\text{Reliability} = \\frac{\\lambda_i^2}{Var(\\text{item}_i)} = \\frac{\\lambda_i^2}{\\lambda_i^2+\\theta_i}\n\\]\n\nIf all items are standarised to a standard normal, then \\(\\lambda_i^2\\) is equal to the reliability.\n\nCommunality and relaibility is important for two reasons:\n\nItems with higher reliability are considered more “accurate” measures of the latent variable factor. They thus ensure better model estimation - and when a factor has low relaibility, we will often drop it.\nThese allow us to calculate factor scores.\n\nFactor scores are essentially values of the latent variable for individuals in our study. This allows us to use our observed items to calculate the latent variable value that any individual should have, which we can then put into another statistical model.\nWe often conduct factor anlaysis for the sole purpose of getting factor scores. Factor scores are calculated as a weighted linear combination of all items:\n\\[\n\\text{factor scores} = w_0 + w_1 \\ \\text{item}_1 + w_2 \\  \\text{item}_2 + \\dots\n\\]\nThe weights are calculated based on the communalities. The items with the highest communalities tend to get the strongest weights, while the items with the least communalities get the smallest weights.\n\nTo calculate factor analysis, we have to first run a factor analysis model in the same way we did in the last page.\n\nlibrary(psych)\nlibrary(GPArotation)\n\n# eliminate missing observations\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\n# factor model\nfa &lt;- fa(data[,items], nfactors=1, fm=\"ml\")\n\nR will automatically calculate factor scores in the estimation process, so all we have to do is access it within our output object:\n\nfa$scores\n\nYou can save this into your dataset, and use for other purposes.",
    "crumbs": [
      "Latent Variable Models",
      "Reliability and Factor Scores"
    ]
  },
  {
    "objectID": "latent/factor.html",
    "href": "latent/factor.html",
    "title": "Factor Analysis Model",
    "section": "",
    "text": "Factor analysis is a way to model a continuous latent variable, based on a bunch of continuous observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nLatent Factor (F)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe factor is the unobserved latent variable we want to measure. We assume that the factor is continuous, and normally distributed:\n\\[\n\\text{factor} \\sim \\mathcal N(0, 1)\n\\]\n\nWhere \\(\\kappa = 0\\) is the mean of the factor, and \\(\\phi =1\\) is the variance of the factor. Technically, the factor only needs to be normally distributed - we can have different means and variances. However, for identification reasons, we typically fix the factor to a standard normal.\n\nThe \\(p\\) number of items are the variables we observe, that we believe are a measurement of the latent factor. Factor analysis assumes that each observed item is associated with the latent factor with a linear regression model:\n\\[\n\\begin{align}\n\\text{item}_1 & = \\tau_1 + \\lambda_1 \\ \\text{factor} + \\delta_1 \\\\\n\\text{item}_2 & = \\tau_2 + \\lambda_2 \\ \\text{factor} + \\delta_2 \\\\\n& \\vdots \\qquad \\qquad \\vdots \\\\\n\\text{item}_p & = \\tau_p + \\lambda_p \\ \\text{factor} + \\delta_p\n\\end{align}\n\\]\n\n\\(\\tau_i\\) is the intercept of the model.\n\\(\\lambda_i\\) is the coefficient that describes the relationship between any item and the factor. These are called factor loadings.\n\\(\\delta_i\\) is the error term - the part of an item not explained by the factor. They are called the unique factors.\n\n\nWe assume that the error terms are normally distributed \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\), that the different error terms \\(\\delta_1, \\dots, \\delta_p\\) are uncorrelated with each other, and factor is uncorrelated with the errors (exogeneity).\n\nThe factor loadings \\(\\lambda_i\\) represent the relationship/covariance between any item and a factor. If the items have been standardised to a standard normal, then \\(\\lambda_i\\) is also the correlation coefficient between items and factor.\n\nNote: this isn’t entirely true for multiple factors, as we will discuss in a later page.\n\nThese factor loadings help us interpret our latent variable. The sign of the factor loading tells us the direction in which our latent variable is measuring. The absolute size of the factor loading tells us how important that item is to the factor.\n\n\n\n\n\n\nInterpretation Example\n\n\n\n\n\nThis is a typical output of factor anlaysis. ML1 and ML2 are the two factors (let us focus on just ML1), and the rows are items (which are trust in different institutions).\n\nFor factor 1 (ML1), we can see that pol_parties, politicians have very large loadings. EP, parliament, and UN have moderate loadings. Police and legal have almost 0 loadings. Almost all loadings are positive.\nThis tells us that factor 1 is a latent variable that measures mostly trust in politicians, rather than the legal/policing system. Since the loadings are almost all positive, we can conclude that higher values of factor 1 mean higher levels of trust in legal/policing systems.\n\n\n\nWe can conduct hypothesis testing with each factor loading \\(\\lambda\\) with a z-test. The null hypothesis is that \\(\\lambda = 0\\), and the alternate hypothesis is that \\(\\lambda ≠ 0\\). If the p-value is less than 0.05, we know that there is a relationship between that item and the factor.\n\nTo implement factor analysis, we will need the psych and GPArotation package:\n\nlibrary(psych)\nlibrary(GPArotation)\n\nFirst, we should get rid of missing observations:\n\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\nFor factor analysis with one factor, we use the syntax:\n\nfa &lt;- fa(data[,items], nfactors=1, fm=\"ml\")\nprint(fa1)",
    "crumbs": [
      "Latent Variable Models",
      "Factor Analysis Models"
    ]
  },
  {
    "objectID": "latent/irt.html",
    "href": "latent/irt.html",
    "title": "Item Response Theory",
    "section": "",
    "text": "Item Response Theory (IRT), also called Latent Trait Models, is an extension of factor anlaysis, that deals with cases with a continuous latent factor but a binary/categorical set of observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nLatent Factor (F)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe factor is the unobserved latent variable we want to measure. We assume that the factor is continuous, and normally distributed:\n\\[\n\\text{factor} \\sim \\mathcal N(0, 1)\n\\]\nWe must have at least 3 observed items for estimation purposes. The items are associated with the latent factor with a binomial logistic model:\n\\[\n\\begin{align}\n\\text{logit}[Pr(\\text{item}_1 = 1 \\ | \\ \\text{factor})] & = \\tau_1 + \\lambda_1 \\ \\text{factor} \\\\\n\\text{logit}[Pr(\\text{item}_2 = 2 \\ | \\ \\text{factor})] & = \\tau_2 + \\lambda_2 \\ \\text{factor} \\\\\n\\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots & \\qquad \\qquad \\vdots \\\\\n\\text{logit}[Pr(\\text{item}_p = 1 \\ | \\ \\text{factor})] & = \\tau_p + \\lambda_p \\ \\text{factor} \\\\\n\\end{align}\n\\]\n\nThe logit link function is defined as \\(x/1-x\\). For categorical (more than 2 categories) items, a multinomial or ordinal logistic model is used. But this is pretty rare for someone to actually fit a model of this type.\n\n\\(\\tau_i\\) is the intercept of the model, called the difficulty. \\(\\lambda_i\\) is the coefficeint that describes the relationship between any item \\(X_i\\) and the factor. These are called factor loadings. It is also called the discrimination parameter.\nThese factor loadings are interepreted in a very similar way to factor analysis. The sign of the factor loading tells us the direction in which our latent variable is measuring. The absolute size of the factor loading tells us how important that item is to the factor.\nThe intercepts \\(\\tau_i\\) also tell us how “common” a value of 1 for the item is.\n\n\n\n\n\n\nExample of Interpretation\n\n\n\n\n\nBelow is a example of an interpretation - the items are binary (things that politicians want/don’t want).\n\nWe can first look at the signs:\n\nThe latent factor increases with agreeing (a value of 1) with PartialBirth, CapitalGains, and CAFTA.\nThe latent factor decreases with agreeing (a value of 1) with StemCell, IraqWithdraw, Immigration, and Minimum Wage.\n\nFrom this, we can kind of get a sense that latent variable is measuring right-left political affiliation, with higher values indicating right-wing, since agreeing left wing policies like IraqWithdraw, Immigration, and Minimum Wage are negative.\nThe absolute size of the factor loadings are quite similar - we can see that CAFTA is the least important to the factor.\nThe \\(d\\) (which in this output are the intercepts) show the average agreement with each item - we see the most people agree with MinimumWage and StemCell, and the least people agree with CAFTA and Immigration.\n\n\n\nThe factor scores are also very similar to factor analysis - they are a linear combination, with weights determined by how large a loading is for a certain item.\nThe models can also accommodate confirmatory analysis and multiple factors, just like factor analysis can.\n\nTo run an item response theory mode, we need the mirt package:\n\nlibrary(mirt)\n\nThen, we run the model as follows (make sure to subset your data to only include the items).\n\nmodel &lt;- mirt(data = my_data, model = 1, SE = TRUE)\ncoef(result)\n\n\nmodel = 1 indicates how many factors you want to include.\n\nTo get factor scores, we do the following:\n\nscores &lt;- as.vector(fscores(model, method = \"EAP\"))\n\n\n\n\n\n\n\nMore on Choosing Between Models\n\n\n\n\n\nTo compare and choose between different models, we have a few ways.\n\nWe can use a likelihood ratio test for nested models (see the regression section for more details).\n\n\nlavTestLRT(model1, model2)\n\n\nWe can use a global goodness of fit test - essentially a likelihood ratio but with the full sample covariance matrix as the null model. We want to fail to reject the null, because we want our model to be as close to the sample covariance matrix as possible.\n\n\nlavTestLRT(model)\n\n\nWe can use AIC and BIC to compare models since factor models are estimated with MLE. These are included in the output.\n\nUnfortunately, fit indicies like RMSEA do not work on IRT models.",
    "crumbs": [
      "Latent Variable Models",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "latent/class.html",
    "href": "latent/class.html",
    "title": "Latent Class Model",
    "section": "",
    "text": "Latent class models are an extension of factor analysis, that deals with cases of a categorical latent variable with categorical/binary observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nLatent Factor (F)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe factor s a categorical variable with \\(C\\) number of categories, which are also called latent classes. These categories are treated as if they have no inherent order, and we can choose the number of categories.\nWe also have categorical observed items with each item \\(i\\) having \\(K_i\\) number of categories. The latent class models connects each item with a factor through a item response probability:\n\\[\n\\begin{align}\nPr(\\text{item}_1 = k \\ | \\ \\text{factor} = c) & = \\pi_{1, kc} \\\\\nPr(\\text{item}_2 = k \\ | \\ \\text{factor} = c) & = \\pi_{2, kc} \\\\\n\\vdots \\qquad \\qquad \\vdots \\qquad \\qquad & \\vdots \\\\\nPr(\\text{item}_p = k \\ | \\ \\text{factor} = c) & = \\pi_{p, kc} \\\\\n\\end{align}\n\\]\n\nEach \\(\\pi_{i, kc}\\) is the probability of any item item being in category \\(k\\), given the factor is category \\(c\\).\n\nWe also have another part of the measurement mode, the structural model, which determines the probability of each category in the factor:\n\\[\n\\alpha_c = Pr(\\text{factor}=c)\n\\]\nInterpretation of the latent factor \\(F\\) depends on these item response probabilities \\(\\pi_{i, kc}\\). An example is provided below, because it can be a little confusing.\n\n\n\n\n\n\nExample of Interpretation\n\n\n\n\n\nBelow, the columns are the different classes/categories of the factors, and the big rows are each item.\n\nThe first class (the first column) has the highest probabilities if individuals never worry about crime, no real effect on quality of life, never worry about burglary, and no real effect on quality of life. Thus, we can conclude this first category of the latent variable is something like - not worried about crime.\nThe second class (the 2nd column), where the top responses have the highest probabilities except for the frequency of worry about burglaries - where the probabilities are highest for some of the time and just occasionally. This suggests that this second category is measuring something like - only worried about burglary, and no other crime.\n\n\n\nWe can also create factor scores - which is a little different, because now we are basically assigning every unit in our data to a category of the latent factor. This is done by calculating the posterior probability of being in each class:\n\\[\n\\widehat{Pr}(\\text{factor} = c \\ | \\ \\text{item}_1 = k_1, \\text{item}_2 = k_2, \\dots )\n\\]\nWe calcaulte this probability for all categories \\(c\\) in the factor. Whichever category \\(c\\) of the latent variable has the highest probability, is the category a unit is assigned to.\n\nThis can be considered quite similar to that of cluster analysis, which will be introduced later.\n\n\nTo implement latent class models, we will need the polLCA package:\n\nlibrary(poLCA)\n\nThis package requires that our categories of items are labelled starting with 1. This means if you have a binary variable of 0 and 1, you will need to change it to 1 and 2.\nTo begin, we will first need to create a vector of our item names:\n\nvars &lt;- c(\"X1\",\"X2\",\"X3\",\"X4\")\n\nThen, let us fit our model as follows:\n\nform &lt;- cbind(X1, X2, X3, X4) ~1\nmodel &lt;- poLCA(form,\n              my_data[,vars],\n              nclass=2, #number of categories for factor\n              na.rm=F,\n              nrep=10) \n\n\nna.rm = F means to include missing values when estimating (which is recommended). nrep = 10 indicates how many times to run the gradient descent algorithm - more is better, but will take longer.\n\nThe traditional output is hard to read, so we will use a function:\n\n# function\nLCA.probs &lt;- function(res){\n  probs &lt;- res$probs\n  item.p &lt;- NULL\n  for(i in seq_along(probs)){\n        m.tmp &lt;- t(probs[[i]])\n        rownames(m.tmp) &lt;- paste(names(probs)[i],colnames(probs[[i]]),sep=\".\")\n        item.p &lt;- rbind(item.p,m.tmp)\n  }\n  item.p &lt;- round(item.p,3)\n  class.p &lt;- res$P\n  names(class.p) &lt;- colnames(item.p)\n  list(item.probabilities=item.p,class.probabilities=class.p)\n}\n\n# output results\nLCA.probs(model)\n\nWe can calculate factor scores/classification as follows:\n\nmodel$predclass\n\nWe can choose our model based on the AIC or BIC score.\n\nmodel$aic\nmodel$bic",
    "crumbs": [
      "Latent Variable Models",
      "Latent Class Models"
    ]
  },
  {
    "objectID": "latent/longitudinal.html",
    "href": "latent/longitudinal.html",
    "title": "Lagged Response Models",
    "section": "",
    "text": "Longitudinal data (also called panel data) is when we have the same variables observed over several time points for the same units. Structural equation models can accommodate panel data as well.\nLet us say we have some latent response variable \\(\\eta_{it}\\), which is measured by observed indicators \\(Y_{it1}, Y_{it2}, \\dots, Y_{itp}\\). We also have a few explanatory variables:\n\n\\(Z_i\\) is a explanatory variable that determines characteristics of units, that does not change over time.\n\\(X_{it}\\) are time-varying explanatory variables.\n\nA lagged response model is a regression model that says that the response variable \\(\\eta_{it}\\) not only depends on the explanatory variables \\(Z_i\\) and \\(X_{it}\\), but also previous values of \\(\\eta_{it}\\).\n\\[\n\\eta_{it} = \\beta_0 + \\beta_xX_{it} + \\beta_zZ_i + \\underbrace{\\beta_\\eta \\eta_{i, t-1}}_{\\text{lagged}} + \\eps_{it}\n\\]\nIn a diagram, this model can be modelled as:\n\n\n\n\n\n\nNote how \\(\\eta\\) in \\(t=3\\) depends on both \\(X_{it}\\), \\(Z_t\\), and \\(\\eta\\) from \\(t=2\\).\n\nRecall the response variable \\(\\eta\\) is itself a latent variable, measured with indicators \\(Y_{it1}, Y_{it2}, \\dots, Y_{itp}\\). This also implies a measurement model relating latent \\(\\eta_{it}\\) with each observed item:\n\\[\n\\b Y_{it} = \\b\\tau_t + \\b\\lambda_t \\b\\eta_{it} + \\b\\eps_{it}\n\\]\n\nWhere all are vectors encompassing measurement models for all items \\(Y_{it1}, Y_{it2}, \\dots, Y_{itp}\\). Note how the parameters have the subscript \\(t\\) - this implies a different measurement model for each item, at each time period.\n\nHowever, because each item \\(Y_p\\) is observed over time, it is likely that \\(Y_p\\) from \\(t=1\\) is correlated with \\(Y_p\\) from \\(t=2\\) ( autocorrelation). We have to take this into account in one of two ways:\n\nWe could constrain the model, assuming that the measurement model is identical throughout different time periods. That means \\(\\tau_t\\) and \\(\\lambda_t\\) will now no longer depend on the time period (the same for all time periods).\nOr we could complicate the model, by including correlation between the error terms \\(\\eps\\) for the same item.\n\n\nTo implement lagged response models, we need the lavaan package:\n\nlibrary(lavaan)\n\nFor a constrained two-period model, the estimation is as follows:\n\nmodel &lt;- '\n# explanatory variables Measurement models \n  explanatory1 =~ NA*X1 + X2 + X3\n  explanatory2 =~ NA*Z1 + Z2 + Z3\n\n# response variables for time = 1 and time = 2\n    response1 =~ la*Y1t1 + lb*Y2t1 + lc*Y3t1\n    response2 =~ la*Y1t2 + lb*Y2t2 + lc*Y3t2\n\n# time = 1 model\n  response1 ~ explanatory1 + explanatory2\n  \n# time = 2 model with lagged time = 1\n  response2 ~ explanatory1 + explanatory2 + response1\n\n# Fixing the variances of the exogenous latent variables LR and AL  \n  LR~~1*LR\n  AL~~1*AL\n'\n\nsem &lt;- sem(model, data = my_data, missing=\"FIML\")\nsummary(sem)\n\n\nThe labels la*, lb*, and lc* indicate to R to make all factor loadings with the same labels equivalent to each other. Notice how we put the same labels on the different time period measurement models.\n\nFor a unconstrained model, the code is as follows:\n\nmodel &lt;- '\n# explanatory variables Measurement models \n  explanatory1 =~ NA*X1 + X2 + X3\n  explanatory2 =~ NA*Z1 + Z2 + Z3\n\n# response variables for time = 1 and time = 2\n    response1 =~ Y1t1 + Y2t1 + Y3t1\n    response2 =~ Y1t2 + Y2t2 + Y3t2\n\n## Error covariance in the measurement model\n  Y3t1 ~~ Y3t2\n\n# time = 1 model\n  response1 ~ explanatory1 + explanatory2\n  \n# time = 2 model with lagged time = 1\n  response2 ~ explanatory1 + explanatory2 + response1\n\n# Fixing the variances of the exogenous latent variables LR and AL  \n  LR~~1*LR\n  AL~~1*AL\n'\n\nsem &lt;- sem(model, data = my_data, missing=\"FIML\")\nsummary(sem)",
    "crumbs": [
      "Latent Variable Models",
      "Lagged Response Models"
    ]
  },
  {
    "objectID": "latent/confirmatory.html",
    "href": "latent/confirmatory.html",
    "title": "Confirmatory Analysis",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nIn the past few chapters, we have focused on exploratory analysis, essentially allowing the data to “speak for itself” and estimate all of the model’s parameters to measure the latent variable.\nHowever, what if we already have some ideas about our latent variables - some hypotheses on how different items are related to the latent variable. This is what we can confirmatory analysis, where we study how well a hypothesised model fits the data.\nThe basic approach of confirmatory analysis is to set some factor loadings \\(\\lambda\\) for certain variables to 0. This essentially means that based on our preconceived theories, establish that some items do not measure a certain factor.\n\nBy setting enough loadings to 0, we no longer have to worry about rotation issues discussed previously.\n\nFor example, perhaps theoretically we believe that items 1 and 2 only explain factor 1, and only items 3 and 4 explain latent factor 2. We can set the items we believe to not explain each factor to 0.\n\n\n\n\n\n\n\n\nexample2\n\n\nF1\n\nLatent Factor (F1)\n\n\n\nX1\n\nItem 1 (X1)\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2 (X2)\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3 (X3)\n\n\n\nF1-&gt;X3\n\n\n0\n\n\n\nF2\n\nLatent Factor (F2)\n\n\n\nF2-&gt;X2\n\n\n0\n\n\n\nF2-&gt;X3\n\n\nλ\n\n\n\nX4\n\nItem 4 (X4)\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\n\n\n\n\n\n\nAnother difference is in the assumptions we make on the actual latent variables. Recall that we previously assumed the latent factors are standardly normally distributed:\n\\[\n\\text{factor} \\sim \\mathcal N(0, 1)\n\\]\nIn confirmatory analysis, we still assume the factor is normally distributed, but we allow for the mean and variance of the distirbution to be estimated:\n\\[\n\\text{factor} \\sim \\mathcal N(\\kappa, \\phi)\n\\]\nInstead, we fix each factor (if we have multiple) to the scale of one of the items, with each factor fixed to a different item. Basically, this means that the factor will take the same scale/measurement characteristics as one of the items. This is done by fixing that item’s intercept \\(\\tau_i = 0\\), and the same item’s factor loading for that specific factor at \\(\\lambda = 1\\).\nThe estimation and interpretation of confirmatory analysis are essentially identical to that of exploratory analysis we have previously looked at.\nThere is a special set of tests to see if the parameters we set equal to 0 actually make sense. Modification indicies are a sort of hypothesis test for this - larger values indicate parameters that if were not 0, could improve the fit of the model.\n\nThere are also expected parameter changes (EPC), which basically estimate what parameters set to 0 would actually be equal to, if they were added to the model.\n\n\nBelow in the R-code I providem more ways of choosing between confirmatory models and exploratory models (and really any factor models).\n\n\nFor confirmatory analysis, we will need the lavaan package:\n\nlibrary(lavaan)\n\nWe fit a confirmatory model in the following way:\n\n# specify formula\nformula &lt;- '\nf1 =~ X1 + X2 + 0*X3 \nf2 =~ 0*X2 + X3 + X4\n'\n\n# estimate model\nmodel &lt;- sem(formula,\n           data = my_data,\n           std.lv = TRUE,\n           missing = \"fiml\")\nsummary(model)\n\n\nWe put a 0* before any item for that factor we want to set equal to 0. You can also use this command to estimate exploratory models.\n\nIf we have theoretical reasons, we can also force different factor load gins to be equal by including the same labels before as follows:\n\n# specify formula\nformula &lt;- '\nf1 =~ c1*X1 + c1*X2 + X3 \nf2 =~ X2 + X3 + X4\n'\n\n\nSince label c1 appears before X1 and X2 for factor f1, that means their factor loadings will be forced to be equal. You can include more labels as well.\n\nTo create factor scores, we simply use the predict function:\n\npredict(model)\n\n\n\n\n\n\n\nMore on Choosing Between Models\n\n\n\n\n\nTo compare and choose between different models, we have a few ways.\n\nWe can use a likelihood ratio test for nested models (see the regression section for more details).\n\n\nlavTestLRT(model1, model2)\n\n\nWe can use a global goodness of fit test - essentially a likelihood ratio but with the full sample covariance matrix as the null model. We want to fail to reject the null, because we want our model to be as close to the sample covariance matrix as possible.\n\n\nlavTestLRT(model)\n\n\nWe can use AIC and BIC to compare models since factor models are estimated with MLE. These are included in the output.\n\nThere are also a series of fit indicies made for factor analysis. These are included in the output.\n\nRoot Mean Square Error of Approximation (RMSEA): 0 is a perfect fit, 0.05 or smaller is a good fit, and anything above 0.1 is a poor fit.\nStandard Root Mean Square Residual (SRMR): smaller values the better, anything below 0.08 is a good fit.\nTucker and Lewis Index (TLI): 1 is the best fit, anything below 0.9 is a poor fit, and anything above 1 may indicate overfitting.\nComparative Fit Index: values between 0 and 1, anything close to 1 is a good fit.",
    "crumbs": [
      "Latent Variable Models",
      "Confirmatory Analysis"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Additional Resources",
    "section": "",
    "text": "Here are a list of additional resources that are useful for causal inference and social science statistics.\n\nDr. Asjad Naqvi has a repository of the modern advancements going on in the recent difference-in-differences revolution.\nCausal Inference: The Mixtape as a great resource for details on causal inference methodology.\nCausal Inference for the Brave and True is another great resource of causal inference, with application in Python.\nFixest (an R package used for causal inference and regression) is a very useful thing to be familiar with.\nBen Lambert is a terrific resource for econometrics. He also has a lot of resources on general statistics, including factor analysis and bayesian statistics.\nGreat textbooks for causal inference: Angrist and Pischke have two books - Mostly Harmless Econometrics (more advanced) and Mastering Metrics (more simple). A very technical book is Imbens and Rubin’s Causal Inference for Statistics, Social, and Biomedical Sciences.\nGreat textbooks for more theoretical econometrics/statistics include Wooldridge’s Introductory Econometrics.",
    "crumbs": [
      "Home",
      "Additional Resources"
    ]
  }
]