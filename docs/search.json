[
  {
    "objectID": "randomisation.html",
    "href": "randomisation.html",
    "title": "The Magic of Randomisation",
    "section": "",
    "text": "This chapter covers how randomisation solves the problem of selection bias, and why randomisation is considered the “gold standard” of causal inference.\n\nLet us say we are interested in this question:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nOur concern is a confounder. For example, smartness of an individual could mean they are more likely to get a scholarship. Since smart people tend to perform well at university, that means the people who get treated are different from those who don’t get treated.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmartness is not the only confounder. Other confounders could be family income, athletic ability, etc.\n\nBut what if randomness (like flipping a coin) controls who gets the treatment or not. A coin will be flipped to decide if every person in our study will get the scholarship. This means that the randomness (the coin), and not the confounder, are causing selection into treatment:\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;Y\n\n\n\n\n\n\nO\n\nCoin\n\n\n\nO-&gt;D\n\n\n\n\n\n\n\n\n\n\n\nSince the confounder is no longer causing who gets the treatment and who doesn’t, that means there is no more concern of selection bias.\n\nRandomisation also means that every individual has the same chance of being treated or untreated, so the two groups will, on average, the same as each other. That means:\n\\[\n\\textcolor{red}{\\text{grades}_\\text{untreated}^{(0)}} = \\textcolor{red}{\\text{grades}_\\text{treated}^{(0)}}\n\\]\nAnd thus, since that is true, that means correlation equals causation.\n\nThis is established by the law of large numbers, but it is a little technical for here.\n\nSo if our treatment is randomly assigned (individuals randomly assigned to treatment or control), correlation does equal causation.\n\nRandomisation is the gold standard of causal inference. There is no better method.\n\nRandomisation is possible if you are running your own experiment: you can use a random number generator to assign treatment.\nRandomisation is also possible if there is something that is being randomly assigned in the real world. For example, the US green card lottery randomly chooses who gets accepted.\n\nHowever, randomisation is not always possible to due to cost of running experiments, non-compliance of individuals within experiments, and impracticality.\n\nNon-compliance is an issue that can be solved pretty easily with an instrumental variable, given a few assumptions about the non-compliance.",
    "crumbs": [
      "Home",
      "Magic of Randomisation"
    ]
  },
  {
    "objectID": "latent/pca.html",
    "href": "latent/pca.html",
    "title": "Principle Components Analysis",
    "section": "",
    "text": "Note: PCA is not a latent variable model itself, but can be used to approximate a latent variable model.\n\nPrinciple components analysis (PCA) takes observed variables (called features), and change them into new variables called principle components, without losing any information.\nEssentially, what PCA does is it takes our features, and finds the “axis” in which there is the most variation, and makes that the 1st component. Then it finds the “axis” with the 2nd most variation, and makes that the 2nd component, and so on.\n\n\n\n\n\n\nNote that each principle component is orthogonal/uncorrelated with each other by design. Also note that the 1st component explains the most variance in our features, then the 2nd component, and so on.\n\nEach new principle component is a weighted average of the observed variables:\n\\[\n\\begin{align}\n\\comp_1 & = \\omega_{11}\\feat_1 + \\omega_{21}\\feat_2 + \\omega_{31}\\feat_3 + \\dots \\\\\n\\comp_2 & = \\omega_{12}\\feat_1 + \\omega_{22} \\feat_2 + \\omega_{32}\\feat_3 + \\dots \\\\\n\\end{align}\n\\]\nThe \\(\\omega\\)’s are the weights of each observed feature in creating a principle component, and are determined by the correlation matrix of the features. For interpreting a component, we often “normalise” the weights \\(\\omega\\) to get the correlation between a observed feature and a principle component:\n\\[\nCorr(\\feat_i, \\comp_j) =  \\omega_{ij} \\cdot \\sqrt{Var(\\comp_j)}\n\\]\n\nThis value is also called a component loading. Note that this is only true given we perform PCA on the correlation matrix (which is standard, but you can use a covariance matrix).\n\nInterpreting the principle components is identical to that of factor analysis - just using correlations rather than factor loadings. Below, the rows are features describing how much an individual trusts different institutions. The columns are the principle components.\n\n\n\n\n\nIn component 1, we see that the correlation is pretty high for all of the variables, and positive. We might conclude component 1 measures general trust in institutions.\nIn component 2, we can see that the loadgins for legal and police are the highest (in absolute terms) and negative. Meanwhile, politicians, pol_parties, and EP are positive and still not too small. The other loadings are quite small. We might interpret this component as sort of a tradeoff between political trust and legal/law enforcement trust, with higher values indicating more political trust, and lower values indicating more trust in legal/police.\nJust like factor analysis, we can also calculate principle component scores, which are the individual values of each principle component for each individual in our data.\n\nTo implement principle components analysis, we do the following:\n\npca &lt;- princomp(~ X1 + X2 + X3,\n                data = my_data,\n                cor = TRUE, #use correlation matrix\n                scores = TRUE, #calculate pc scores\n                na.action=na.exclude)\nsummary(pca)\n\nNow, to get the component loadings/corelation, we do the following:\n\n# grab the weights\nweights &lt;- loadings(pca)\n\n# grab the sqrt of variance\nsqrt.var &lt;- pca$sdev\n\n# calculate component loadings/correlation\nprint(t(t(weights)*sqrt.var), cutoff = 0, digits=4)\n\nTo access principle component scores, we do:\n\npca$score\n\nIf we are performing other statistical analysis, we might want to choose how many of the new principle components we want to use. A scree-plot shows the percentage of variance each component explains.\n\nscreeplot(pca, type='l', main=\"\")\n\n\nTo choose the amount of components to use, we look for the “elbow” in the plot - basically when adding another additional PC does not really increase the amount of variance explained significantly anymore.",
    "crumbs": [
      "Latent Variable Models",
      "Principle Components Analysis"
    ]
  },
  {
    "objectID": "latent/identify.html",
    "href": "latent/identify.html",
    "title": "Identification and Rotation",
    "section": "",
    "text": "Make sure you have read all the previous pages on factor analysis before this.\n\nWhen estimating factor analysis models, we encounter many identification issues. One identification issue is rotation.\nFor example, imagine our latent variable is left-right political spectrum leaning. Our latent variable could use positive values for right-wing and negative values for left-wing. Or, it could do the opposite. There is no real substantive difference.\nThus, in 1-factor models, most software will arbitrarily choose a rotation, as it does not really make a huge difference.\nIn multiple factor models, rotation becomes more complex. Because of some linear algebra, you can have almost infinite rotations of factors that make no difference. Just like in the 1-factor model, software will arbitrarily choose one rotation.\nHowever, in multiple factor models, the chosen rotation can make a big difference in how easy it is to interpret our solutions.\n\nThe default rotation is often an orthogonal rotation. This means the factors (if visualised in space) are perpendicular to each other, with zero correlation.\nAn oblique rotation is one where the factors are allowed to be correlated with each other (non-perpindicular).\n\n\n\n\n\n\nThe oblique rotations are generally easier to interpret. This is because they make some items have close to 0 factor loadings, which means we can easily say that factor is not measuring that specific item.\nRotations are a unique identification issue. Factor analysis also has some lack-of-parameters identification issues that result in no possible solutions. To avoid this, we should generally have at least 3 items for a 1-factor model, and 2 items per factor for a multiple-factor model.",
    "crumbs": [
      "Latent Variable Models",
      "Identification and Rotation"
    ]
  },
  {
    "objectID": "latent/longitudinal.html",
    "href": "latent/longitudinal.html",
    "title": "Lagged Response Models",
    "section": "",
    "text": "A lagged response model is a regression model that says that the outcome variable not only depends on the input variables, but also previous values of the outcome from the past.\n\\[\n\\eta_{it} = \\beta_0 + \\underbrace{\\beta_1 X_{1,it} + \\beta_2 X_{2,it}}_{t} + \\underbrace{\\beta_3 \\ \\eta_{i,t-1}}_{t-1} + \\eps_{it}\n\\]\n\n\n\n\n\n\n\n\nexample2\n\n\nY1\n\nη (t-1)\n\n\n\nY2\n\nη (t)\n\n\n\nY1-&gt;Y2\n\n\nLag\n\n\n\nX1\n\nX1 and X2\n\n\n\nX1-&gt;Y1\n\n\n\n\n\nX1-&gt;Y2\n\n\n\n\n\n\n\n\n\n\n\nRecall the response variable is itself a latent variable, measured with observed items. This also implies a measurement model relating the latent response with each observed item:\n\\[\n\\b X_{t} = \\b\\tau_t + \\b\\eta'\\b\\lambda_t + \\b\\delta_{t}\n\\]\n\nThis is just the model for one (of many) items. Note how the parameters have the subscript \\(t\\) - this implies a different measurement model for each item, at each time period.\n\nHowever, because each item is observed over multiple time periods, it is likely that an item from \\(t=1\\) is correlated with the same item from \\(t=2\\) ( autocorrelation). We have to take this into account in one of two ways:\n\nWe could constrain the model, assuming that the measurement model is identical throughout different time periods. That means \\(\\tau_t\\) and \\(\\lambda_t\\) will now no longer depend on the time period (the same for all time periods, for the same item).\nOr we could complicate the model, by including correlation between the error terms \\(\\eps\\) for the same item at different times.\n\n\nTo implement lagged response models, we need the lavaan package:\n\nlibrary(lavaan)\n\nFor a constrained two-period model, the estimation is as follows:\n\nmodel &lt;- '\n# explanatory variables Measurement models \n  explanatory1 =~ NA*X1 + X2 + X3\n  explanatory2 =~ NA*Z1 + Z2 + Z3\n\n# response variables for time = 1 and time = 2\n    response1 =~ la*Y1t1 + lb*Y2t1 + lc*Y3t1\n    response2 =~ la*Y1t2 + lb*Y2t2 + lc*Y3t2\n\n# time = 1 model\n  response1 ~ explanatory1 + explanatory2\n  \n# time = 2 model with lagged time = 1\n  response2 ~ explanatory1 + explanatory2 + response1\n\n# Fixing the variances of the exogenous latent variables LR and AL  \n  LR~~1*LR\n  AL~~1*AL\n'\n\nsem &lt;- sem(model, data = my_data, missing=\"FIML\")\nsummary(sem)\n\n\nThe labels la*, lb*, and lc* indicate to R to make all factor loadings with the same labels equivalent to each other. Notice how we put the same labels on the different time period measurement models.\n\nFor a unconstrained model, the code is as follows:\n\nmodel &lt;- '\n# explanatory variables Measurement models \n  explanatory1 =~ NA*X1 + X2 + X3\n  explanatory2 =~ NA*Z1 + Z2 + Z3\n\n# response variables for time = 1 and time = 2\n    response1 =~ Y1t1 + Y2t1 + Y3t1\n    response2 =~ Y1t2 + Y2t2 + Y3t2\n\n## Error covariance in the measurement model\n  Y3t1 ~~ Y3t2\n\n# time = 1 model\n  response1 ~ explanatory1 + explanatory2\n  \n# time = 2 model with lagged time = 1\n  response2 ~ explanatory1 + explanatory2 + response1\n\n# Fixing the variances of the exogenous latent variables LR and AL  \n  LR~~1*LR\n  AL~~1*AL\n'\n\nsem &lt;- sem(model, data = my_data, missing=\"FIML\")\nsummary(sem)",
    "crumbs": [
      "Latent Variable Models",
      "Lagged Response Models"
    ]
  },
  {
    "objectID": "latent/class.html",
    "href": "latent/class.html",
    "title": "Latent Class Model",
    "section": "",
    "text": "Latent class models are an extension of factor analysis, that deals with cases of a categorical latent variable with categorical/binary observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe factors a categorical variable, which each category \\(c\\) called a latent class. We also have categorical observed items. The latent class models connects each item with a factor through a item response probability:\n\\[\n\\begin{align}\n& Pr(X_1 = \\text{category }k_i \\ | \\ \\xi = \\text{category }c)\\\\\n& Pr(X_2 = \\text{category }k_i  \\ | \\ \\xi = \\text{category }c)\\\\\n& Pr(X_3 = \\text{category }k_i  \\ | \\ \\xi = \\text{category }c)\\\\\n\\end{align}\n\\]\n\n\\(k_1\\) is one specific category of item \\(1\\), same for \\(k_2\\) for item \\(2\\), and so on. \\(c\\) is one specific category of the factor.\n\nWe also have another part of the measurement mode, the structural model, which determines the probability of each category in the factor:\n\\[\n\\alpha_c = Pr(\\xi=\\text{category }c)\n\\]\nInterpretation of the latent factor depends on the item response probabilities. Below, the columns are the different classes/categories of the factor, and the big rows are each item.\n\n\n\n\n\nThe first class (the first column) has the highest probabilities if individuals never worry about crime, no real effect on quality of life, never worry about burglary, and no real effect on quality of life. Thus, we can conclude this first category of the latent variable is something like - not worried about crime.\nThe second class (the 2nd column), where the top responses have the highest probabilities except for the frequency of worry about burglaries - where the probabilities are highest for some of the time and just occasionally. This suggests that this second category is measuring something like - only worried about burglary, and no other crime.\nWe can also create factor scores - which is a little different, because now we are basically assigning every unit in our data to a category of the latent factor. This is done by calculating the posterior probability of being in each class:\n\\[\n\\widehat{Pr}(\\xi = \\text{category }c \\ | \\ X_1 = \\text{category }k_1, X_2 = \\text{category }k_2, \\dots )\n\\]\nWe calculate this probability for all categories \\(c\\) in the factor. Whichever category \\(c\\) of the latent variable has the highest probability, is the category a unit is assigned to.\n\nThis can be considered quite similar to that of cluster analysis, which will be introduced later.\n\n\nTo implement latent class models, we will need the polLCA package:\n\nlibrary(poLCA)\n\nThis package requires that our categories of items are labelled starting with 1. This means if you have a binary variable of 0 and 1, you will need to change it to 1 and 2.\nTo begin, we will first need to create a vector of our item names:\n\nvars &lt;- c(\"item1\",\"item2\",\"item3\",\"item4\")\n\nThen, let us fit our model as follows:\n\nform &lt;- cbind(item1, item2, item3, item4) ~1\nmodel &lt;- poLCA(form,\n              my_data[,vars],\n              nclass = 2, #number of categories for factor\n              na.rm = F,\n              nrep = 10) \n\n\nna.rm = F means to include missing values when estimating (which is recommended). nrep = 10 indicates how many times to run the gradient descent algorithm - more is better, but will take longer.\n\nThe traditional output is hard to read, so we will use a function:\n\n# function\nLCA.probs &lt;- function(res){\n  probs &lt;- res$probs\n  item.p &lt;- NULL\n  for(i in seq_along(probs)){\n        m.tmp &lt;- t(probs[[i]])\n        rownames(m.tmp) &lt;- paste(names(probs)[i],colnames(probs[[i]]),sep=\".\")\n        item.p &lt;- rbind(item.p,m.tmp)\n  }\n  item.p &lt;- round(item.p,3)\n  class.p &lt;- res$P\n  names(class.p) &lt;- colnames(item.p)\n  list(item.probabilities=item.p,class.probabilities=class.p)\n}\n\n# output results\nLCA.probs(model)\n\nWe can calculate factor scores/classification as follows:\n\nmodel$predclass\n\nWe can choose our model based on the AIC or BIC score.\n\nmodel$aic\nmodel$bic",
    "crumbs": [
      "Latent Variable Models",
      "Latent Class Models"
    ]
  },
  {
    "objectID": "latent/irt.html",
    "href": "latent/irt.html",
    "title": "Item Response Theory",
    "section": "",
    "text": "Item Response Theory (IRT) is an extension of factor analysis, that deals with cases with a continuous latent factor but a binary/categorical set of observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nWe must have at least 3 observed items for estimation purposes. The items are associated with the continuous latent factor with a binomial logistic model:\n\\[\n\\begin{align}\n\\text{logit}[Pr(X_1 = 1 \\ | \\ \\xi)] & = \\tau_1 + \\lambda_1 \\ \\xi \\\\\n\\text{logit}[Pr(X_2 = 1 \\ | \\ \\xi)] & = \\tau_2 + \\lambda_2 \\ \\xi \\\\\n\\text{logit}[Pr(X_3 = 1 \\ | \\ \\xi)] & = \\tau_3 + \\lambda_3 \\ \\xi \\\\\n\\end{align}\n\\]\n\nFor categorical (more than 2 categories) items, a multinomial or ordinal logistic model is used. But this is pretty rare for someone to actually fit a model of this type.\n\n\\(\\tau_i\\) is the intercept of the model, called the difficulty. \\(\\lambda_i\\) is the coefficeint that describes the relationship between any item \\(X_i\\) and the factor. These are called factor loadings. It is also called the discrimination parameter.\nThese factor loadings \\(\\lambda\\) are interepreted in a very similar way to factor analysis. The intercepts \\(\\tau_i\\) also tell us how “common” a value of 1 for the item is. For example, below, the items are policies that politicians support/don’t oppose.\n\n\n\n\n\n\na1 are the loadings \\(\\lambda\\), and d is the intercept. The rows are binary items - with 1 indicating a politician supports that, and 0 indicating a politician opposes that.\n\nWe can first look at the signs of \\(\\lambda\\): The latent factor increases with agreeing (a value of 1) with PartialBirth, CapitalGains, and CAFTA. The latent factor decreases with agreeing (a value of 1) with StemCell, IraqWithdraw, Immigration, and Minimum Wage.\nFrom this, we can kind of get a sense that latent variable is measuring right-left political affiliation, with higher values indicating right-wing, since agreeing left wing policies like IraqWithdraw, Immigration, and Minimum Wage are negative.\nThe absolute size of the factor loadings are quite similar - we can see that CAFTA is the least important item to the factor.\n\nTo run an item response theory mode, we need the mirt package:\n\nlibrary(mirt)\n\nThen, we run the model as follows (make sure to subset your data to only include the items).\n\nmodel &lt;- mirt(data = my_data, model = 1, SE = TRUE)\ncoef(result)\n\n\nmodel = 1 indicates how many factors you want to include.\n\nTo get factor scores, we do the following:\n\nscores &lt;- as.vector(fscores(model, method = \"EAP\"))\n\n\n\n\n\n\n\nMore on Choosing Between Models\n\n\n\n\n\nTo compare and choose between different models, we have a few ways.\n\nWe can use a likelihood ratio test for nested models (see the regression section for more details).\n\n\nlavTestLRT(model1, model2)\n\n\nWe can use a global goodness of fit test - essentially a likelihood ratio but with the full sample covariance matrix as the null model. We want to fail to reject the null, because we want our model to be as close to the sample covariance matrix as possible.\n\n\nlavTestLRT(model)\n\n\nWe can use AIC and BIC to compare models since factor models are estimated with MLE. These are included in the output.\n\nUnfortunately, fit indicies like RMSEA do not work on IRT models.",
    "crumbs": [
      "Latent Variable Models",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "latent/factor.html",
    "href": "latent/factor.html",
    "title": "Factor Analysis Model",
    "section": "",
    "text": "Factor analysis is a way to model a unobserved continuous latent variable \\(\\xi\\), based on a bunch of continuous observed items \\(X_1, X_2, \\dots\\).\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nEach item \\(X_i\\) is connected with the latent factor \\(\\xi\\) by a linear regression:\n\\[\n\\begin{align}\nX_1 & = \\tau_1 + \\blue{\\lambda_1} \\ \\xi + \\delta_1 \\\\\nX_2 & = \\tau_2 + \\blue{\\lambda_2} \\ \\xi + \\delta_2 \\\\\nX_3 & = \\tau_3 + \\blue{\\lambda_3} \\ \\xi + \\delta_3 \\\\\n\\end{align}\n\\]\n\n\\(\\tau\\) is the intercept of the model. \\(\\lambda\\) is the coefficient, called the factor loadings. \\(\\delta\\) is the error term - the part of an item not explained by the factor - they are called the unique factors.\n\n\n\n\n\n\n\nDetails: Assumptions of the Model\n\n\n\n\n\n\nWe make a few assumptions on this model:\n\nWe assume the factor is continuous, and normally distributed with mean \\(\\kappa\\) and variance \\(\\phi\\). We often assume \\(\\kappa = 0\\) and \\(\\phi = 1\\) for identification purposes.\nWe assume the items are also normally distributed.\nWe assume that the error terms are normally distributed \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\), that the different error terms \\(\\delta_1, \\dots, \\delta_p\\) are uncorrelated with each other. This implies that the correlation between any two items is entirely explained by the factor (there is no separate correlation between items).\nWe assume the factor is uncorrelated with the error term (exogeneity).\n\n\n\n\n\nThe factor loadings \\(\\blue{\\lambda_i}\\) represent the relationship/covariance between any item and a factor. These factor loadings help us interpret our latent variable. The sign of the factor loading tells us the direction in which our latent variable is measuring. The absolute size of the factor loading tells us how important that item is to the factor.\nFor example, take this factor variable called personality, explained by a set of items (rich, admire, success, respect) that show how important a certain quality is to an individual.\n\n\n\n\n\nWe can see all the factor loadings \\(\\lambda\\) are positive, which means that the higher values of the personality factor is measuring higher levels of importance of being rich, being admired, being successful, and being respected.\nWe can also see the loadings for respect and success are much higher than admire or rich. Thus, we can conclude the personality factor is more measuring the importance of being respected or successful, than being rich or admired.\n\n\n\n\n\n\nDetails: More and Factor Loadings\n\n\n\n\n\n\nIf all our items are standardised to a standard normal distribution, that also implies that our factor loadings \\(\\lambda\\) are equal to the correlation coefficients between items and factor.\nWe can conduct hypothesis testing with each factor loading \\(\\lambda\\) with a z-test to see if there is a significant relationship between a factor an an item.\n\n\n\n\n\nTo implement factor analysis, we will need the psych and GPArotation package:\n\nlibrary(psych)\nlibrary(GPArotation)\n\nFirst, we should get rid of missing observations:\n\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\nFor factor analysis with one factor, we use the syntax:\n\nfa &lt;- fa(data[,items], nfactors=1, fm=\"ml\")\nprint(fa1)",
    "crumbs": [
      "Latent Variable Models",
      "Factor Analysis Models"
    ]
  },
  {
    "objectID": "latent/scores.html",
    "href": "latent/scores.html",
    "title": "Reliability and Factor Scores",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nRecall our measurement models linking each item \\(i\\) to our factor:\n\\[\nX_i = \\tau_i + \\lambda_i\\ \\xi + \\delta_i\n\\]\nThe the error term has a mean of 0 and a variance of \\(\\theta_i\\). The variance of each item can be shown to equal:\n\\[\nVar(X_i) = \\lambda_i^2 + \\theta_i\n\\]\nThus, this allows us to essentially “split” the variance in any item into two parts:\n\n\\(\\lambda_i^2\\) is the part of the variance in the item explained by the latent factor. We call this the communality of the item \\(i\\).\n\\(\\theta_i\\) is the residual variance, the part of the variance not explained by our factor.\n\nWe can also calculate the percentage/proportion of variance in item \\(i\\) that our factor explains, called the reliability:\n\\[\n\\text{Reliability} = \\frac{\\lambda_i^2}{Var(X_i)} = \\frac{\\lambda_i^2}{\\lambda_i^2+\\theta_i}\n\\]\n\nIf all items are standarised to a standard normal, then \\(\\lambda_i^2\\) is equal to the reliability.\n\nItems with higher reliability are considered more “accurate” measures of the latent variable factor. They thus ensure better model estimation - and when a factor has low relaibility, we will often drop it.\nFactor scores \\(\\widehat\\xi\\) are essentially values of the latent variable for individuals in our study. This allows us to use our observed items to calculate the latent variable value that any individual should have, which we can then put into another statistical model.\n\\[\n\\widehat\\xi = \\omega_0 + \\omega_1 X_1 + \\omega_2 X_2 + \\dots\n\\]\nThe weights are calculated based on the communalities. The items with the highest communalities tend to get the strongest weights, while the items with the least communalities get the smallest weights.\n\nTo calculate factor analysis, we have to first run a factor analysis model in the same way we did in the last page.\n\nlibrary(psych)\nlibrary(GPArotation)\n\n# eliminate missing observations\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\n# factor model\nfa &lt;- fa(data[,items], nfactors=1, fm=\"ml\")\n\nR will automatically calculate factor scores in the estimation process, so all we have to do is access it within our output object:\n\nfa$scores\n\nYou can save this into your dataset, and use for other purposes.",
    "crumbs": [
      "Latent Variable Models",
      "Reliability and Factor Scores"
    ]
  },
  {
    "objectID": "soo/interact.html",
    "href": "soo/interact.html",
    "title": "Fully Interacted Estimator",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nFrom selection on observables, we know that our causal effect is a weighted average:\n\\[\n\\ate = \\sum\\tau_\\text{X} \\cdot Pr( X)\n\\]\nNotice how the weights are the probability of the confounder values of \\(\\b x\\). With some complex math (Angrist 1998), we can actually show that OLS actually estimates:\n\\[\n\\hat\\beta_\\text{OLS} = \\sum \\tau_X\\cdot \\underbrace{\\frac{Var(D_i | X)Pr(X)}{\\sum Var(D_i | X^c)Pr( X^c)}}_{\\text{weight}}\n\\]\n\nWhere \\(X^c\\) is the complement (not \\(X\\)).\n\nThese weights are not equivalent to the selection on observables \\(\\ate\\). Thus, if not all \\(\\tau_\\text{X}\\) are exactly the same (which implies heterogeneity), then our linear regression estimator will incorrectly estimate the ATE.\nHeterogeneity is present in almost all situations we are interested in. Lin (2013) proposes the fully interacted estimator, which allows for consistent estimation of the ATE even with heterogeneity:\n\\[\nY_i = \\alpha + D_i\\ \\ate + \\underbrace{(\\b X_i - \\b{\\mean X})'\\b\\beta + D_i(\\b X_i - \\b{\\mean X})'\\b\\gamma}_{\\text{interactions with de-meaned covariates}}+ \\eps_i\n\\]\nThe new OLS estimate of \\(\\ate\\) in this estimator will technically still be a biased estimator of the ATE, but the bias is negligible.\n\nBefore you implement the estimator, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the estimatr package:\n\nlibrary(estimatr)\n\nThen, we can use the lm_lin() function to estimate:\n\nate &lt;- estimatr::lm_lin(outcome ~ treatment,\n                        covariates = ~ covariate1 + covariate2,\n                        data = my_data)\nsummary(ate)\n\nThe output will be the ATE - the average treatment effect for all units in the study.",
    "crumbs": [
      "Selection on Observables",
      "Fully Interacted Estimator"
    ]
  },
  {
    "objectID": "soo/ipw.html",
    "href": "soo/ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nLet us look at this example, with a confounder.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nReceiving Scholarship\n\n\n\nY\n\nUniversity Grades\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s pretend there are only dumb and smart people (for simplicity). Our treated and control groups might be:\n\n\n\n\n\n\n\nTreated (Got Scholarship)\nUntreated (Did not get scholarship)\n\n\nSmart Students (x4)\nSmart Students (x1)\n\n\nDumb Students (x1)\nDumb Students (x4)\n\n\n\nOur two groups have pre-existing differences. However, by emphasising certain individuals, we can make it seem like there are no more imbalances. For example, weighting might make our above table become:\n\n\n\n\n\n\n\nTreated (Got Scholarship)\nUntreated (Did not get scholarship)\n\n\nSmart Students (x4)\nSmart Students (emphasise to x4)\n\n\nDumb Students (emphasise to x4)\nDumb Students (x4)\n\n\n\n\nSee how the underrepresented individuals in each group (treated/untreated) were weighted upwards. More technically, inverse probability weighting emphasises/weights an individual by the inverse of their likelihood to receive treatment.\n\nWe can see there is no more pre-existing differences after weighting. Thus, selection bias has been solved.\n\nBefore you inverse probability weighting, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the estimatr package:\n\nlibrary(estimatr)\n\nTo estimate the propensity scores and weights, we can use the glm() command:\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity, type = \"response\")\nmy_data$ipw &lt;- ifelse(mydata$D == 1,\n                      1/my_data$pscore,\n                      1/(1-my_data$pscore))\n\nFinally, we need to use the lm_robust() command to estimate our causal effects:\n\nate &lt;- estimatr::lm_robust(Y ~ D, data = my_data, weights = ipw)\nsummary(ate)\n\nThe output will be the ATE - the average treatment effect for all units in the study.",
    "crumbs": [
      "Selection on Observables",
      "Inverse Probability Weighting"
    ]
  },
  {
    "objectID": "soo/regress.html",
    "href": "soo/regress.html",
    "title": "Linear Regression Estimator",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nSelection on observables is about controlling for confounders. Linear regression is a very natural way to control for confounders.\nUsing this idea, we can implement causal inference with regression, with one of the explanatory variables being our treatment variable, and the rest of the explanatory variables being control variables.\n\\[\nY_i = \\alpha + D_i\\ \\ate + \\covs + \\eps_i\n\\]\nOur ordinary least squares (OLS) estimate \\(\\hat\\ate\\) is an unbiased estimator of the true ATE given three conditions are met:\n\nWe meet the selection on observables assumption of conditional ignorability. Conditional ignorability implies exogeneity, which means the estimate is unbiased.\nThe relationship between our continuous confounders and outcome variable is linear. This is because if the true relationship between these two is not linear, then our linear model is wrong, so it is not properly controlling for confounders.\nThere is no heterogeneity in treatment effects. Angrist (1998), Lin (2013), and Słoczyński (2022) have proven that when there is heterogeneity, OLS is estimating another quantity that is not the ATE. This is covered in more detail on fully interacted estimator page\n\n\nHeterogeneity means that different individuals have different individual treament effects \\(\\tau_i\\). OLS only estimates the ATE if there is homogeneity - all \\(\\tau_i\\) are equal.\n\n\nBefore you implement the estimator, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the fixest package:\n\nlibrary(fixest)\n\nTo run a regression estimator, we do:\n\nfeols(outcome ~ treatment + covariate1 + covariate2,\n      data = data,\n      se = \"hetero\")\n\n\nWe typically assume heteroscedasticity, so we use heteroscedasticity-robust standard errors. If you can prove homoscedasticity, then you can use normal standard errors.\n\nThe coefficient for the treatment variable will be the ATE - the average treatment effect for all units in the study.\n\nAssuming you have met all the assumptions of selection on observables, and the special assumptions for the linear regression estimator shown above.",
    "crumbs": [
      "Selection on Observables",
      "Linear Regression Estimator"
    ]
  },
  {
    "objectID": "soo/soochoose.html",
    "href": "soo/soochoose.html",
    "title": "Choosing an Estimator",
    "section": "",
    "text": "We know how selection on observables works now. But how do we actually implement selection on observables? Below are a list of estimators and their strengths/weaknesses. You can use multiple simultaneously for robustness.\n\n\n\n\n\n\nLinear Regression Estimator\n\nEstimand: ATE\nPros: very simple, works well with small datasets.\nCons: 1) requires linear relationship between confounders and outcome, 2) does not work when there are heterogenous treatment effects.\n\n\nKevin’s Comments: since heterogeneity is so common in almost everything, I would recommend against using this estimator unless nothing else is possible. The fully interacted estimator (below) is just a better version of this.\n\n\n\nFully Interacted Estimator\n\nEstimand: ATE\nPros: 1) modified regression to allow for heterogenous effects, 2) still relatively simple.\nCons: requires linear relationship between confounders and outcome\n\n\n\nDistance Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome. 2) relatively intuitive idea.\nCons: 1) can be badly biased when more than 3-5 confounders, 2) throws out unmatched data so wastes data.\n\n\nKevin’s Comments: there is little reason to use distance matching over genetic matching, unless your machine physically cannot estimate genetic matching.\n\n\n\nPropensity Score Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) can handle larger amounts of confounders than distance matching.\nCons: 1) needs a large sample size to not be biased, 2) throws out unmatched data so wastes data.\n\n\nKevin’s Comments: there is little reason to use propensity score matching over genetic matching, unless your machine physically cannot estimate genetic matching.\n\n\n\nGenetic Matching\n\nEstimand: ATT\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) shown to be the best matching estimator\nCons: 1) throws out unmatched data so wastes data, 2) can be computationally taxing.\n\n\n\nInverse Probability Weighting\n\nEstimand: ATE\nPros: 1) non-parametric, so no need to assume the type of relationship between confounders and outcome, 2) does not waste data like matching methods do.\nCons: requires a large sample size to be unbiased.",
    "crumbs": [
      "Selection on Observables",
      "Choosing an Estimator"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resources on Causal Inference and Social Science Statistics",
    "section": "",
    "text": "For my personal website, see http://kevinli03.github.io\nThis repository contains resources on causal inference and social statistics, specifically focused on applications to politics and economics.\n\nUse the top navigation bar to navigate between different methods.\nUse the side navigation bar to navigate within a method.\n\nR-code for implementation is included. There are some stuff that doesn’t fit into any category, which I have included in the home page sidebar.\n\nNotation Notes:\n\nFor potential outcomes, green means under treatment, and red means under control.\n\\(i\\) refers to units within our data, and \\(t\\) refers to time periods.\nGreek letters are used for parameters that need to be estimated. I typically use \\(\\beta\\) for most coefficients multiplied to variables, \\(\\tau\\) to represent causal effects, and \\(\\eps\\) to represent the error term.",
    "crumbs": [
      "Home",
      "Homepage"
    ]
  },
  {
    "objectID": "did/ifect.html",
    "href": "did/ifect.html",
    "title": "Liu et al (2024) Interactive Fixed Effects Counterfactual Estimator",
    "section": "",
    "text": "See Liu et al (2024) for more technical details.\n\nLiu et al (2024) proposes to solve the issues with two-way fixed effects by just not doing comparisons and weighting. Instead, we estimate the missing counterfactuals and then calculate the treatment effect.\n\nThis is similar to Gardner (2021) and his 2-stage difference-in-differences.\n\nThus, the goal of this interactive fixed effects counterfactual estimator (IFEct) is to estimate \\(\\pCred\\). They model these counterfactuals as:\n\\[\n\\pCred = \\underbrace{\\unit + \\time + \\cov}_{\\text{classic TWFE}} + \\b\\lambda_i'\\b\\xi_t + \\eps_{it}\n\\]\nAs you can see, the first part of this model follows the two-way fixed effects model for counterfactuals \\(\\pCred\\), just like in Gardner (2021). However, Liu et al (2024) add an extra term: \\(\\b\\lambda_i'\\b\\xi_t\\).\n\n\\(\\b\\xi_t\\) is a vector of latent variables/factors \\(\\xi_{1t}, \\xi_{2t}, \\dots\\). These latent variables change in magnitude depending on time \\(t\\) (they are time-varying).\n\\(\\b\\lambda_i\\) is a vector of factor loadings \\(\\lambda_{1i}, \\lambda_{2i}, \\dots\\), that describes the “influence” every latent factor has on unit \\(i\\). Different units \\(i\\) can have different relationships with each factor.\n\n\nThese latent factors and factor loadings are estimated with control data \\(D_{it} = 0\\). For more on latent variables, see the section on latent variable modelling.\n\nThis is essentially a complicated way of saying that this model adjusts for potential violations in parallel trends. This is because the latent factors are time-varying, and different units \\(i\\) can have different relationship with each factor, meaning their can be non-parallel trends.\nThis flexibility in the estimation of this model makes it more robust to minor violations of parallel trends. It can also better adjust for heterogeneity, not just between different initial treatment times, but also different covariate values.\n\nTo implement the IFEct estimator, we will need the package fect:\n\nlibrary(fect)\n\nBefore we start, we will need to make sure our unit variable is numerical (so units 1, 2, …). This is often not the case in most datasets (for example, country is usually UK, USA,…, not 1,2,…).\n\ndta &lt;- my_data # new data frame\ndta &lt;- dta %&gt;%\n  group_by(unit) %&gt;%\n  mutate(id = cur_group_id()) %&gt;%\n  ungroup() # change country into a numbered variable\n\nNow, we can estimate our model as follows:\n\nset.seed(17)\nmodel &lt;- fect(outcome ~ treatment + covariate1 + covariate2,\n             data = dta,\n             index = c(\"id\",\"time\"), #don't change id, change time to your period variable\n             method = \"ife\",\n             se = TRUE,\n             nboots = 100) # more is better, but takes longer\nprint(model)\n\nWe can also plot our results:\n\nplot(model)",
    "crumbs": [
      "Difference-in-Differences",
      "Liu et al (2024) IFEct"
    ]
  },
  {
    "objectID": "did/did2s.html",
    "href": "did/did2s.html",
    "title": "Gardner (2021) Two-Stage Difference-in-Differences",
    "section": "",
    "text": "See Kyle Butts’ github page for more info on this estimator. Asjad Naqvi also has a helpful page.\n\nGardner (2021) proposes to solve the issues with two-way fixed effects by just not doing comparisons and weighting. Instead, we estimate the missing counterfactuals and then calculate the treatment effect.\nThus, the goal of 2-stage difference-in-differences is to estimate \\(\\pCred\\). Using the idea of two-way fixed effects, we can derive a model for these missing counterfactuals:\n\\[\n\\pCred = \\unit + \\time + \\cov + \\eps_{it}\n\\]\n\nThis equation is obtained by taking the two-way fixed effects equation and plugging in \\(D_{it} = 0\\).\n\nGardner’s 2-stage difference-in-differences uses this equation and framework above, and attempts to estimate \\(\\pCred\\).\n\nFirst, Gardner uses only control units (\\(D_{it} = 0\\)) in a regression to estimate \\(\\time\\), \\(\\unit\\), and \\(\\beta\\). The choice of using only control units is to ensure that the estimation of hypothetical controls is accurate and not influenced by the treatment.\n\n\\[\nY_{it|D = 0} = \\unit + \\time + \\cov + \\eps_{it}\n\\]\n\nUsing estimated \\(\\widehat\\unit\\), \\(\\widehat\\time\\), and \\(\\widehat\\beta\\), we can estimate \\(\\pCred\\) counterfactuals for the treated.\n\n\\[\n\\red{\\widehat Y_{it}^{(0)}} = \\widehat\\alpha_i + \\widehat\\gamma_t + \\b X_{it}' \\widehat{\\b\\beta}\n\\]\n\nOnce we have estimated \\(\\pCred\\), we can calculate the treatment effects for the treated units, as we have an estimate for the hypothetical outcomes.\n\n\\[\n\\att = \\pT - \\pCred\n\\]\n\nThen, we average all the individual treatment effects to get the ATT.\n\n\nSteps 3-4 are a simplification - Gardner uses a second regression to calculate the final treatment effects, rather than directly computing the treatment effects with the estimated counterfactual.\n\n\nTo implement 2-stage difference-in-differences, we need the did2s package:\n\nlibrary(did2s)\n\nFor the estimation of the ATT, we do the following:\n\nmodel &lt;- did2s(data = my_data,\n               yname = \"outcome\",\n               first_stage = ~ covariate1 + covariate2 | unit + time,\n               second_stage = ~i(treatment),\n               treatment = \"treatment\",\n               cluster_var = \"unit\")\nmodel\n\n\nIf you have no covariates, replace the covariates with 0 (so it looks something like ” ~ 0 | unit + time”)\n\nFor estimation of dynamic treatment effects (and testing of parallel trends), we do the following:\n\nmodel &lt;- did2s(data = my_data,\n               yname = \"outcome\",\n               first_stage = ~ covariate1 + covariate2 | unit + time,\n               second_stage = ~i(rel_time, ref = c(-1, Inf)),\n               treatment = \"treatment\",\n               cluster_var = \"unit\")\nmodel\niplot(model) #to view dynamic treatment effects plot\n\n\nIf you have no covariates, replace the covariates with 0 (so it looks something like ” ~ 0 | unit + time”)\nNote: the inf is the reference for the never-treated control group, for the variable time_to_treat. If it is NA’s, then you do not need to include it, just put ref = -1.",
    "crumbs": [
      "Difference-in-Differences",
      "Gardner (2021) 2-stage DiD"
    ]
  },
  {
    "objectID": "did/cs.html",
    "href": "did/cs.html",
    "title": "Callaway and Sant’Anna Flexible Matching and Reweighting Estimator",
    "section": "",
    "text": "Callaway and Sant’Anna (2021) proposes to solve the issues with two-way fixed effects by carefully selecting the comparisons, and properly weighting the comparisons:\n\nWe first divide all units \\(i\\) into groups, based on their first initial year of receiving treatment. Each unit who was first treated in year \\(g\\) is assigned to group \\(g\\).\nThen, we define a new causal estimand of interest: what they call the group-time ATT. This is essentially the dynamic treatment effect of group \\(g\\) at time \\(t\\):\n\n\\[\n\\tau_{g, t}^\\text{ATT} = \\green{\\mean Y_t^{(1)}} - \\red{\\mean Y_t^{(0)}} \\ | \\ G_{i, g} = 1\n\\]\n\nWhere \\(G_{i,g}\\) is a variable that equals 1 if unit \\(i\\) is initially treated in group \\(g\\). It is 0 otherwise.\n\n\nNow the issue (as is the case for all DiD), is that the counterfactual \\(\\red{\\mean Y_t^{(0)}}\\) is unobserved. Now, we need some way to estimate it.\nTo estimate the “trend” if the treated units were hypothetically untreated, we look only at the never-treated group. This ensures that we are making proper comparisons (instead of comparing with already-treated units). We use this group to approximate the trend if hypothetically our treated units were untreated.\n\n\nIf this estimation of “trend” makes little sense, see the intro page on DiD.\n\n\nWith the trend, we can now estimate the missing counterfactual, allowing us to obtain the group-time ATT \\(\\tau_{g,t}^\\text{ATT}\\).\nNow, we weight each \\(\\tau_{g,t}^\\text{ATT}\\) together based on how frequent each group \\(g\\) is. By properly weighting, we solve the weighting issue of TWFE.\n\n\nThe final step of weighting can be done to obtain one overall ATT, or dynamic ATTs, or even we can decide not to aggregate and report each individual group-time \\(\\tau_{g,t}^\\text{ATT}\\).\n\nUnlike the other estimators covered, Callaway and Sant’Anna do not rely on regression models. The comparisons are made through grouping and matching, and the individual effects are weighted.\n\nTo implement this estimator, we will need the did package:\n\nNote: there are issues when including covariates, so this method may not be the best if you need covariates to condition for parallel trends.\n\n\nlibrary(did)\n\nThe data will need pre-processing. We will need to make sure the never-treated group has a initial treatment year of 0, and we will need the unit variable to be a numerical id variable.\n\ndta &lt;- my_data \n\n# set treat_year = 0 for never-treated (assuming NA)\ndta$treat_year &lt;- ifelse(is.na(dta_modify$treat_year), 0, dta_modify$treat_year)\n\n# numerical id variable\ndta &lt;- dta %&gt;%\n  group_by(unit) %&gt;%\n  mutate(id = cur_group_id()) %&gt;%\n  ungroup()\n\nNow, we can use the pre-process function in the did package to make the final touches before starting.\n\nset.seed(17)\ndta &lt;- dta %&gt;%\n  pre_process_did(yname = \"outcome\",\n                  tname = \"treatment\",\n                  idname = \"id\", #don't change\n                  gname = \"treat_year\",\n                  allow_unbalanced_panel = T,\n                  data = .)\n\nNow, we can run the analysis:\n\nmodel &lt;- dta[['data']] %&gt;%\n  att_gt(yname = \"outcome\",\n         tname = \"year\",\n         idname = \"id\", #don't change\n         gname = \"treat_year\",\n         control_group = c(\"nevertreated\"),\n         est_method = 'dr',\n         base_period = 'universal',\n         allow_unbalanced_panel = T,\n         data = .)\n\nWe can view our results in multiple ways. For a singular ATT:\n\natt &lt;- did::aggte(model, type = \"simple\", na.rm = TRUE)\nsummary(att)\n\nWe can also see ATT’s grouped by initial treatment year:\n\natt_group &lt;- aggte(model, type = \"group\", na.rm = TRUE)\nggdid(att_group)",
    "crumbs": [
      "Difference-in-Differences",
      "Callaway and Sant'Anna (2021)"
    ]
  },
  {
    "objectID": "did/etwfe.html",
    "href": "did/etwfe.html",
    "title": "Wooldridge (2021) Extended Two-Way Fixed Effects",
    "section": "",
    "text": "For technical details, see Wooldridge (2021) and Wooldridge (2023).\n\nThere are two issues with the two-way fixed effects estimator: forbidden comparisons and issues with weighting. Wooldridge (2021, 2023) proposes the extended two-way fixed effects (ETWFE) estimator, that solves both of these issues by:\n\nThe ETWFE estimator estimates all possible heterogeneous effects separately (by including a lot of interactions in the regression), using only valid comparisons, solving the forbidden comparison problem:\n\n\\[\n\\begin{align}\nY_{it} = & \\ \\underbrace{\\unit + \\time}_{\\text{fixed effects}} + \\cov + \\underbrace{\\blue{\\tau_1} G_i D_{it}T_t + \\blue{\\tau_2}G_i D_{it}T_t \\b X_{it} }_{\\text{interactions for heterogeneity}} + \\eps\n\\end{align}\n\\]\n\n\\(G_i\\) is a categorical variable that indicates what initial treatment period group unit \\(i\\) belongs to. \\(T_t\\) is a categorical variable that indicates the different time periods.\n\n\nMany coefficients \\(\\blue{\\tau_1}\\) and \\(\\blue{\\tau_2}\\) are estimated that capture different heterogeneity in treatment effects.\nThese coefficients are manually aggregated together into one ATT using proper weighting based on how frequent each heterogeneous treatment effect occurs in our sample. This solves the weighting problem of TWFE.\n\n\nWe can also aggregate them in terms of relative years to initial treatment, which allows us to compute dynamic treatment effects. However, this estimator can only compute post-treatment dynamic treatment effects.\n\nWooldridge’s 2023 paper also extends this framework to work with logistic regression, poisson regression, and negative binomial regression, which is useful when we deal with non-continuous outcome variables.\n\n\n\n\n\n\nComparison to Sun and Abraham (2021)\n\n\n\n\n\n\nWooldridge’s extended two-way fixed effects estimator is very similar to Sun and Abraham’s estimator. The three main differences are:\n\nSun and Abraham can calculate dynamic treatment effects for every time period (both pre and post treatment). Wooldridge’s can only calculate post-treatment. Thus, only Sun and Abraham can test for parallel trends.\nSun and Abraham is more strict with covariates for parallel trends, while Wooldridge’s allows for more flexibility and robustness to violations of parallel trends. This is because Wooldridge’s estimator contains far more interactions with covariates.\nWooldridge’s estimator allows for non-linear models (poisson, logistic, etc.).\n\n\n\n\n\n\nTo implement extended two-way fixed effects, we will need the etwfe package:\n\nlibrary(etwfe)\n\nTo estimate all the group-time ATTs, we do the following:\n\nmodel &lt;- etwfe(fml = outcome ~ covariate1 + covariate2,\n              tvar = time,\n              gvar = initial_treat_year, #initial period of treatment for unit\n              data = my_data,\n              vcov = ~unit, # cluster se by unit\n              family = NULL) \n\n\nIf you have no covariates, replace them with a 0, so it looks something like “outcome ~ 0”.\nYou can delete the family argument if you are just using a standard linear model. You can also replace NULL with “logit”, “poisson”, and “negbin”.\n\nWe can then aggregate our group-time ATT’s. To obtain one single ATT, we can use the following code:\n\nemfx(model)\n\nWe can also get dynamic ATT’s for post-treatment periods:\n\ndynamic &lt;- emfx(model, type = \"event\")\nplot(dynamic)",
    "crumbs": [
      "Difference-in-Differences",
      "Wooldridge (2021) ETWFE"
    ]
  },
  {
    "objectID": "did/did.html",
    "href": "did/did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "In April 1992, New Jersey increased its minimum wage. We might wonder how that caused changes in employment We have data from before and after the change:\n\n\n\n\n\n\n\nMar 1992 (Before Min. Wage Increase)\nDec 1992 (After Min. Wage Increase)\n\n\n\\(\\red{20.44}\\)\n\\(\\green{21.03}\\)\n\n\n\n\nRed indicates not treated, green indicates treated.\n\nOur treatment effect after the raise of minimum wage in December 1992 would be:\n\\[\n\\tau_\\text{Dec92} = \\green{Y_\\text{Dec92}^{(1)}} - \\red{Y_\\text{Dec92}^{(0)}}\n\\]\nWe don’t observe the counterfactual in December 1992 without the minimum wage increase.\n\nWe cannot use March 1992 because there could have been something that happened between March and December that changes the trend in employment (like a recession).\n\nDifference-in-differences estimates the hypothetical by looking at another state that did not receive a change in minimum wage during this time - like Pennsylvania:\n\n\n\n\n\n\n\nMar 1992 (Pennsylvania, No Min. Wage Increase)\nDec 1992 (Pennsylvania, No Min. Wage Increase)\n\n\n\\(\\red{23.33}\\)\n\\(\\red{21.17}\\)\n\n\n\n\nThese are employment values in Pennsylvania, where no mininum wage changes occured.\n\nPennsylvania tells us the trend in employment without a change in minimum wage (-2.16). This trend tells us that if New Jersey hadn’t received a minimum wage increase, their employment rate would be the March 1992 level minus 2.16.\n\n\n\n\n\n\n\nMar 92 (NJ, Not Minimum Wage Increase)\nDec 92 (NJ, Hypothetical No Minimum Wage Increase)\n\n\n\\(\\red{20.44}\\)\n\\(\\red{\\text{March 92} - 2.16}\\)\n\n\n\n\nThe March 1992 value is copied from the first table.\n\nNow, we have an approximation of New Jersey’s missing hypothetical outcome, so we can now estimate the causal effect.\n\\[\n\\begin{align}\n\\tau_\\text{Dec92} & = \\green{Y_\\text{Dec92}^{(1)}} - \\red{Y_\\text{Dec92}^{(0)}} \\\\\n& = \\green{Y_\\text{Dec92}^{(1)}} - \\left(\\red{Y_\\text{Mar92}^{(0)}} - 2.16 \\right)\n\\end{align}\n\\]\n\nThis entire idea of using another group’s trend to estimate counterfactuals depends on two assumptions:\n\n\n\n\n\n\n\nAssumption\nDescription\n\n\nParallel Trends\nThis means that had the treated group not received treatment, they would have followed the same trend as the control group.\n\n\nStable Unit Treatment Value Assumption (SUTVA)\nThis means that if Ava is treated, that does not affect Mia’s outcome (and for any other 2 individuals).\n\n\n\n\nI will show how to test for parallel trends later in Dynamic treatment effects.\n\nThere are two types of difference-in-differences designs - classical and staggered, which we will cover in the next few pages.",
    "crumbs": [
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did/staggered.html",
    "href": "did/staggered.html",
    "title": "Stagged Difference-in-Differences",
    "section": "",
    "text": "Ensure you understand the classical difference-in-differences design before starting this.\n\nIn the classical difference-in-differences design, all units in the treatment group receive treatment at the same time. However, in reality, this is not that common. It is very common that some units receive treatment before others.\n\nFor example, in the US, states often adopt different policies at different times - such as the staggered adoption of mail-in voting, or the staggered adoption of the common core curriculum.\n\nThe staggered difference-in-differences design is an extension on the classical design, which allows for staggered implementation of treatment. Essentially, this design is a combination of a bunch of smaller classical difference-in-differences designs.\n\nFor example, consider that we have 4 groups. Group 1 receives the treatment in period 1, group 2 receives the treatment in period 2, and group 3 receives the treatment in period 3. Group 4 never receives the treatment.\nIn this scenario, we essentially have 3 smaller classical difference-in-differences combined into one larger study: one for group 1, one for group 2, and one for group 3.\n\nUnlike the classical design, there are far more estimators to choose from.\n\n\n\nTwo-Way Fixed Effects Estimator\nThe TWFE estimator is possible in staggered DiD just like in the classical design - and is quite simple. However, there are a variety of issues outlined here, that make it more dubious.\n\n\nGardner (2021) 2-Stage DiD\nThis estimator accounts for the issues with TWFE by estimating the missing counterfactuals to find the treatment effect.\n\nKevin’s Notes: very simple and intuitive, however, not very flexible or robust. If you are interested in a better counterfactual estimator, use Liu et al (2024).\n\n\n\nWooldridge (2021) Extended Two-Way Fixed Effects\nThis estimator accounts for the issues with TWFE by adding interaction terms to model heterogeneity.\n\nKevin’s Notes: Can handle non-linear models like poisson and logistic. Very similar to Sun and Abraham. It handles conditional parallel trends more flexibly than Sun and Abraham. However, it cannot estimate pre-treatment dynamic effects, so if you need to test parallel trends use Sun and Abraham.\n\n\n\nSun and Abraham (2021)\nThis estimator accounts for the issues with TWFE by adding interaction terms to model heterogeneity.\n\nKevin’s Notes: Very similar to Wooldridge. It can estimate pre-treatment dynamic treatment effects when Wooldridge cannot. However, it is less flexible and robust when dealing with conditional parallel trends.\n\n\n\nCallaway and Sant’Anna (2021)\nThis estimator accounts for the issues with TWFE through matching and reweighting.\n\nKevin’s Notes: Probably the most well regarded in econometrics. Intuitively, it might be the simplest, however, technically, it is very complex. It is semi-parametric, making it more flexible than regression based methods.\n\n\n\nLiu et al (2022) Counterfactual\nThis estimator accounts for the issues with TWFE by estimating the missing counterfactuals to find the treatment effect.\n\nKevin’s Notes: This is a more complex estimator that (debatedly) might not even be a difference-in-differences estimator, as it draws a lot from synthetic controls. It is quite flexible and preferable to Gardner (2021), but is more complex and hard to explain.",
    "crumbs": [
      "Difference-in-Differences",
      "Staggered DiD Design"
    ]
  },
  {
    "objectID": "did/twfe.html",
    "href": "did/twfe.html",
    "title": "Two-Way Fixed Effects Estimator",
    "section": "",
    "text": "Note: if you have staggered treatment implementation, TWFE can produce incorrect results.\n\nThe two-way fixed effects estimator is the most commonly used estimator in difference-in-differences, used when we have panel data with multiple observations observed in multiple time periods. The estimator is a linear regression model specified as following:\n\\[\nY_{it} = \\underbrace{\\unit + \\time}_{\\text{fixed effects}} + D_{it}\\att + \\cov + \\eps_{it}\n\\]\nThe fixed effects \\(\\alpha_i\\) and \\(\\gamma_t\\) are “special” intercepts. Unlike our normal intercepts in regression, these intercepts take different values depending on the unit and time.\n\nIntercept \\(\\alpha_i\\) has a different value for each unit \\(i\\).\nIntercept \\(\\gamma_t\\) has a different value for each time period \\(t\\).\n\nWhat fixed effects does is that it calculates different intercepts for each unit \\(i\\), and each time period \\(t\\). Thus, these intercepts reflect differences on-average between different units \\(i\\), and different time periods \\(t\\).\nSince the differences between units \\(i\\) and time periods \\(t\\) are included in these fixed effects intercept terms, this allows us to account for differences between units and differences between time periods.\n\nOr in other words, we are essentially “controlling” for differences between units, and differences between years.\n\nBy accounting for differences between units and between differences in time periods, we allow our observations to be more on a level playing field, allowing for us to calculate the causal effect. The estimate for \\(\\att\\) is our treatment effect (ATT). This estimate is unbiased if the parallel trends assumption is met.\n\nTo implement two-way fixed effects, we need the fixest package:\n\nlibrary(fixest)\n\nWe implement two-way fixed effects as follows:\n\nmodel &lt;- feols(outcome ~ treatment + covariate1 + covariate2 | unit + time,\n               data = my_data,\n               se = \"cluster\")",
    "crumbs": [
      "Difference-in-Differences",
      "Two-way Fixed Effects"
    ]
  },
  {
    "objectID": "did/twfestaggered.html",
    "href": "did/twfestaggered.html",
    "title": "Issues with Two Way Fixed Effects in Staggered DiD",
    "section": "",
    "text": "See Goodman-Bacon (2021) for a more technical overview of TWFE decomposition.\n\nThe two-way fixed effects estimator is possible with staggered difference-in-differences. But what is TWFE actually doing? Let’s say we have 3 groups - a group treated early, a group that was treated late, and a untreated group.\n\n\n\n\n\nTwo-way fixed effects essentially estimates 4 different differences, shown below:\n\n\n\n\n\nThe actual TWFE estimate is a weighted average of these differences. The weights are a function of mainly the telative timing of treatment - groups treated more in the middle (so not too early or too late) are weighted higher. The weights are seen below in the figure.\n\n\n\n\n\nThere are two problems with these comparison conducted by TWFE:\n\nSome of these comparison (see comparison D above) may be “forbidden” - such that already-treated units are used as controls after they are treated.\nWeighting can result in weird weights - sometimes some groups treated early/late might receive negative weights (see figure above).\n\n\nThis site shows how you can explore the comparison and weighting issue in R.\n\nThese problems with TWFE mean that TWFE is biased in estimating the ATT in staggered difference-in-differences when there is heterogenous treatment effects.\n\nTWFE is unbiased with homogeneity, but homogeneity is quite rare in scenarios we typically research.",
    "crumbs": [
      "Difference-in-Differences",
      "Issues with TWFE"
    ]
  },
  {
    "objectID": "did/sunab.html",
    "href": "did/sunab.html",
    "title": "Sun and Abraham (2021) Estimator",
    "section": "",
    "text": "For technical details, see Sun and Abraham (2021) or this github page.\n\nThere are two issues with the two-way fixed effects estimator: forbidden comparisons and issues with weighting. Sun and Abraham (2021) proposes a new estimator that solves both of these issues by:\n\nThe estimator divides all units into groups based on their initial treatment implementation. These groups are encoded into a new categorical variable \\(G_i\\).\nThen, the estimator calculates dynamic ATT’s separately for group, using only valid comparisons. This is done by interacting a relative-time variable \\(\\widetilde T_{it}\\) with \\(G_i\\).\n\n\\[\nY_{it} = \\underbrace{\\unit + \\time}_{\\text{fixed effecs}} +\\underbrace{\\blue{\\tau}G_i \\widetilde T_{it}}_{\\text{interactions}} +  \\cov + \\eps_{it}\n\\]\n\n\\(G_i\\) is a categorical variable that indicates what initial treatment period group unit \\(i\\) belongs to. \\(\\widetilde T_{it}\\) is a relative time categorical variable that describes unit \\(i\\) at time \\(t\\), and how many periods away from the initial treatment period of that unit \\(i\\) is.\n\n\nThis model will produce many coefficients \\(\\blue\\tau\\) (in fact, one for each post-treatment time period for each initial treatment year group).\nFinally, these separate heterogeneous effects are manually aggregated together into dynamic ATT’s by relative treatment time using proper weighting based on how frequent each initial treatment period group appears in our sample. This solves the weighting problem with TWFE.\n\nBecause of the focus of Sun and Abraham on dynamic treatment effects, it is often the go-to estimator for dynamic treatment effects and checking parallel trends.\n\n\n\n\n\n\nComparison with Wooldridge (2021) ETWFE\n\n\n\n\n\nSun and Abraham’s estimator is very similar to Wooldridge’s extended two-way fixed effects estimator. The three main differences are:\n\nSun and Abraham can calculate dynamic treatment effects for every time period (both pre and post treatment). Wooldridge’s can only calculate post-treatment. Thus, only Sun and Abraham can test for parallel trends.\nSun and Abraham is more strict with covariates for parallel trends, while Wooldridge’s allows for more flexibility and robustness to violations of parallel trends. This is because Wooldridge’s estimator contains far more interactions with covariates.\nWooldridge’s estimator allows for non-linear models (poisson, logistic, etc.).\n\n\n\n\n\nTo implement this estimator, we will need the fixest package:\n\nlibrary(fixest)\n\nWe implement Sun and Abraham’s estimator as follows:\n\nmodel &lt;- feols(outcome ~ sunab(initial_treat_year, time) + covariate1 + covariate2 | unit + time,\n               data = my_data,\n               vcov = ~unit)\niplot(model)",
    "crumbs": [
      "Difference-in-Differences",
      "Sun and Abraham (2021)"
    ]
  },
  {
    "objectID": "frameworks.html",
    "href": "frameworks.html",
    "title": "Basics of Causality",
    "section": "",
    "text": "This page covers the potential outcomes framework and the causal estimands.\n\nIn causal inference, we are interested in causal questions:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nTreatment (D)\n\n\n\nY\n\nOutcome (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\n\nWe generally assume the treatment \\(D\\) is binary.\n\nImagine we have two hypothetical parallel worlds that are copies of each other. Both of these worlds are identical except for one aspect: the treatment:\n\n\n\n\n\n\n\n\nParallel World\nTreatment\nPotential Outcome\n\n\nUnit \\(i\\) does not Receive Treatment\n\\(\\text{treatment}_i = 0\\)\n\\(\\T\\)\n\n\nUnit \\(i\\) Receives Treatment\n\\(\\text{treatment}_i = 1\\)\n\\(\\Cred\\)\n\n\n\n\nThe only difference between the two worlds is the treatment. Thus, any difference in outcomes between the two worlds must be the causal effect of the treatment.\n\\[\n\\tau_i = \\T - \\Cred\n\\]\n\nTechnically, we need another assumption, SUTVA, for this to be true. I will explain this assumption as part of the identification assumptions.\n\nHowever, in reality, we do not have two parallel worlds. Thus, by definition, one of the potential outcomes is not observed in our real world - the counterfactual.\n\n\n\n\n\n\n\n\nIn the Real World\nObserved Outcome\nCounterfactual\n\n\n\\(i\\) receives treatment\n\\(\\Y= \\T\\)\n\\(\\Cred\\)\n\n\n\\(i\\) did not receive treatment\n\\(\\Y = \\Cred\\)\n\\(\\T\\)\n\n\n\n\nThe fundamental problem of causal inference is that in order to calculate our individual treatment effect \\(\\tau\\), we need both potential outcomes. Our goal is to estimate causal effects without observing counterfactuals. This is difficult at the individual level, so instead, we focus on average treatment effects for groups:\n\n\n\n\n\n\n\n\nGroup Effects\nNotation\nDefinition\n\n\nAverage Treatment Effect (ATE)\n\\(\\tau_\\text{ATE}\\)\nThe average treatment effects for all individuals in our study (treated and untreated).\n\n\nAverage Treatment Effect on the Treated (ATT)\n\\(\\tau_\\text{ATT}\\)\nThe average treatment effect but only for individuals who receive the treatment in our study.\n\n\nLocal Average Treatment Effect (LATE)\n\\(\\tau_\\text{LATE}\\)\nThe average treatment effect but only for a specific (local) group of individuals in a study.",
    "crumbs": [
      "Home",
      "Basics of Causality"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Basics of Causality",
    "section": "",
    "text": "This page covers how confounders cause pre-existing differences between treated and untreated (selection bias), meaning correlation is not causation.\n\nLet us look at this causal question:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nGoing to the Hospital (D)\n\n\n\nY\n\nHealth Outcomes (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nWe have a group that went to hospital (we will call this group “went”), and a group that “did not go”. Using our potential outcomes framework, we can define the treatment effect of the group that went to the hospital:\n\\[\n\\tau_\\text{went} = \\textcolor{green}{{\\text{health}}_{\\text{went}}^{(1)}} - \\textcolor{red}{{\\text{health}}_{\\text{went}}^{(0)}}\n\\]\n\nIn red is the counterfactual we do not observe. This is because the individuals who went to the hospital were treated, so we cannot see the world where they are in control.\n\nNow compare the treatment effects above to correlation, which is defined as the difference in observed outcomes:\n\\[\n\\begin{align}\n\\text{correlation} & =  \\text{health}_\\text{went} - \\text{health}_\\text{did not go} \\\\\n& = \\textcolor{green}{\\text{health}_{\\text{went}}^{(1)}} - \\textcolor{red}{\\text{health}_\\text{did not go}^{(0)}}\n\\end{align}\n\\]\nIf we compare this correlation to our \\(\\tau_\\text{treated}\\), we see:\n\\[\n\\text{if  } \\textcolor{red}{\\text{health}_\\text{did not go}^{(0)}} ≠ \\textcolor{red}{\\text{health}_\\text{went}^{(0)}}\n\\]\nIf this is true (they are different), then correlation is not causation.\n\nThese two quantities are potential outcomes under control, or in another way to think of it, outcomes of the two groups prior to treatment happening.\n\nThus, if there is a difference between the average health outcomes between those who went to the hospital, and those who did not go to the hospital, before treatment is administered, then correlation is not equal to causation. This is because we cannot tell if the difference between the groups is due to treatment, or due to their pre-existing differences.\n\nWhat causes pre-existing differences? Confounders. For example, in our hospital-health example, a confounder could be smoking.\n\nSmoking is not the only possible confounder, we just use it as an example. Drinking, age, etc. are all other potential confounders.\n\nSmoking will worsen health outcomes. Someone who smokes is also more likely to visit the hospital with health complications. That means people who go to the hospital start out with (on average) worse health outcomes than people who did not go to the hospital.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nGoing to the Hospital (D)\n\n\n\nY\n\nHealth Outcomes (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmoking (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA confounder is a third variable that has the following characteristics:\n\nThe confounder is correlated (positive or negative) with the outcome variable.\nThe confounder causes who gets and doesn’t get the treatment.\nThe confounder is not itself caused by the treatment\n\n\n\n\n\nNote requirement 3 - it is a common mistake. Any result of the treatment \\(D\\) cannot be a confounder.\n\nConfounders cause pre-existing differences, which cause correlation to not equal causation. We must account for confounders to uncover causal effects.",
    "crumbs": [
      "Home",
      "Issue of Selection Bias"
    ]
  },
  {
    "objectID": "soo/distance.html",
    "href": "soo/distance.html",
    "title": "Distance Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red), since Mia receives the treatment.. However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Matched Individual}}\n\\]\nDistance matching matches an individual that is treated (like Mia) with one that is not treated based on how close their confounding values are. We define closeness by Mahalanobis distance:\n\\[\n\\text{distance}_{i, j} = \\sqrt{(\\b x_i - \\b x_j)' \\ \\b\\Sigma_x^{-1} (\\b x_i - \\b x_j)}\n\\]\n\nWhere \\(i\\) and \\(j\\) are two units we want to measure the distance between, \\(\\b x\\) is a vector of confounder values, and \\(\\b\\Sigma_x\\) is the covariance matrix of confounders.\n\nBecause distance matching depends on finding matches in a n-dimensional space, it is subject to the curse of dimensionality. This essentially means that the more confounders you have, the more dimensions you have to match over, and the harder it is to find good matches. So we typically do not use any more than 3-5 confounders with distance matching.\n\nBad matches means incorrectly using someone’s counterfactual, resulting in bad estimates.\n\n\nBefore you implement distance matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables.\nWe will need the Matching package.\n\nlibrary(Matching)\n\nNow, we can implement the matching as follows.\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,c(\"X1\",\"X2\", \"X3\")],\n             M = 1,\n             BiasAdjust = TRUE,\n             Weight = 2)\nsummary(att)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Distance Matching"
    ]
  },
  {
    "objectID": "soo/soo.html",
    "href": "soo/soo.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "Our issue in causal inference is that a confounder is causing pre-existing differences:\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nReceiving Scholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nIntellegence (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nBy definition, as the confounder changes, your likelihood of getting treatment changes. As the confounder changes, the outcome value will also change.\nThese issues occur when the confounder changes in value. So what if we hold the confounders constant? Then, there would be no changes in confounders - so the variation in treatment assignment and outcomes cannot be attributed to the confounder.\nFor example, let’s assume that intelligence has two values: smart and dumb. Let us calculate the treatment effect on university grades within each level of intelligence:\n\\[\n\\begin{align}\n\\tau_\\text{smart} & = \\green{Y_\\text{smart}^{(1)}} - \\red{Y_\\text{smart}^{(0)}}\\\\\n\\tau_\\text{dumb} & = \\green{Y_\\text{dumb}^{(1)}} - \\red{Y_\\text{dumb}^{(0)}}\n\\end{align}\n\\]\nThe confounder is constant here, so no selection bias. Thus, within each category, correlation is equal to causation. Our overall causal effect will be a weighted average of the categories:\n\\[\n\\tau = \\tau_\\text{smart} Pr(\\text{smart})  \\ + \\ \\tau_\\text{dumb} Pr(\\text{dumb})\n\\]\n\nThe weights of this weighted average are the probability/frequency of that value of the confounder.\n\nObviously, most confounders have more than 2 categories, and we often have more confounders. But the same intuition applies.\n\\[\n\\tau = \\sum_X \\tau_\\text{X} \\cdot Pr(X)\n\\]\n\nFor selection on observables to work, we need to meet 3 assumptions:\n\n\n\n\n\n\n\nAssumption\nDescription\n\n\nConditional Ignorability\nThis means that we must account for all possible confounders (cannot miss a single one).\n\n\nCommon Support\nThis means no one can have a 100% chance of being in treatment or control. They always have a chance to be in either, no matter their confounding values.\n\n\nStable Unit Treatment Value Assumption (SUTVA)\nThis means that if Ava is treated, that does not affect Mia’s outcome (and for any other 2 individuals).\n\n\n\n\nWe have a wide choice of estimators that we can use. Use the sidebar or links in the table to access each estimator’s page.",
    "crumbs": [
      "Selection on Observables"
    ]
  },
  {
    "objectID": "soo/pscore.html",
    "href": "soo/pscore.html",
    "title": "Propensity Score Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red), since Mia receives the treatment.. However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Matched Individual}}\n\\]\nPropensity Score Matching matches an individual that is treated (like Mia) with one that is not treated based on how similar their likelihoods of treatment are.\nWhat is a likelihood of treatment? Well we know confounders cause people to get the treatment or not treatment. Thus, using an individual’s confounder values, we can estimate their likelihood of getting treatment, called a propensity score.\n\\[\n\\text{propensity score } \\pi =Pr(\\green{D_i = 1})\n\\]\nPropensity scores are typically estimated with a logistic regression. This also means that propensity score matching shares the same weaknesses of logistic regression - including assuming linear relatinoships between confounders and propensities, and only being unbiased in large sample sizes.\n\nBefore you implement propensity score matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the Matching package.\n\nlibrary(Matching)\n\nFirst, we need to estimate the propensity scores with a logistic regression:\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity,\n                          type = \"response\")\n\n\nA random forest model is also possible, but less common.\n\nNow, we can implement the matching as follows.\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,\"pscore\"],\n             M = 1,\n             BiasAdjust = TRUE,\n             Weight = 2)\nsummary(att)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Propensity Score Matching"
    ]
  },
  {
    "objectID": "soo/genetic.html",
    "href": "soo/genetic.html",
    "title": "Genetic Matching",
    "section": "",
    "text": "See the pros and cons of this estimator in the choosing an estimator page.\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red), since Mia receives the treatment.. However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Matched Individual}}\n\\]\nLike distance matching, genetic matching matches an individual that is treated (like Mia) with one that is not treated based on how close their confounding values are. However, genetic matching uses a slightly different variation of mahalanobis distance:\n\\[\n\\text{distance}_{i, j}(\\b W) = \\sqrt{(\\b x_i - \\b x_j)' \\ (\\b\\Sigma_x^{-\\frac{1}{2}})' \\ \\b W \\ \\b\\Sigma_x^{-\\frac{1}{2}}  (\\b x_i - \\b x_j)}\n\\]\n\nWhere \\(i\\) and \\(j\\) are two units we want to measure the distance between, \\(\\b x\\) are their confounder values, and \\(\\b\\Sigma_x\\) is the covariance matrix of confounders. \\(\\b W\\) is a weights matrix.\n\nThe weights \\(\\b W\\) are estimated to make the treated and untreated groups as similar as possible. This balance between treated and untreated eliminates selection bias. Then, matching is done with the units that have the smallest distance.\n\nBefore you start genetic matching, make sure you have reasons to believe you meet the neccessary assumptions for selection on observables:\nWe will need the Matching and MatchIt package.\n\nlibrary(Matching)\n\nFirst, we need to estimate the propensity scores with a logistic regression.\n\nIt is recommended to use the propensity score as one of the controls on which to genetic match on.\n\n\npropensity &lt;- glm(D ~ X1 + X2,\n                  data = my_data,\n                  family = \"binomial\")\nmy_data$pscore &lt;- predict(propensity,\n                          type = \"response\")\n\nThen, we use the GenMatch() function to estimate a weights matrix \\(\\b W\\):\n\nset.seed(333) #any number works\ngen &lt;- GenMatch(Tr = my_data$D,\n                    X = my_data[,c(\"X1\",\"X2\",\"pscore\")],\n                    BalanceMatrix = my_data[,c(\"X1\",\"X2\")],   \n                    estimand = \"ATT\",\n                    M = 2,\n                    replace = TRUE,\n                    ties = FALSE,\n                    distance.tolerance = 0,\n                    print.level = 0,\n                    pop.size = 200)\n\n\nYou can increase pop.size to increase the accuracy - but it will increase the time and computational power needed.\n\nNow, let us conduct estimation with genetic matching:\n\natt &lt;- Match(Y = my_data$Y,\n             Tr = my_data$D,\n             X = my_data[,c(\"X1\",\"X2\",\"pscore\")],\n             estimand = \"ATT\",\n             M = 2,\n             replace = TRUE,\n             ties = FALSE,\n             distance.tolerance = 0,\n             Weight.matrix = gen$Weight.matrix,\n             Weight = 3)\n\nOur output estimate will be the ATT - the average treatment effect for those units who received the treatment.",
    "crumbs": [
      "Selection on Observables",
      "Genetic Matching"
    ]
  },
  {
    "objectID": "latent/sem.html",
    "href": "latent/sem.html",
    "title": "Structural Equation Models",
    "section": "",
    "text": "Structural Equation Models (SEMs) allow us to combine measurement models (ex. factor analysis) with linear regression models between the latent variables. We split our latent variables into latent explanatory variables \\(\\xi\\) and latent outcome variables \\(\\eta\\).\n\n\n\n\n\n\n\n\nexample2\n\n\nF1\n\nξ (Unobserved)\n\n\n\nX1\n\nX1\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3\n\n\n\nF1-&gt;X3\n\n\nλ\n\n\n\nF2\n\nη (Unobserved)\n\n\n\nF1-&gt;F2\n\n\nLinear Model\n\n\n\nX4\n\nY1\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\nX5\n\nY2\n\n\n\nF2-&gt;X5\n\n\nλ\n\n\n\nX6\n\nY3\n\n\n\nF2-&gt;X6\n\n\nλ\n\n\n\n\n\n\n\n\n\nThis structural equation model contains a linear regression between the latent input variable explaining the latent outcome variable.\n\\[\n\\eta = \\beta_0 + \\beta_1\\ \\xi + \\eps\n\\]\nThe latent outcome variable and input variable are unobserved, and we only observe items. We can use these observed items to create a factor analysis model for both the outcome variable, and the input variable.\nThe interpretations of the model are quite straight forward - the measurment models are interpreted in the same way as factor analysis, and the linear model between latent variable is interpreted the same way as a linear model between any other variables.\n\nEx: the linear model says that as explanatory increases by 1, response increases by an expected \\(\\beta\\) units.\n\nStructural models don’t have to have just one dependent and independent latent variable. We can have multiple dependent and independent variables, multiple linear models, and also measure correlations within the dependent and independent variables:\n\n\n\n\n\n\n\n\nexample2\n\n\nX1\n\nξ 1\n\n\n\nX2\n\nξ 2\n\n\n\nX1-&gt;X2\n\n\n\nCorrelation\n\n\n\nY1\n\nη 2\n\n\n\nX1-&gt;Y1\n\n\nβ1\n\n\n\nY2\n\nη 2\n\n\n\nX1-&gt;Y2\n\n\nγ1\n\n\n\nX2-&gt;Y1\n\n\nβ2\n\n\n\nX2-&gt;Y2\n\n\nγ2\n\n\n\nY1-&gt;Y2\n\n\n\nCorrelation\n\n\n\n\n\n\n\n\n\n\nNote: we cannot have linear models between two independent, or two dependent variables. They have to be seperate - within each group, only correlations are possible.\n\nIn this example, we are measuring the correlation between the independent variables, the correlation between the dependent variables, and we have two regression models:\n\\[\n\\begin{align}\n\\eta_1 & = \\beta_0 + \\beta_1\\ \\xi_1 + \\beta_2\\ \\xi_2 + \\eps_1 \\\\\n\\eta_2 & = \\gamma_0 + \\gamma_1\\ \\xi_1 + \\gamma_2\\ \\xi_2 + \\eps_2 \\\\\n\\end{align}\n\\]\n\nAnd each response and latent variable all have their own measurement models.\n\n\nTo implement structural equation models, we use the lavaan package:\n\nlibrary(lavaan)\n\nThen, we have to specify the relationships between variables we want to fit in our structural model - including measurement models, regression models, and correlations:\n\nmodel &lt;- '\n# Measurement models \n  input1 =~ NA*item1 + item2 + item3\n  input2 =~ NA*item4 + item5 + item6\n  outcome =~ item1 + item2 + item3\n  \n# Regressions\n  outcome ~ input1 + input2\n\n# Covariances\n  input1 ~~ input2\n\n# Fixing the variances of independent variables at 1\n  input1~~1*input1\n  input2~~1*input2\n'\n\n\nNote how outcome does not have NA*, and outcome doesn’t have its variance fixed at 1 at the end. Meanwhile, input1 and input2 do. This is standard to fix outcome variables with a variance of 1, and input variables are allowed to be “free”.\n\nNow, we estimate our specified model:\n\nsem &lt;- sem(model,\n           data = my_data,\n           missing=\"FIML\")\nsummary(sem)\n\n\nThe missing argument tells the software to keep observations with missing values (this is typically a good thing). But it can take longer, so you can delete this argument for it to use only complete observations.",
    "crumbs": [
      "Latent Variable Models",
      "Structural Equation Models"
    ]
  },
  {
    "objectID": "latent/multiplefac.html",
    "href": "latent/multiplefac.html",
    "title": "Multiple Latent Factors",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nFactor analysis can work with multiple latent factors \\(\\xi_1, \\xi_2, \\dots\\).\n\n\n\n\n\n\n\n\nexample2\n\n\nF1\n\nξ 1\n\n\n\nF2\n\nξ 1\n\n\n\nF1-&gt;F2\n\n\n\nCorrelation\n\n\n\nX1\n\nX1\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3\n\n\n\nF1-&gt;X3\n\n\nλ\n\n\n\nF2-&gt;X2\n\n\nλ\n\n\n\nF2-&gt;X3\n\n\nλ\n\n\n\nX4\n\nX4\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\n\n\n\n\n\n\nEach item can measure either only some or all of the factors. The factors can also both have the same items, but put emphasis on different items. The factors can also be correlated.\n\nWe can also have much more than just 2 factors. However, we typically need 3 items for the first factor, and 2 items for each additional factor, or else we won’t have enough degrees of freedom for estimation.\n\nJust like in single factor analysis, we use a linear regression to express the relationship between any item \\(i\\) and the factors. However, this time, each regression will relate each item to all factors:\n\\[\n\\begin{align}\nX_1 & = \\tau_1 + \\lambda_{1}^{(1)}\\ \\xi_1 + \\lambda_{1}^{(2)}\\ \\xi_2 + \\dots + \\delta_1 \\\\\nX_2 & = \\tau_2 + \\lambda_{2}^{(1)}\\ \\xi_1 + \\lambda_{2}^{(2)}\\ \\xi_2 + \\dots + \\delta_2 \\\\\n& \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\qquad \\qquad \\vdots \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nDetails: Assumptions of the Model\n\n\n\n\n\n\nWe still assume the factors are normally distributed:\n\\[\n\\sim \\mathcal N(\\b \\kappa, \\b\\Phi)\n\\]\nWhere \\(\\b\\kappa\\) is a vector of all the means of each factor, and \\(\\b\\Phi\\) is a variance-covariance matrix of all the factors.\nUsing our conventional identification assumption like in single factor analysis, we will assume each factor is a standard normal \\(\\mathcal N (0, 1)\\). This implies that \\(\\b\\kappa = 0\\).\nOur variance matrix \\(\\b\\Phi\\) is a little more complicated - the variances of each factor is 1 (as assumed in a standard normal), however, the matrix \\(\\b\\Phi\\) also includes the covariances between factors. This is an additional thing that we will need to estimate that was not present in one-factor models.\nFor example, factor 1 and 2 might be correlated with each other, which is reflected in \\(\\b\\Phi\\).\n\n\n\n\nThe factor loadings \\(\\lambda\\) are still the relationship between each factor and each item, and are interpreted in the same way as a 1-factor model. However, the the interpretation of communality and relaibility are no longer valid with more than 1-factor.\n\nWhen interpreting - do one factor at a time. Start with one, then go to the next. Interpreting each factor is the same as shown in the page on factor analysis.\n\nAs seen in the figure above, factors can also be correlated with each other. Our model estimation will also estimate the correlation between factors. Just like in single factor analysis, factor scores are also possible.\n\nTo implement multiple factor anlaysis, the procedure is quite similar to standard factor analysis. First, we will need the psych and GPArotation package:\n\nlibrary(psych)\nlibrary(GPArotation)\n\nFirst, we should get rid of missing observations:\n\nall.obs &lt;- apply(my_data, 1, FUN=function(x){all(!is.na(x))})\ndta &lt;- my_data[all.obs,]\n\nFor factor analysis with multiple factors, the notation is as follows:\n\nfa &lt;- fa(data[,items], nfactors=2, fm=\"ml\", rotate=\"oblimin\")\nprint(fa)\n\n\nChange nfactors=2 to however many factors you want. Note that you cannot have too many, this will be discussed in the next page on identification.\nThis code also uses the “oblimin” rotation, which will be discussed in the next page on identification and rotation.",
    "crumbs": [
      "Latent Variable Models",
      "Multiple Latent Factors"
    ]
  },
  {
    "objectID": "latent/latent.html",
    "href": "latent/latent.html",
    "title": "Latent Variable Models",
    "section": "",
    "text": "A latent variable model connects a unobserved variable (latent factor \\(\\xi\\)) with a few observed variables (items \\(X_1, X_2, \\dots\\)) that are considered imperfect measures of the latent factor.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\n\nFor example, maybe we want to measure the political ideology of a senator. We cannot directly observe the political ideology (the latent factor), but we can observe how they vote on different bills (the items)\n\nLatent variable models assume we can model the relationship between each observed item and the latent factor with some sort of regression model. The coefficient of each regression (often denoted \\(\\lambda\\)) is the relationship between each item and the factor.\n\nFor example, in the figure above, each item (X1, X2, X3) is related to the factor (F) with by a \\(\\lambda\\) coefficient.\n\nThese \\(\\lambda\\) are called factor loadings. We can interpret the estimated factor loadings \\(\\widehat\\lambda\\) to interpret what the unobserved factor actually is measuring.\n\nFor example, if a factor has a strong relationship with one item, and a weaker relationship with another item, we might conclude that the factor measures the first item more than the second item.\n\nWe can also use latent variable models to create factor scores \\(\\widehat\\xi\\), which are basically actual values of the latent variable for each individual \\(i\\) in our data. This allows us to use the latent variable in other statistical models.\nThe choice of latent variable model depends on the type of items/factors:\n\n\n\n\n\n\n\n\nModel\nFactor Type\nItem Type\n\n\nFactor Analysis\nContinuous\nContinuous\n\n\nItem Response Theory\nContinous\nCategorical/Binary\n\n\nStructural Class Models\nCategorical/Binary\nCategorical/Binary\n\n\n\n\nAll models allow for multiple factors \\(\\xi_1, \\xi_2, \\dots\\) as well. If we have many items, some items might only measure on factor, others both factors.\nEach latent model also has the ability to conduct confirmatory analysis. This is basically when we, based on theoretical reasons (such as reading the literature), impose certain restrictions on models, and test if this hypothesised model is good.\n\nThis typically takes the form of setting certain factor loadings \\(\\lambda\\) to 0, meaning a certain item does not measure one of the factors.\n\nWe can also combine different structural models together to form structural equation models (see the structural equation models part).",
    "crumbs": [
      "Latent Variable Models"
    ]
  },
  {
    "objectID": "latent/cluster.html",
    "href": "latent/cluster.html",
    "title": "Cluster Analysis",
    "section": "",
    "text": "Cluster analysis is a method of finding groups/clusters of similar individuals in our dataset, based on a set of variables. We choose the number of clusters we want to divide our data into, and each individual will be assigned to a specific cluster.\n\n\n\n\n\n\nAbove is an example of a cluster anlaysis with 2 clusters. Latent Class Models can also be considered to be a form of cluster analysis.\n\nCluster anlaysis is often conducted by K-means clustering. This form of clustering focuses on the distance between different individuals in our dataset. The idea is that a good clustering will have as small within-cluster variation as possible.\n\nIn other words, you want individuals in the same cluster to be similar.\n\nThus, K-means clustering tries to minimise the squared euclidean distance between all the points within a cluster and the centroid (“center”) of the cluster:\nHow do we minimise this distance? The K-means clustering algorithm works like this:\n\nChoose how many total clusters we want.\nRandomly assign each unit to one of the clusters.\nCalculate the centroids of each cluster.\nRe-assign each individual to the cluster whose centroid is closest to that individual.\nKeep repeating steps 3 to 4 until all units are in the cluster whose centroid is closest to that individual.\nRepeat the whole process a few times to find the global optimum.\n\n\nK-means clustering is algorithmic, not model based (unlike the very similar Latent Class models). This means that K-means does not have model summary statistics like AIC, or significance tests.\n\nTo interpret the clusters, we have to use our own field expertise. We look at the individuals in each cluster, as well as their mean variable values, and try to assign meaning to them.\n\n\n\n\n\n\nOther Clustering Methods\n\n\n\n\n\nK-means is the most common, but not the only way to cluster. Other methods include:\n\nLatent Class Models, which we covered previously. They technically count as cluster analysis, since we are assinging individuals to a category of a latent variable. This is a model-based method.\nGaussian Mixture Modelling: this is a variation of latent class models that deals with continuous observed items, but categorical variables.\nHierarchical Cluster Anlaysis: This basically starts with each unit in its own cluster, before merging clusters that are similar. Then it repeats that continuously. We can choose to analyse any specific cluster set within this process.\n\n\n\n\n\nTo implement cluster anlaysis, we should first start by standardising our variables.\n\nvars &lt;- c(\"X1\", \"X2\", \"X3\") #names of variables to cluster by\n\n# Standardise function\nstandard &lt;- function(x){\n  (x-mean(x))/sd(x)\n}\n\n# apply standardisation\nmy_data.z &lt;- global[,c(\"Unit Names Variable\", vars)]\nmy_data.z[,vars] &lt;- sapply(my_data.z[,vars], standard)\ncolnames(my_data.z)[-1] &lt;- vars.z\ndta &lt;- merge(my_data, my_data.z, by=\"Unit Names Variable\")\n\nThen, we can implement cluster anlaysis:\n\nset.seed(1236) #set seed\n\nres &lt;- kmeans(dta[,varsz],\n                  4, #change to number of clusters\n                  nstart = 10) #number of times to run algorithm\nprint(res)\n\nWe can access each individual’s assigned cluster with the following:\n\nres$cluster\n\n\nYou can save this vector back into your original dataset for further anlaysis.",
    "crumbs": [
      "Latent Variable Models",
      "Cluster Analysis"
    ]
  },
  {
    "objectID": "latent/group.html",
    "href": "latent/group.html",
    "title": "Multiple Group Models",
    "section": "",
    "text": "We often deal with data that is clustered or grouped. For example, if we are using European data, we might have observations from different countries. If we are using USA data, we might have observations from different states.\nHowever, there might be differences between these clusters. We might want to make our model be able to account for these differences between clusters. One way to account for differences between groups is to essentially run different models for different groups:\n\\[\n\\begin{align}\n\\eta^{(g=1)} & = \\beta_0^{(g=1)} + \\beta_1^{(g=1)} \\xi^{(g= 1)} + \\eps \\\\\n\\eta^{(g=2)} & = \\beta_0^{(g=2)} + \\beta_1^{(g=2)} \\xi^{(g= 2)} + \\eps \\\\\n\\eta^{(g=3)} & = \\beta_0^{(g=3)} + \\beta_1^{(g=3)} \\xi^{(g= 3)} + \\eps \\\\\n\\end{align}\n\\]\n\nWhere \\(g\\) stands for group.\n\nThis essentially means for each group, they have their own \\(\\beta_0\\) and \\(\\beta_1\\) estimates. Since we know \\(\\beta_1\\) is the relationship between input and output, this essentially allows the relationship between input and output to vary by group.\n\nFor example, this would allow the relationship between education and income to vary by country.\n\nIf our outcome and input variables are measurement models, we can also play around with these in a few ways.\n\nWe could assume the same measurement models across groups.\nOr, just like above, we could assume each group has its own measurement model, with its own \\(\\lambda\\) factor loading varying by group.\n\nWe noted in factor analysis that we typically assume the (outcome and input) factor is a standard normal distribution with mean of 0 and variance of 1.\nIn clustered analysis, we can’t actually assume this. Why? Well, if we allow each group to have a different relationship between input and outcome, by definition, the outcome’s mean will differ by country. So instead, we fix one group (usually randomly) to be standard normal with mean of 0 and variance of 1, and allow the variances of all the other groups be freely estimated.\n\nTo implement structural equation models, we use the lavaan package:\n\nlibrary(lavaan)\n\nThen, we have to specify the relationships between variables we want to fit in our structural model - including measurement models, regression models, and correlations:\n\nmodel &lt;- '\n# Measurement models \n  input1 =~ NA*item1 + item2 + item3\n  input2 =~ NA*item4 + item5 + item6\n  outcome =~ item1 + item2 + item3\n  \n# Regressions\n  outcome ~ input1 + input2\n\n# Covariances\n  input1 ~~ input2\n\n# Fixing the variances of independent variables at 1\n  input1~~c(1,NA)*input1\n  input2~~c(1,NA)*input2\n  outcome~~c(1,NA)*outcome\n'\n\nNow, we estimate our specified model:\n\nsem &lt;- sem(model,\n           data = my_data,\n           group = \"groupvariable\",\n           group.equal=c(\"intercepts\",\"loadings\",\"residuals\"),\n           missing=\"FIML\")\nsummary(sem)\n\n\nThe missing argument tells the software to keep observations with missing values (this is typically a good thing). But it can take longer, so you can delete this argument for it to use only complete observations.",
    "crumbs": [
      "Latent Variable Models",
      "Multiple Group Models"
    ]
  },
  {
    "objectID": "latent/confirmatory.html",
    "href": "latent/confirmatory.html",
    "title": "Confirmatory Analysis",
    "section": "",
    "text": "Make sure you have read the previous page on factor analysis before this.\n\nSometimes, we already have some ideas about our latent variables - some hypotheses on how different items are related to the latent variable. This is what we can confirmatory analysis, where we study how well a hypothesised model fits the data.\nThe basic approach of confirmatory analysis is to set some factor loadings \\(\\lambda\\) for certain variables to 0. This essentially means that based on our preconceived theories, establish that some items do not measure a certain factor.\n\nBy setting enough loadings to 0, we no longer have to worry about rotation issues discussed previously.\n\nFor example, perhaps theoretically we believe that items 1 and 2 only explain factor 1, and only items 3 and 4 explain latent factor 2. We can set the items we believe to not explain each factor to 0.\n\n\n\n\n\n\n\n\nexample2\n\n\nF1\n\nLatent Factor 1\n\n\n\nX1\n\nItem 1\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nItem 2\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nItem 3\n\n\n\nF1-&gt;X3\n\n\n0\n\n\n\nF2\n\nLatent Factor 2\n\n\n\nF2-&gt;X2\n\n\n0\n\n\n\nF2-&gt;X3\n\n\nλ\n\n\n\nX4\n\nItem 4\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe estimation and interpretation of confirmatory analysis are essentially identical to that of exploratory analysis we have previously looked at.\n\n\n\n\n\n\nDetails: Identification Assumptions\n\n\n\n\n\n\nRecall that we previously assumed the latent factors are standardly normally distributed. In confirmatory analysis, we still assume the factor is normally distributed, but we allow for the mean and variance of the distribution to be estimated:\nInstead, we fix each factor (if we have multiple) to the scale of one of the items, with each factor fixed to a different item. Basically, this means that the factor will take the same scale/measurement characteristics as one of the items. This is done by fixing that item’s intercept \\(\\tau_i = 0\\), and the same item’s factor loading for that specific factor at \\(\\lambda = 1\\).\n\n\n\n\nThere is a special set of tests to see if the parameters we set equal to 0 actually make sense. Modification indicies are a sort of hypothesis test for this - larger values indicate parameters that if were not 0, could improve the fit of the model.\nThere are also expected parameter changes (EPC), which basically estimate what parameters set to 0 would actually be equal to, if they were added to the model.\n\nBelow in the R-code I provide more ways of choosing between confirmatory models and exploratory models (and really any factor models).\n\n\nFor confirmatory analysis, we will need the lavaan package:\n\nlibrary(lavaan)\n\nWe fit a confirmatory model in the following way:\n\n# specify formula\nformula &lt;- '\nf1 =~ X1 + X2 + 0*X3 \nf2 =~ 0*X2 + X3 + X4\n'\n\n# estimate model\nmodel &lt;- sem(formula,\n           data = my_data,\n           std.lv = TRUE,\n           missing = \"fiml\")\nsummary(model)\n\n\nWe put a 0* before any item for that factor we want to set equal to 0. You can also use this command to estimate exploratory models.\n\nIf we have theoretical reasons, we can also force different factor load gins to be equal by including the same labels before as follows:\n\n# specify formula\nformula &lt;- '\nf1 =~ c1*X1 + c1*X2 + X3 \nf2 =~ X2 + X3 + X4\n'\n\n\nSince label c1 appears before X1 and X2 for factor f1, that means their factor loadings will be forced to be equal. You can include more labels as well.\n\nTo create factor scores, we simply use the predict function:\n\npredict(model)\n\n\n\n\n\n\n\nMore on Choosing Between Models\n\n\n\n\n\nTo compare and choose between different models, we have a few ways.\n\nWe can use a likelihood ratio test for nested models (see the regression section for more details).\n\n\nlavTestLRT(model1, model2)\n\n\nWe can use a global goodness of fit test - essentially a likelihood ratio but with the full sample covariance matrix as the null model. We want to fail to reject the null, because we want our model to be as close to the sample covariance matrix as possible.\n\n\nlavTestLRT(model)\n\n\nWe can use AIC and BIC to compare models since factor models are estimated with MLE. These are included in the output.\n\nThere are also a series of fit indicies made for factor analysis. These are included in the output.\n\nRoot Mean Square Error of Approximation (RMSEA): 0 is a perfect fit, 0.05 or smaller is a good fit, and anything above 0.1 is a poor fit.\nStandard Root Mean Square Residual (SRMR): smaller values the better, anything below 0.08 is a good fit.\nTucker and Lewis Index (TLI): 1 is the best fit, anything below 0.9 is a poor fit, and anything above 1 may indicate overfitting.\nComparative Fit Index: values between 0 and 1, anything close to 1 is a good fit.",
    "crumbs": [
      "Latent Variable Models",
      "Confirmatory Analysis"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Additional Resources",
    "section": "",
    "text": "Here are a list of additional resources that are useful for causal inference and social science statistics.\n\nDr. Asjad Naqvi has a repository of the modern advancements going on in the recent difference-in-differences revolution.\nCausal Inference: The Mixtape as a great resource for details on causal inference methodology.\nCausal Inference for the Brave and True is another great resource of causal inference, with application in Python.\nFixest (an R package used for causal inference and regression) is a very useful thing to be familiar with.\nBen Lambert is a terrific resource for econometrics. He also has a lot of resources on general statistics, including factor analysis and bayesian statistics.\nGreat textbooks for causal inference: Angrist and Pischke have two books - Mostly Harmless Econometrics (more advanced) and Mastering Metrics (more simple). A very technical book is Imbens and Rubin’s Causal Inference for Statistics, Social, and Biomedical Sciences.\nGreat textbooks for more theoretical econometrics/statistics include Wooldridge’s Introductory Econometrics.",
    "crumbs": [
      "Home",
      "Additional Resources"
    ]
  }
]