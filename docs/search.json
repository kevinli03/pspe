[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Additional Resources",
    "section": "",
    "text": "Here are a list of additional resources that are useful for causal inference and social science statistics.\n\nDr. Asjad Naqvi has a repository of the modern advancements going on in the recent difference-in-differences revolution.\nYiqing Xu also has a page on modern advancements in difference-in-differences.\nCausal Inference: The Mixtape as a great resource for details on causal inference methodology.\nCausal Inference for the Brave and True is another great resource of causal inference, with application in Python.\nFixest (an R package used for causal inference and regression) is a very useful thing to be familiar with.\nBen Lambert is a terrific resource for econometrics. He also has a lot of resources on general statistics, including factor analysis and bayesian statistics.\nGreat textbooks for causal inference: Angrist and Pischke have two books - Mostly Harmless Econometrics (more advanced) and Mastering Metrics (more simple). A very technical book is Imbens and Rubin’s Causal Inference for Statistics, Social, and Biomedical Sciences.\nGreat textbooks for more theoretical econometrics/statistics include Wooldridge’s Introductory Econometrics.",
    "crumbs": [
      "Home",
      "Additional Resources"
    ]
  },
  {
    "objectID": "latent/cluster.html",
    "href": "latent/cluster.html",
    "title": "Cluster Analysis",
    "section": "",
    "text": "Cluster analysis is a method of finding groups/clusters of similar individuals in our dataset, based on a set of variables. We choose the number of clusters we want to divide our data into, and each individual will be assigned to a specific cluster.\n\n\n\n\n\n\nAbove is an example of a cluster anlaysis with 2 clusters. Latent Class Models can also be considered to be a form of cluster analysis.\n\nCluster anlaysis is often conducted by K-means clustering. This form of clustering focuses on the distance between different individuals in our dataset. The idea is that a good clustering will have as small within-cluster variation as possible.\n\nIn other words, you want individuals in the same cluster to be similar.\n\nThus, K-means clustering tries to minimise the squared euclidean distance between all the points within a cluster and the centroid (“center”) of the cluster:\nHow do we minimise this distance? The K-means clustering algorithm works like this:\n\nChoose how many total clusters we want.\nRandomly assign each unit to one of the clusters.\nCalculate the centroids of each cluster.\nRe-assign each individual to the cluster whose centroid is closest to that individual.\nKeep repeating steps 3 to 4 until all units are in the cluster whose centroid is closest to that individual.\nRepeat the whole process a few times to find the global optimum.\n\n\nK-means clustering is algorithmic, not model based (unlike the very similar Latent Class models). This means that K-means does not have model summary statistics like AIC, or significance tests.\n\nTo interpret the clusters, we have to use our own field expertise. We look at the individuals in each cluster, as well as their mean variable values, and try to assign meaning to them.\n\n\n\n\n\n\nOther Clustering Methods\n\n\n\n\n\nK-means is the most common, but not the only way to cluster. Other methods include:\n\nLatent Class Models, which we covered previously. They technically count as cluster analysis, since we are assinging individuals to a category of a latent variable. This is a model-based method.\nGaussian Mixture Modelling: this is a variation of latent class models that deals with continuous observed items, but categorical variables.\nHierarchical Cluster Anlaysis: This basically starts with each unit in its own cluster, before merging clusters that are similar. Then it repeats that continuously. We can choose to analyse any specific cluster set within this process.\n\n\n\n\n\nTo implement cluster anlaysis, we should first start by standardising our variables.\n\nvars &lt;- c(\"X1\", \"X2\", \"X3\") #names of variables to cluster by\n\n# Standardise function\nstandard &lt;- function(x){\n  (x-mean(x))/sd(x)\n}\n\n# apply standardisation\nmy_data.z &lt;- global[,c(\"Unit Names Variable\", vars)]\nmy_data.z[,vars] &lt;- sapply(my_data.z[,vars], standard)\ncolnames(my_data.z)[-1] &lt;- vars.z\ndta &lt;- merge(my_data, my_data.z, by=\"Unit Names Variable\")\n\nThen, we can implement cluster anlaysis:\n\nset.seed(1236) #set seed\n\nres &lt;- kmeans(dta[,varsz],\n                  4, #change to number of clusters\n                  nstart = 10) #number of times to run algorithm\nprint(res)\n\nWe can access each individual’s assigned cluster with the following:\n\nres$cluster\n\n\nYou can save this vector back into your original dataset for further anlaysis.",
    "crumbs": [
      "Latent Variable Models",
      "Cluster Analysis"
    ]
  },
  {
    "objectID": "latent/latent.html",
    "href": "latent/latent.html",
    "title": "Latent Variable Models",
    "section": "",
    "text": "A latent variable model connects a unobserved variable (latent factor \\(\\xi\\)) with a few observed variables (items \\(X_1, X_2, \\dots\\)) that are considered imperfect measures of the latent factor.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\n\nFor example, maybe we want to measure the political ideology of a senator. We cannot directly observe the political ideology (the latent factor), but we can observe how they vote on different bills (the items)\n\nLatent variable models assume we can model the relationship between each observed item and the latent factor with some sort of regression model. The coefficient of each regression (often denoted \\(\\lambda\\)) is the relationship between each item and the factor.\n\nFor example, in the figure above, each item (X1, X2, X3) is related to the factor (F) with by a \\(\\lambda\\) coefficient.\n\nThese \\(\\lambda\\) are called factor loadings. We can interpret the estimated factor loadings \\(\\widehat\\lambda\\) to interpret what the unobserved factor actually is measuring.\n\nFor example, if a factor has a strong relationship with one item, and a weaker relationship with another item, we might conclude that the factor measures the first item more than the second item.\n\nWe can also use latent variable models to create factor scores \\(\\widehat\\xi\\), which are basically actual values of the latent variable for each individual \\(i\\) in our data. This allows us to use the latent variable in other statistical models.\nThe choice of latent variable model depends on the type of items/factors:\n\n\n\n\n\n\n\n\nModel\nFactor Type\nItem Type\n\n\nFactor Analysis\nContinuous\nContinuous\n\n\nItem Response Theory\nContinous\nCategorical/Binary\n\n\nStructural Class Models\nCategorical/Binary\nCategorical/Binary\n\n\n\n\nAll models allow for multiple factors \\(\\xi_1, \\xi_2, \\dots\\) as well. If we have many items, some items might only measure on factor, others both factors.\nWe can also combine different structural models together to form structural equation models (see the structural equation models part).",
    "crumbs": [
      "Latent Variable Models"
    ]
  },
  {
    "objectID": "latent/factor.html",
    "href": "latent/factor.html",
    "title": "Factor Analysis Model",
    "section": "",
    "text": "# Packages Needed\nlibrary(lavaan)\n\nFactor analysis models an unobserved continuous latent variable \\(\\xi\\), based on continuous observed items \\(X_1, X_2, \\dots\\). Each item is connected with the factor \\(\\xi\\) by a linear model:\n\\[\n\\begin{align}\nX_1 & = \\tau_1 + \\blue{\\lambda_1} \\ \\xi + \\delta_1 \\\\\nX_2 & = \\tau_2 + \\blue{\\lambda_2} \\ \\xi + \\delta_2 \\\\\nX_3 & = \\tau_3 + \\blue{\\lambda_3} \\ \\xi + \\delta_3 \\\\\n\\end{align}\n\\]\n\n\\(\\tau\\) is the intercept of the model. \\(\\lambda\\) is the coefficient, called the factor loadings. \\(\\delta\\) is the error term.\n\n\nformula &lt;- '\nfactor =~ rich + admire + success + respect\n'\n# Formula is always factor =~ item1 + item 2\n# You can add multiple factors f1, f2, and multiple measurement models together\n\nmodel &lt;- sem(\n  model   = formula,  # fomrula from above\n  data    = df,       # your data\n  std.ov  = TRUE,     # standardises the observed variables\n  std.lv  = TRUE,\n  missing = \"fiml\"    # this allows use of missing data, but may take longer\n)\n      \ncoef(model)\n    # You can also use summary(model), which gives a more comprehensive output\n\n#&gt;     factor=~rich   factor=~admire  factor=~success  factor=~respect \n#&gt;            0.490            0.688            0.827            0.484 \n#&gt;       rich~~rich   admire~~admire success~~success respect~~respect \n#&gt;            0.760            0.527            0.316            0.766 \n#&gt;           rich~1         admire~1        success~1        respect~1 \n#&gt;            0.000            0.000            0.001            0.001\n\n\n\nThe factor=~item are \\(\\lambda\\). The item~~item are \\(\\delta\\). The item~1 are \\(\\tau\\). Each column is one specific model for one item.\n\nThe factor loadings \\(\\blue{\\lambda_i}\\) represent the relationship/covariance between any item and a factor. These factor loadings help us interpret our latent variable. The sign of the factor loading tells us the direction in which our latent variable is measuring. The absolute size of the factor loading tells us how important that item is to the factor.\n\nIn the output above, all \\(\\lambda\\) are positive, meaning as item increases, the factor also increases. success is the item with the largest \\(\\lambda\\), so it is measured the most in the factor.\n\nFactor scores \\(\\widehat\\xi\\) are essentially values of the latent variable for individuals in our study. This allows us to use our observed items to calculate the latent variable value that any individual should have, which we can then put into another statistical model. We can calculate them in R:\n\nfactor.scores &lt;- lavPredict(model)\n\n# visualise factor scores (not required)\nhead(factor.scores)\n\n#&gt;           factor\n#&gt; [1,]  1.10023478\n#&gt; [2,]  1.20216306\n#&gt; [3,] -0.29398041\n#&gt; [4,] -0.02358533\n#&gt; [5,]  1.50881758\n#&gt; [6,]  0.70699829\n\n\n\nThese are the first 6 observation’s factor scores. You can use the variable you saved the factor scores to in a regression or other statistical model.",
    "crumbs": [
      "Latent Variable Models",
      "Factor Analysis Models"
    ]
  },
  {
    "objectID": "soo/genetic.html",
    "href": "soo/genetic.html",
    "title": "Genetic Matching",
    "section": "",
    "text": "# Packages Needed:\nlibrary(Matching)\nlibrary(fixest)\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red), since Mia receives the treatment.. However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Matched Individual}}\n\\]\nLike distance matching, genetic matching matches based on how close the pair’s confounding values are. However, there are two differences.\nFirst, researchers recommend to include propensity scores as another confounder in the genetic matching process:\n\n# Logistic Model for Propensity Scores\npropensity &lt;- femlm(\n  fml    = treatment ~ cov1 + cov2,  # include all confounders\n  data   = df,                       # your data\n  family = \"logit\"                   # tell R to use logistic reg\n)\n\n# Predice Propensity Scores for All Observations\ndf$pscore &lt;- predict(propensity, type = \"response\")\n\nSecond, genetic matching uses a slightly different variation of mahalanobis distance:\n\\[\n\\text{distance}_{i, j}(\\b W) = \\sqrt{(\\b x_i - \\b x_j)' \\ (\\b\\Sigma_x^{-\\frac{1}{2}})' \\ \\b W \\ \\b\\Sigma_x^{-\\frac{1}{2}}  (\\b x_i - \\b x_j)}\n\\]\n\nWhere \\(i\\) and \\(j\\) are two units we want to measure the distance between, \\(\\b x\\) are their confounder values, and \\(\\b\\Sigma_x\\) is the covariance matrix of confounders. \\(\\b W\\) is a weights matrix. We must include all confounders.\n\nThe weights \\(\\b W\\) are estimated to make the treated and untreated groups as similar as possible. This balance between treated and untreated eliminates selection bias. We estimate the weights as follows:\n\nset.seed(333) #replicability\ngen &lt;- GenMatch(\n  Tr            = df$treatment,                   # treatment var\n  X             = df[,c(\"cov1\",\"cov2\",\"pscore\")], # include all confounders and pscore\n  BalanceMatrix = df[,c(\"cov1\",\"cov2\")],          # include all confounders\n  estimand      = \"ATT\",\n  M             = 2,\n  replace       = TRUE,\n  ties          = FALSE,\n  print.level   = 0,\n  pop.size      = 50                              # 200 is standard. larger is better, takes longer\n)\n\nThen, matching is done with the units that have the smallest distance:\n\nmodel &lt;- Match(\n  Y             = df$outcome,                     # outcome var\n  Tr            = df$treatment,                   # treatment var\n  X             = df[,c(\"cov1\",\"cov2\",\"pscore\")], # include all confounders and pscore\n  estimand      = \"ATT\",\n  M             = 2,\n  replace       = TRUE,\n  ties          = FALSE,\n  Weight.matrix = gen$Weight.matrix,              # weight matrix from GenMatch\n  Weight        = 3\n)\n\nsummary(model)\n\n#&gt; \n#&gt; Estimate...  1.2485 \n#&gt; SE.........  0.11159 \n#&gt; T-stat.....  11.189 \n#&gt; p.val......  &lt; 2.22e-16 \n#&gt; \n#&gt; Original number of observations..............  1000 \n#&gt; Original number of treated obs...............  255 \n#&gt; Matched number of observations...............  255 \n#&gt; Matched number of observations  (unweighted).  510\n\n\nIf all confounders are included, our output estimate will be the ATT - the average treatment effect for those units who received the treatment.\n\nOmission of any confounder will cause inaccurate matches, thus causing inaccurate results.\n\nGenetic matching is the best form of matching. So if you are using matching, you should use genetic matching unless it is not possible to do.\nGenetic matching (and all matching) also throws out a lot of data that is unmatched, which can be wasteful.",
    "crumbs": [
      "Selection on Observables",
      "Genetic Matching"
    ]
  },
  {
    "objectID": "soo/interact.html",
    "href": "soo/interact.html",
    "title": "Fully Interacted Estimator",
    "section": "",
    "text": "# Packages Needed:\nlibrary(estimatr)\n\nLet us say we have data on all confounders. A natural way to account for all confounders is to control for all confounders in a linear regression. With all confounders included, our model should be exogenous, and the \\(\\beta_\\text{OLS}\\) estimate should be unbiased.\n\\[\nY_i = \\alpha + D_i\\beta_\\text{OLS} + \\covs\n\\]\nHowever, there is an issue with OLS estimates under treatment heterogeneity. We know that our causal effect is a weighted average of our conditional treatment effects:\n\\[\n\\ate = \\sum\\tau_\\text{CATE}(x) \\cdot Pr( x)\n\\]\nNotice how the weights are the probability of the confounder values of \\(\\b x\\). With some complex math (Angrist 1998), we can show OLS estimates:\n\\[\n\\hat\\beta_\\text{OLS} = \\sum \\tau_\\text{CATE}(x)\\cdot \\underbrace{\\frac{Var(D_i | x)Pr(x)}{\\sum Var(D_i | x^c)Pr( x^c)}}_{\\text{weight}}\n\\]\nThese weights are not equivalent to the selection on observables \\(\\ate\\). Thus, if not all \\(\\tau_\\text{X}\\) are exactly the same (homogeneity), then our OLS estimator (OLS) will incorrectly estimate the ATE. Lin (2013) proposes the fully interacted estimator, which allows for consistent estimation of the ATE even with heterogeneity:\n\\[\nY_i = \\alpha + D_i\\ \\ate + \\underbrace{(\\b X_i - \\b{\\mean X})'\\b\\beta + D_i(\\b X_i - \\b{\\mean X})'\\b\\gamma}_{\\text{interactions with de-meaned covariates}}+ \\eps_i\n\\]\n\nmodel &lt;- lm_lin(\n  formula    = outcome ~ treatment,  # do not put confounders here\n  covariates = ~ cov1 + cov2,        # confounder vars\n  data       = df                    # your data\n)\n\nfixest::coeftable(model)[,1:4] # Can also use summary(model)\n\n#&gt;                     Estimate Std. Error    t value      Pr(&gt;|t|)\n#&gt; (Intercept)       1.24650767 0.02619477 47.5861367 1.648701e-258\n#&gt; treatment         2.01048186 0.10019394 20.0659035  1.845547e-75\n#&gt; cov1_c            0.55380266 0.05129078 10.7973144  8.939762e-26\n#&gt; cov2_c            0.50556650 0.02780868 18.1801684  5.513869e-64\n#&gt; treatment:cov1_c  1.76749938 0.16932802 10.4383158  2.825094e-24\n#&gt; treatment:cov2_c -0.07576978 0.08290933 -0.9138873  3.609977e-01\n\n\n\nThe estimate for treatment is our ATE estimate.\n\nIf all confounders are included, the new OLS estimate of \\(\\ate\\) in this estimator will technically still be a biased estimator of the ATE, but the bias is negligible.\nThis estimator (as a linear estimator naturally does) assumes that the relationship between confounders and outcome is linear. If this assumption is hard to justify, then our estimate of \\(\\ate\\) will be biased.",
    "crumbs": [
      "Selection on Observables",
      "Fully Interacted Estimator"
    ]
  },
  {
    "objectID": "soo/ipw.html",
    "href": "soo/ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "# Packages Needed:\nlibrary(fixest)\n\nLet us look at this example, with a confounder.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nReceiving Scholarship\n\n\n\nY\n\nUniversity Grades\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nOur treated and control groups might look like:\n\n\n\n\n\n\n\n\nTreated (Got Scholarship)\nUntreated (Did not get scholarship)\n\n\nSmart Students (x4)\nSmart Students (x1)\n\n\nDumb Students (x1)\nDumb Students (x4)\n\n\n\n\nOur two groups have pre-existing differences. However, by emphasising certain individuals, we can make it seem like there are no more imbalances:\n\n\n\n\n\n\n\n\nTreated (Got Scholarship)\nUntreated (Did not get scholarship)\n\n\nSmart Students (x4)\nSmart Students (emphasise to x4)\n\n\nDumb Students (emphasise to x4)\nDumb Students (x4)\n\n\n\n\n\nSee how the underrepresented individuals in each group (treated/untreated) were weighted upwards. We can see there is no more pre-existing differences after weighting. Thus, selection bias has been solved.\n\nThe weights/emphasis an individual by the inverse of their likelihood to receive treatment. This is estimated as the inverse of the propensity score, which is estimated using all confounders.\n\nOmission of any confounder will cause inaccurate propensity scores, and inaccurate weights.\n\n\n# Logistic Model for Propensity Scores\npropensity &lt;- femlm(\n  fml    = treatment ~ cov1 + cov2,  # treat with confounders\n  data   = df,                       # your data\n  family = \"logit\"                   # specifies to use logistic reg\n)\n\n# Predice Propensity Scores for All Observations\ndf$pscore &lt;- predict(propensity, type = \"response\")\n\n# inverse of propensity score for weights:\ndf$ipw &lt;- ifelse(df$treatment == 1, 1/df$pscore, 1/(1-df$pscore))\n\nNow with the weights, we can estimate the causal effect with a regression:\n\nmodel &lt;- feols(\n  fml     = outcome ~ treatment,  # do not put confounders here\n  data    = df,                   # your data with weights\n  weights = ~ipw                  # use weights in regression\n)\n\ncoeftable(model) # Can also use summary(model)\n\n#&gt;             Estimate Std. Error  t value     Pr(&gt;|t|)\n#&gt; (Intercept) 1.255843 0.05857962 21.43823 3.655095e-84\n#&gt; treatment   1.759346 0.08626784 20.39399 1.485515e-77\n#&gt; attr(,\"type\")\n#&gt; [1] \"IID\"\n\n\n\nThe treatment estimate is our ATE.\n\nIf all confounders are included in the propensity score estimation, the estimate will be equal to the \\(\\ate\\).\nBecause weighting is dependent on the estimation of propensity scores, it is sensitive to poorly estimated propensity scores with the logistic regression, This tends to happen with smaller sample sizes, so the IPW estimator can be poor in small sample sizes.",
    "crumbs": [
      "Selection on Observables",
      "Inverse Probability Weighting"
    ]
  },
  {
    "objectID": "soo/estimators.html",
    "href": "soo/estimators.html",
    "title": "Quasi-Random Estimators",
    "section": "",
    "text": "There are many different estimators that can be used to control for confounders.\nChoosing the right estimator is difficult - it depends on the scenario in question. My recommendation is to try as many estimators as possible, and report all of their results. It is a good sign if all your estimators agree on the direction and significance of your causal effect.\n\n\n\n\n\n\n\nEstimator\nWhen to Use\n\n\n\n\nFully Interacted Estimator\n\nEstimand: ATE\n\nWhen you have data on all the confounders, and you believe the confounders have a linear relationship with the outcome.\n\n\nInverse Probability Weighting\n\nEstimand: ATE\n\nWhen you have data on all the confounders, and have a large dataset.\n\n\nDistance Matching\n\nEstimand: ATT\n\nWhen you have data on all the confounders, and have less than 3-5 confounders.\n\nNote: you should generally use genetic matching unless not possible.\n\n\n\nPropensity Score Matching\n\nEstimand: ATT\n\nWhen you have data on all the confounders, and have a large dataset.\n\nNote: you should generally use genetic matching unless not possible.\n\n\n\nGenetic Matching\n\nEstimand: ATT\n\nWhen you have data on all the confounders.\n\nNote: best matching method.\n\n\n\nCausal Forests\n\nEstimand: CATE\n\nWhen you are interested in heterogenous treatment effects.",
    "crumbs": [
      "Selection on Observables",
      "Estimators for SOO"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Basics of Causality",
    "section": "",
    "text": "This page covers how confounders cause pre-existing differences between treated and untreated (selection bias), meaning correlation is not causation.\n\nLet us look at this causal question:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nGoing to the Hospital (D)\n\n\n\nY\n\nHealth Outcomes (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nWe have a group that went to hospital (we will call this group “went”), and a group that “did not go”. Using our potential outcomes framework, we can define the treatment effect of the group that went to the hospital:\n\\[\n\\tau_\\text{went} = \\textcolor{green}{{\\text{health}}_{\\text{went}}^{(1)}} - \\textcolor{red}{{\\text{health}}_{\\text{went}}^{(0)}}\n\\]\n\nIn red is the counterfactual we do not observe. This is because the individuals who went to the hospital were treated, so we cannot see the world where they are in control.\n\nNow compare the treatment effects above to correlation, which is defined as the difference in observed outcomes:\n\\[\n\\begin{align}\n\\text{correlation} & =  \\text{health}_\\text{went} - \\text{health}_\\text{did not go} \\\\\n& = \\textcolor{green}{\\text{health}_{\\text{went}}^{(1)}} - \\textcolor{red}{\\text{health}_\\text{did not go}^{(0)}}\n\\end{align}\n\\]\nIf we compare this correlation to our \\(\\tau_\\text{treated}\\), we see:\n\\[\n\\text{if  } \\textcolor{red}{\\text{health}_\\text{did not go}^{(0)}} ≠ \\textcolor{red}{\\text{health}_\\text{went}^{(0)}}\n\\]\nIf this is true (they are different), then correlation is not causation.\n\nThese two quantities are potential outcomes under control, or in another way to think of it, outcomes of the two groups prior to treatment happening.\n\nThus, if there is a difference between the average health outcomes between those who went to the hospital, and those who did not go to the hospital, before treatment is administered, then correlation is not equal to causation. This is because we cannot tell if the difference between the groups is due to treatment, or due to their pre-existing differences.\n\nWhat causes pre-existing differences? Confounders. For example, in our hospital-health example, a confounder could be smoking.\n\nSmoking is not the only possible confounder, we just use it as an example. Drinking, age, etc. are all other potential confounders.\n\nSmoking will worsen health outcomes. Someone who smokes is also more likely to visit the hospital with health complications. That means people who go to the hospital start out with (on average) worse health outcomes than people who did not go to the hospital.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nGoing to the Hospital (D)\n\n\n\nY\n\nHealth Outcomes (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmoking (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA confounder is a third variable that has the following characteristics:\n\nThe confounder is correlated (positive or negative) with the outcome variable.\nThe confounder causes who gets and doesn’t get the treatment.\nThe confounder is not itself caused by the treatment\n\n\n\n\n\nNote requirement 3 - it is a common mistake. Any result of the treatment \\(D\\) cannot be a confounder.\n\nConfounders cause pre-existing differences, which cause correlation to not equal causation. We must account for confounders to uncover causal effects.",
    "crumbs": [
      "Home",
      "Issue of Selection Bias"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resources on Causal Inference and Social Science Statistics",
    "section": "",
    "text": "For my personal website, see http://kevinli03.github.io\nThis repository contains resources on causal inference and social statistics methods, and implementing these methods in R.\n\nDifference-in-Differences, including modern advancements from 2021-2025.\nOther causal estimators, such as instrumental variables, regression discontinuity, and more.\nLatent variable models, such as factor analysis, item response theory, and structural equation modelling.\n\nThe home page sidebar contains basics on regression, and why correlation is not causation.\n\nThis repository isn’t meant to teach you these methods (although I do include explanations of these methods). It is primary aimed as a reference when implementing methods. Check out the additional resources for more teaching material.\n\nUse the top navigation bar to navigate between different methods. Use the side navigation bar to navigate within a method.",
    "crumbs": [
      "Home",
      "Homepage"
    ]
  },
  {
    "objectID": "frameworks.html",
    "href": "frameworks.html",
    "title": "Basics of Causality",
    "section": "",
    "text": "This page covers the potential outcomes framework and the causal estimands.\n\nIn causal inference, we are interested in causal questions:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nTreatment (D)\n\n\n\nY\n\nOutcome (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\n\nWe generally assume the treatment \\(D\\) is binary.\n\nImagine we have two hypothetical parallel worlds that are copies of each other. Both of these worlds are identical except for one aspect: the treatment:\n\n\n\n\n\n\n\n\nParallel World\nTreatment\nPotential Outcome\n\n\nUnit \\(i\\) does not Receive Treatment\n\\(\\text{treatment}_i = 0\\)\n\\(\\T\\)\n\n\nUnit \\(i\\) Receives Treatment\n\\(\\text{treatment}_i = 1\\)\n\\(\\Cred\\)\n\n\n\n\nThe only difference between the two worlds is the treatment. Thus, any difference in outcomes between the two worlds must be the causal effect of the treatment.\n\\[\n\\tau_i = \\T - \\Cred\n\\]\n\nTechnically, we need another assumption, SUTVA, for this to be true. I will explain this assumption as part of the identification assumptions.\n\nHowever, in reality, we do not have two parallel worlds. Thus, by definition, one of the potential outcomes is not observed in our real world - the counterfactual.\n\n\n\n\n\n\n\n\nIn the Real World\nObserved Outcome\nCounterfactual\n\n\n\\(i\\) receives treatment\n\\(\\Y= \\T\\)\n\\(\\Cred\\)\n\n\n\\(i\\) did not receive treatment\n\\(\\Y = \\Cred\\)\n\\(\\T\\)\n\n\n\n\nThe fundamental problem of causal inference is that in order to calculate our individual treatment effect \\(\\tau\\), we need both potential outcomes. Our goal is to estimate causal effects without observing counterfactuals. This is difficult at the individual level, so instead, we focus on average treatment effects for groups:\n\n\n\n\n\n\n\n\nGroup Effects\nNotation\nDefinition\n\n\nAverage Treatment Effect (ATE)\n\\(\\tau_\\text{ATE}\\)\nThe average treatment effects for all individuals in our study (treated and untreated).\n\n\nAverage Treatment Effect on the Treated (ATT)\n\\(\\tau_\\text{ATT}\\)\nThe average treatment effect but only for individuals who receive the treatment in our study.\n\n\nLocal Average Treatment Effect (LATE)\n\\(\\tau_\\text{LATE}\\)\nThe average treatment effect but only for a specific (local) group of individuals in a study.",
    "crumbs": [
      "Home",
      "Basics of Causality"
    ]
  },
  {
    "objectID": "did/match.html",
    "href": "did/match.html",
    "title": "PanelMatch Estimator",
    "section": "",
    "text": "# Packages Needed:\nlibrary(PanelMatch)\n\nThe PanelMatch estimator developed by Imai, Kim, and Wang (2023) is designed for staggered DiD and Non-absorbing DiD (treatment reversal), accounting for issues in TWFE. Before we start, we need to clean our data.\n\n# we need unit and time var to be integers\ndf$unit &lt;- as.integer(df$unit)\ndf$time &lt;- as.integer(df$time)\n\n# PanelMatch does not like tidy dataframes, so do this:\ndf &lt;- as.data.frame(df)\n\ndf.cleaned &lt;- PanelData(\n  panel.data = df,        # your data\n  unit.id    = \"unit\",    # your unit var (integer only)\n  time.id    = \"time\",    # your time period var (integer only)\n  treatment  = \"treat\",   # your treatment var\n  outcome    = \"outcome\"  # your outcome var\n)\n\nThe estimator is based on the potential outcomes framework. For a treated unit, their individual causal effect is.\n\\[\n\\tau_{it} = \\pT - \\pCred\n\\]\nOur issue is that we do not observe \\(\\pCred\\) for treated units. PanelMatch finds an untreated unit from the same time period, with similar “history” and characteristics, and uses that unit as the missing potential outcome:\n\\[\n\\tau_{it} \\approx \\pT - \\red{Y_{\\text{matched individual}, t}^{(0)}}\n\\]\nHow does PanelMatch determine which untreated unit is selected as a match? It depends on 2 things:\n\nThe past “history” of the matched individual before time period \\(t\\). This includes their past treatment history, and their past outcome values.\n\n\nThe authors consider the history in the “lag” period - which is a set number of periods before the given \\(t\\) (we can choose how many years).\n\n\n(optional) if covariates are specified, it will use also consider distance matching on covariates to find a match.\n\n\nmatch &lt;- PanelMatch(\n  lag               = 3,              # how many periods of history to consider\n  panel.data        = df.cleaned,     # PanelData object\n  lead              = c(0:3),         # how many post-treat dynamic effects\n  covs.formula      = ~ covariate,    # covaraites to include\n  refinement.method = \"mahalanobis\",  # if no covariates, put \"none\"\n  qoi               = \"att\",\n  match.missing     = TRUE\n)\n\nOnce we have found the matches, we can now estimate the ATT:\n\nmodel &lt;- PanelEstimate(\n  sets       = match,       # PanelMatch object\n  panel.data = df.cleaned,  # PanelData object\n  pooled     = TRUE\n)\n\nsummary(model)\n\n#&gt;      estimate std.error     2.5%   97.5%\n#&gt; [1,] 3.591479  1.155658 1.184963 5.62424\n\n\nThe output will be our ATT estimate. We can also estimate dynamic treatment effects for post-treatment periods:\n\nmodel &lt;- PanelEstimate(\n  sets       = match,       # PanelMatch object\n  panel.data = df.cleaned,  # PanelData object\n  pooled     = FALSE\n)\n\nsummary(model)\n\n#&gt;      estimate std.error       2.5%    97.5%\n#&gt; t+0 0.9531179  1.308286 -1.8865743 3.358097\n#&gt; t+1 4.5112386  1.288098  1.8311327 6.970220\n#&gt; t+2 3.9352750  1.696733  0.5661204 7.250733\n#&gt; t+3 4.9662839  1.399781  2.0547663 7.503598",
    "crumbs": [
      "Difference-in-Differences",
      "PanelMatch Estimator"
    ]
  },
  {
    "objectID": "did/did2s.html",
    "href": "did/did2s.html",
    "title": "Two-Stage Difference-in-Differences",
    "section": "",
    "text": "# Packages Needed:\nlibrary(did2s)\n\nThe 2-Stage DiD Estimator developed by Gardner (2021) is designed for staggered DiD, accounting for issues in TWFE. The TWFE estimator implies that untreated units \\(D_{it} = 0\\) equals the following:\n\\[\nY_{it} (D_{it} = 0) = \\unit + \\time + \\cov + \\eps_{it}\n\\]\n\nThis equation is obtained from plugging in \\(D_{it} = 0\\) into the two-way fixed effects equation.\n\nThis means parameters \\(\\unit\\), \\(\\time\\), and \\(\\beta\\) should be estimated with only units \\(D_{it} = 0\\). However, in two-way fixed effects, we estimate \\(\\unit\\), \\(\\time\\), and \\(\\beta\\) with both units \\(D_{it} = 0\\) and \\(D_{it} = 1\\). So, we are actually improperly estimating \\(Y_{it}(D_{it} = 0)\\) in two-way fixed effects.\n\nThis is another fancy way of explaining the forbidden comparisons problem - in TWFE, \\(Y_{it}(D_{it} = 0)\\) is mis-estimated to include already-treated units, which is the forbidden comparison.\n\nGardner (2021) proposes to solve this by spliting the TWFE estimator into two parts.\nIn the first-stage, we estimate \\(\\unit\\), \\(\\time\\), and \\(\\beta\\) in a regression with only untreated units \\(D_{it} = 0\\). This will ensure the proper estimation of \\(Y_{it}(D_{it} = 0)\\). Then, using our first-stage estimates of \\(\\widehat\\unit\\), \\(\\widehat\\time\\), and \\(\\widehat\\beta\\), in the second stage regression, estimate \\(\\att\\).\n\nmodel &lt;- did2s(\n  data         = df,                         # your data\n  yname        = \"outcome\",                  # outcome var\n  first_stage  = ~ covariate | unit + time,  # if no covariate: ~ 0 | unit + time\n  second_stage = ~ i(treatment),             # replace D with your treatment\n  treatment    = \"treatment\",                # treatment var\n  cluster_var  = \"unit\"                      # unit var\n)\n\ncoeftable(model) # Can also use summary(model)\n\n#&gt;                   Estimate   Std. Error   t value     Pr(&gt;|t|)\n#&gt; treatment::0 -3.696146e-16 3.319066e-17 -11.13610 2.431288e-27\n#&gt; treatment::1  4.848501e+00 4.512049e-01  10.74567 1.172815e-25\n#&gt; attr(,\"type\")\n#&gt; [1] \"Custom\"\n\n\n\nOur ATT is the estimate for treatment::1, which in this case is 4.845.\n\nFor estimation of dynamic treatment effects, we do the following:\n\nmodel &lt;- did2s(\n  data         = df,                                 # your data\n  yname        = \"outcome\",                          # outcome var\n  first_stage  = ~ covariate | unit + time,          # if no covariate: ~ 0 | unit + time\n  second_stage = ~ i(rel.time, ref = c(-1, 10000)),  # don't change -1. 10000 change to value for never-treated\n  treatment    = \"treatment\",                        # treatment var\n  cluster_var  = \"unit\"                              # unit var\n)\n\niplot(\n  object   = model,            # your model from did2s\n  drop     = \"[[:digit:]]{2}\", # this drops the time periods too far before and after treatment.\n  ref.line = 0                 # adds a dotted line at treatment period\n)\n\n\n\n\n\n\n\n\n\n\nSee Kyle Butts’ github page for more info on this estimator. Asjad Naqvi also has a helpful page.\nAlso see the documentation of the did2s function.",
    "crumbs": [
      "Difference-in-Differences",
      "2-stage DiD Estimator"
    ]
  },
  {
    "objectID": "did/twfe.html",
    "href": "did/twfe.html",
    "title": "Two-Way Fixed Effects Estimator",
    "section": "",
    "text": "# Packages Needed:\nlibrary(fixest)\n\nThe two-way fixed effects estimator is the estimator for classical DiD, and works (albiet poorly) for staggered DiD and non-absorbing DiD:\n\\[\nY_{it} = \\underbrace{\\unit + \\time}_{\\text{fixed effects}} + D_{it}\\att + \\cov + \\eps_{it}\n\\]\n\nIn repeated cross-sections, we have to replace the unit fixed effects \\(\\alpha_i\\) with group fixed effects, group being the level on which treatment is assigned (ex. if treatment is assigned at a state level, we use state fixed effects).\n\n\nmodel &lt;- feols(\n  fml  = outcome ~ treatment + covariate | unit + time,\n  data = df\n)\n\ncoeftable(model) # Can also use summary(model)\n\n#&gt;            Estimate Std. Error  t value     Pr(&gt;|t|)\n#&gt; treatment 4.8506451  0.4552220 10.65556 1.667568e-18\n#&gt; covariate 0.9721658  0.0459722 21.14682 5.226930e-40\n#&gt; attr(,\"type\")\n#&gt; [1] \"Clustered (unit)\"\n\n\n\nThe estimate for treatment is the ATT. In this example, it is 4.850645.\n\nFor dynamic treatment effects, we can\n\n# rel.time is: time period - initial treatment period of a unit\n# thus, rel.time = 0 is first treatment year. negative is pre-treat.\n\nmodel &lt;- feols(\n  fml  = outcome ~ i(rel.time, -1) + covariate | unit + time,\n  data = df\n)\n\niplot(\n  object   = model,\n  drop     = \"[[:digit:]]{2}\",  # this limits pre-post period to -9 to 9.\n  ref.line = 0\n)\n\n\n\n\n\n\n\n\n\nNegative rel.time (pre-treatment) should have 0 significant effects if parallel trends is met. Here, we can see parallel trends is violated. Post rel.time are the dynamic treatment effects over time.\n\nAs has been mentioned, TWFE is a poor estimator for staggered DiD or non-absorbing DiD. The reason for this is because of two factors:\n\nTWFE will make “forbidden” comparisons between treatment groups. For example, TWFE will compare treated units to later-treated units. This is an issue - causal effects should only compare treated to untreated units.\nTWFE does not weight comparisons properly. In the real ATT, comparisons between different groups should be weighted based on how large these different groups are (how many units are in each group). But, in TWFE, weighting depends on treatment timing, not group size. This means some groups may be negatively weighted (which is nonsensical).\n\n\nThis site shows how you can explore the comparison and weighting issue in R. See Goodman-Bacon (2021) for a more technical overview of TWFE decomposition.\n\n\n\n\n\n\n\nIllustration of TWFE Issues\n\n\n\n\n\nThe figure below shows the comparisons TWFE makes, including the “forbidden” comparison D.\n\n\n\n\n\nThe figure below shows how TWFE comparisons are weighted:",
    "crumbs": [
      "Difference-in-Differences",
      "Two-way Fixed Effects"
    ]
  },
  {
    "objectID": "did/etwfe.html",
    "href": "did/etwfe.html",
    "title": "Extended Two-Way Fixed Effects",
    "section": "",
    "text": "# Packages Needed\nlibrary(etwfe)\n\nThe Extended Two-way Fixed Effects Estimator developed by Wooldridge (2021, 2023) is designed for staggered DiD, accounting for issues in TWFE. It is also capable of dealing with non-linear models such as negative binomial, poisson, and logistic.\nThe ETWFE estimator estimates all possible heterogeneous effects separately (by including a lot of interactions with initial treatment year, year, and covariates), using only valid comparisons, solving the forbidden comparison problem:\n\nmodel &lt;- etwfe(\n  fml    = outcome ~ covariate,  # formula\n  tvar   = time,                 # time period var\n  gvar   = first.treated,        # first treat year var\n  data   = df,                   # your data\n  vcov   = ~unit,                # cluster se by unit\n  family = NULL                  # can change to \"logit\", \"negbin\", \"poisson\"\n) \n\nMany coefficients \\(\\blue{\\tau_{it}}\\) and \\(\\blue{\\tau_{itx}}\\) are estimated that capture different heterogeneity in treatment effects. These coefficients are manually aggregated together into one ATT using proper weighting based on how frequent each heterogeneous treatment effect occurs in our sample. This solves the weighting problem of TWFE.\n\nemfx(model)\n\n#&gt; \n#&gt;  .Dtreat Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;     TRUE    -1.21      0.148 -8.23   &lt;0.001 52.2  -1.5 -0.925\n#&gt; \n#&gt; Term: .Dtreat\n#&gt; Type:  response \n#&gt; Comparison: TRUE - FALSE\n\n\n\nWe can see our estimated ATT is -1.21.\n\nWe can also aggregate these coefficients into dynamic treatment effects. However, ETWFE only supports post-treatment dynamic treatment effects, not pre-treatment effects.\n\ndynamic &lt;- emfx(model, type = \"event\")\nplot(dynamic)\n\n\n\n\n\n\n\n\n\nSince ETWFE does not support pre-treatment effects, we cannot use it to test for parallel trends.\n\nWooldridge’s 2023 paper also extends this framework to work with logistic regression, poisson regression, and negative binomial regression, which is useful when we deal with non-continuous outcome variables. This can be accessed by adding an argument to the etwfe() function:\n\nfamily = c(\"logit\", \"poisson\", \"negbin\")\n\n\nOnly choose one of them - you cannot choose multiple families together.\n\n\n\nFor technical details, see Wooldridge (2021) and Wooldridge (2023).\nFor details on the etwfe function, see the documentation.",
    "crumbs": [
      "Difference-in-Differences",
      "Extended Two-way Fixed Effects"
    ]
  },
  {
    "objectID": "did/didmultiple.html",
    "href": "did/didmultiple.html",
    "title": "DIDmultiple Estimator",
    "section": "",
    "text": "# Packages Needed:\nlibrary(DIDmultiplegtDYN)\n\nThe DIDmultiple estimator developed by de Chaisemartin and d’Hautlfoeuille (2020) is designed for staggered designs and classical designs. Its main selling point is its flexibility - it can deal with multiple treatment levels, including continuous treatment variables. Before starting, we generally want to omit NA’s from our data:\n\ndf &lt;- na.omit(df)\n# if your df is large, subset for only the var you will use, then na.omit\n\nThe estimator focuses on switchers - these are individuals that experience a change in treatment status during the study period. For each switcher, control groups are identified as units with the same initial treatment status, that did not change their treatment at the time the switcher did.\nBecause it focuses on switchers, the estimator is able to adapt to continuous treatments. This is particularly useful for many questions in economics/political science that are not just yes/no treatments, but that have levels to them.\n\nmodel &lt;- did_multiplegt_dyn(\n  df         = df,         # your data\n  outcome    = \"outcome\",  # outcome var\n  group      = \"unit\",     # UNIT variable (not cohort)\n  time       = \"time\",     # time period var\n  treatment  = \"treat\",    # treatment var\n  effects    = 4,          # number of post-treat dynamic effects\n  placebo    = 4,          # number of pre-treat effects\n  graph_off  = FALSE,      # if you don't want graph, put TRUE\n  continuous = NULL        # put 1 if it is continuous, can delete arg if not\n)\n\n\n\n\n\n\n\n\nWe can also view the overall ATT and summary below (the ATT is given in the estimate of the section that says “Average cumulative effect per treatment unit”):\n\nsummary(model)\n\n#&gt; \n#&gt; ----------------------------------------------------------------------\n#&gt;        Estimation of treatment effects: Event-study effects\n#&gt; ----------------------------------------------------------------------\n#&gt;              Estimate SE      LB CI    UB CI   N   Switchers\n#&gt; Effect_1     1.15913  1.22235 -1.23663 3.55490 108 55       \n#&gt; Effect_2     4.33475  1.30335 1.78023  6.88927 108 55       \n#&gt; Effect_3     3.82572  1.50893 0.86827  6.78317 108 55       \n#&gt; Effect_4     4.64014  1.28845 2.11483  7.16545 108 55       \n#&gt; \n#&gt; Test of joint nullity of the effects : p-value = 0.0009\n#&gt; ----------------------------------------------------------------------\n#&gt;     Average cumulative (total) effect per treatment unit\n#&gt; ----------------------------------------------------------------------\n#&gt;  Estimate        SE     LB CI     UB CI         N Switchers \n#&gt;   3.48994   1.03907   1.45341   5.52647       432       220 \n#&gt; Average number of time periods over which a treatment effect is accumulated: 2.5000\n#&gt; \n#&gt; ----------------------------------------------------------------------\n#&gt;      Testing the parallel trends and no anticipation assumptions\n#&gt; ----------------------------------------------------------------------\n#&gt;              Estimate SE      LB CI    UB CI   N   Switchers\n#&gt; Placebo_1    -0.88462 1.41062 -3.64939 1.88015 108 55       \n#&gt; Placebo_2    0.50413  1.31849 -2.08007 3.08832 108 55       \n#&gt; Placebo_3    -1.66447 1.38341 -4.37592 1.04697 108 55       \n#&gt; Placebo_4    -2.01542 1.33739 -4.63666 0.60582 108 55       \n#&gt; \n#&gt; Test of joint nullity of the placebos : p-value = 0.2963\n#&gt; \n#&gt; \n#&gt; The development of this package was funded by the European Union.\n#&gt; ERC REALLYCREDIBLE - GA N. 101043899\n\n\nThe output also computes a “joint test of nullity” for the placebos. This is basically a test for the parallel trends assumption. If p&gt;0.05 (not significant), then parallel trends is met.",
    "crumbs": [
      "Difference-in-Differences",
      "DiDmultiple Estimator"
    ]
  },
  {
    "objectID": "did/did.html",
    "href": "did/did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "See Scott Cunningham’s Causal Inference: The Mixtape for a more in depth overview.\n\nDifference-in-differences (DiD) is the most popular causal inference method, that has seen a “revolution” in the last few years. The method is designed for:\n\n\n\n\n\n\n\nPanel Data\nWe observe the same units over multiple periods of time.\n\n\nRepeated Cross Section\nWe observe different units over multiple periods of time.\n\n\n\n\nPanel data is usually preferred, but most estimators can accommodate repeated cross section.\n\nDiD exploits variation in treatment adoption over two dimensions. This typically means variation in treatment adoption between units, and variation of treatment adoption over time. There are three types of DiD designs:\n\n\n\n\n\n\n\nClassical DiD\nWhen there are two groups of units: the treatment group, and the control group. In the pre-treatment time periods, neither group adopts treatment. In the post-treatment time periods, only the treatment group adopts treatment.\n\n\nStaggered DiD\nWhen there are multiple treatment groups, who adopt treatment at different time-periods. For example, group A might adopt treatment in year 2000, group B might adopt treatment in year 2010, and group C is a control group.\n\n\nNon-Absorbing DiD\nA staggered (or classical) DiD, but some units who adopt treatment earlier may turn off treatment later (treatment reversal).\n\n\n\n\nThe choice of estimator will depend on the type of DiD.\n\nAll DiD require 3 identification assumptions:\n\n\n\n\n\n\n\nAssumption\nDescription\n\n\nParallel Trends\nThis means that had the treated group not received treatment, they would have followed the same trend as the control group.\n\n\nNo Anticipation\nThis means that treated units do not respond to treatment prior to the treatment being actually implemented (anticipation).\n\n\nStable Unit Treatment Value Assumption (SUTVA)\nThis means that if Ava is treated, that does not affect Mia’s outcome (and for any other 2 individuals).",
    "crumbs": [
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "did/estimators.html",
    "href": "did/estimators.html",
    "title": "Difference-in-Differences Estimators",
    "section": "",
    "text": "There are two quantities of interest in all DiD designs, that we aim to estimate:\n\nThe Average Treatment Effect on the Treated (ATT): the average effect of the treatment in question, for units that adopted the treatment. This is a one number summary of the effect of the treatment.\nDynamic Treatment Effects: These are ATT estimates, but for every year the treatment takes effect. For example, if treatment starts in 2000, dynamic treatment effects will exist for 2000, 2001, 2002, etc. This helps us see how the treatment evolves over time.\n\n\nDynamic treatment effects can only be estimated in classical DiD if we have more than 1 pre and post treatment period. Dynamic treatment effects can also be used to test the parallel trends assumption.\n\n\n\n\n\n\n\n\nEstimator\nWhen to Use\n\n\n\n\nTwo-Way Fixed Effects\nClassical, Staggered*, Non-Absorbing*\n\n*TWFE is biased when dealing with staggered DiD and reversal of treatment, if there is any heterogeneity in treatment effects.\n\n\n\nInteraction-Weighted Estimator\n\nSun and Abraham (2021)\n\nStaggered\n\nNotes: one of the most popular estimators, especially when dynamic treatment effects are of interest. Quite simple as well.\n\n\n\nDoubly-Robust Estimator\n\nCallaway and Sant’Anna (2021)\n\nStaggered\n\nNotes: Probably the most popular estimator in econometrics. Quite a complex estimator.\n\n\n\nImputation Estimator\n\nLiu, Wang, and Xu (2024)\n\nStaggered, With Reversal\n\nNotes: Probably the most popular in political science. Can be robust even with violations to parallel trends.\n\n\n\nPanelMatch Estimator\n\nImai, Kim, and Wang (2023)\n\nStaggered, Non-Absorbing\n\nNotes: Can be used in non-DiD panel and repeated cross-section designs.\n\n\n\nDIDmultiple Estimator\n\nDe Chaisemartin and d’Haultfoeuille (2021, 2024)\n\nStaggered, Classical, and Continuous Treatments\n\nNotes: one of the only ones that can accomodate continuous treatment variables.\n\n\n\nExtended Two-way Fixed Effects\n\nWooldridge (2021, 2023)\n\nStaggered, and Non-linear Models\n\nNotes: One of the only that can accomodate non-linear models, like logistic, negative binomial, and poisson. Cannot do pre-treatment test for parallel trends.\n\n\n\n2-Stage Difference-in-Differences\n\nGardner (2021)\n\nStaggered\n\nNotes: the most simple estimator for staggered designs, very similar to TWFE. However, can be less robust.",
    "crumbs": [
      "Difference-in-Differences",
      "DiD Estimators"
    ]
  },
  {
    "objectID": "did/design.html",
    "href": "did/design.html",
    "title": "How Difference-in-Differences Works",
    "section": "",
    "text": "This example is from the iconic DiD study conducted by Card and Kreuger (1993).\n\nIn April 1992, New Jersey increased its minimum wage. We might wonder how that caused changes in employment We have data from before and after the change:\n\n\n\n\n\n\n\nMar 1992 (Before Min. Wage Increase)\nDec 1992 (After Min. Wage Increase)\n\n\n\\(\\red{20.44}\\)\n\\(\\green{21.03}\\)\n\n\n\n\nRed indicates not treated, green indicates treated.\n\nOur treatment effect after the raise of minimum wage in December 1992 would be:\n\\[\n\\tau_\\text{Dec92} = \\green{\\text{employment}_\\text{Dec92}^{(\\text{treated})}} - \\red{\\text{employment}_\\text{Dec92}^{(\\text{control})}}\n\\]\nWe don’t observe the counterfactual in December 1992 without the minimum wage increase.\n\nWe cannot use March 1992 because there could have been something that happened between March and December that changes the trend in employment (like a recession).\n\nDifference-in-differences estimates the hypothetical by looking at another state that did not receive a change in minimum wage during this time - like Pennsylvania:\n\n\n\n\n\n\n\nMar 1992 (Pennsylvania, No Min. Wage Increase)\nDec 1992 (Pennsylvania, No Min. Wage Increase)\n\n\n\\(\\red{23.33}\\)\n\\(\\red{21.17}\\)\n\n\n\n\nThese are employment values in Pennsylvania, where no mininum wage changes occured.\n\nPennsylvania tells us the trend in employment without a change in minimum wage (-2.16). This trend tells us that if New Jersey hadn’t received a minimum wage increase, their employment rate would be the March 1992 level minus 2.16.\n\n\n\n\n\n\n\nMar 92 (NJ, Not Minimum Wage Increase)\nDec 92 (NJ, Hypothetical No Minimum Wage Increase)\n\n\n\\(\\red{20.44}\\)\n\\(\\red{\\text{March 92} - 2.16}\\)\n\n\n\n\nThe March 1992 value is copied from the first table.\n\nNow, we have an approximation of New Jersey’s missing hypothetical outcome, so we can now estimate the causal effect.\n\\[\n\\begin{align}\n\\tau_\\text{Dec92} & = \\green{\\text{employment}_\\text{Dec92}^{(\\text{treated})}} - \\red{\\text{employment}_\\text{Dec92}^{(\\text{control})}} \\\\\n& = \\green{\\text{employment}_\\text{Dec92}^{(\\text{treated})}} - \\left(\\red{\\text{employment}_\\text{Mar92}^{(\\text{control})}} - 2.16 \\right)\n\\end{align}\n\\]\n\nThis entire idea of using another group’s trend to estimate counterfactuals depends on the critical assumption of parallel trends: that we can indeed use Pennsylvania’s trend to estimate New Jersey’s trend under control.\nParallel trends essentially says that if the treated units had hypothetically not been treated, we should expect them to follow the same trend as the untreated units.\n\nWe also require the SUTVA assumption since our potential outcomes framework is no longer valid without it. We also require the obvious assumption of no anticipation - basically the treatment is actually taking effect when we are saying it is, and people aren’t anticipating and reacting before the actual treatment takes effect.",
    "crumbs": [
      "Difference-in-Differences",
      "How DiD Works"
    ]
  },
  {
    "objectID": "did/cs.html",
    "href": "did/cs.html",
    "title": "Doubly-Robust Estimator",
    "section": "",
    "text": "# Packages Needed\nlibrary(did)\n\nThe Doubly-Robust estimator developed by Callaway and Sant’Anna (2021) is designed for staggered DiD, accounting for issues in TWFE.\nAll units \\(i\\) are divided into groups, based on their first initial year of receiving treatment. Each unit who was first treated in year \\(g\\) is assigned to group \\(g\\).\nThen, we define a new causal estimand of interest: what they call the group-time ATT. This is essentially the dynamic treatment effect of group \\(g\\) at time \\(t\\). We estimate the group-time ATT.\n\nset.seed(344)\n\nmodel &lt;- att_gt(\n  yname                  = \"outcome\",          # outcome var\n  tname                  = \"time\",             # time period var\n  idname                 = \"unit\",             # unit var\n  gname                  = \"first.treated\",    # first treat year var\n  xformla                = ~ covariate,        # you can delete arg if no covariates\n  est_method             = 'dr',\n  base_period            = 'universal',        # don't change\n  allow_unbalanced_panel = T,\n  data                   = df                  # your data\n)\n\nNow, we can weight each group-time ATT together based on how frequent each group \\(g\\) is. By properly weighting, we solve the weighting issue of TWFE:\n\natt &lt;- aggte(\n  MP    = model,     # model from att_gt\n  type  = \"simple\",\n  na.rm = TRUE\n)\n\nsummary(att)\n\n#&gt; \n#&gt; Call:\n#&gt; aggte(MP = model, type = \"simple\", na.rm = TRUE)\n#&gt; \n#&gt; Reference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n#&gt; \n#&gt; \n#&gt;      ATT    Std. Error     [ 95%  Conf. Int.]  \n#&gt;  -1.1237        0.5207    -2.1444     -0.1031 *\n#&gt; \n#&gt; \n#&gt; ---\n#&gt; Signif. codes: `*' confidence band does not cover 0\n#&gt; \n#&gt; Control Group:  Never Treated,  Anticipation Periods:  0\n#&gt; Estimation Method:  Doubly Robust\n\n\n\nWe can see the ATT estimate is -1.1237.\n\nWe can also see ATT’s weighted to show dynamic treatment effects:\n\natt_dynamic &lt;- aggte(\n  MP    = model,      # model from att_gt\n  type  = \"dynamic\",\n  na.rm = TRUE\n)\n\nggdid(att_dynamic)\n\n\n\n\n\n\n\n\n\n\nSee this github page for more examples of implementing the code.",
    "crumbs": [
      "Difference-in-Differences",
      "Doubly-Robust Estimator"
    ]
  },
  {
    "objectID": "did/sunab.html",
    "href": "did/sunab.html",
    "title": "Interaction-Weighted Estimator",
    "section": "",
    "text": "# Packages Needed:\nlibrary(fixest)\n\nThe Interaction-Weighted estimator developed by Sun and Abraham (2021) is designed for staggered DiD, accounting for issues in TWFE.\nThe estimator divides all units into groups based on their initial treatment implementation. These groups are encoded into a new categorical variable \\(G_i\\).\nThen, the estimator calculates dynamic ATT’s separately for group, using only valid comparisons. This is done by interacting a relative-time variable \\(R_{it}\\) with \\(G_i\\). These interactions will produce many coefficients \\(\\blue{\\tau_{it}}\\) (in fact, one for each time period for each initial treatment year group). These coefficients capture all the heterogeneity in effects.\nThese coefficients are manually aggregated together into dynamic ATT’s by relative treatment time using proper weighting. This solves the weighting problem with TWFE.\n\nmodel &lt;- feols(\n  fml  = outcome ~ sunab(first.treated, time) + covariate | unit + time,\n  data = df,\n  vcov = ~unit\n)\n\niplot(\n  object   = model,             # object from feols\n  drop     = \"[[:digit:]]{2}\",  # this drops periods too far before/after treatment.\n  ref.line = 0                  # draws line at treatment period\n)\n\n\n\n\n\n\n\n\nWe can also aggregate the dynamic ATT’s into one singular ATT:\n\naggregate(model, \"att\")\n\n#&gt;      Estimate Std. Error   t value     Pr(&gt;|t|)\n#&gt; ATT -1.133749  0.2050705 -5.528584 2.882038e-07\n\n\n\n\nFor technical details, see Sun and Abraham (2021) or this github page.\nFor more options with the sunab and aggregate functions, see the documentation for fixest.",
    "crumbs": [
      "Difference-in-Differences",
      "Interaction-Weighted Estimator"
    ]
  },
  {
    "objectID": "did/ifect.html",
    "href": "did/ifect.html",
    "title": "Imputation Estimator",
    "section": "",
    "text": "# Packages Needed:\nlibrary(fect)\n\nThe Imputation Estimator developed by Liu, Xu, and Wang (2024) is designed for staggered DiD and Non-Absorbing DiD (treatment reversal), accounting for issues in TWFE.\nThe estimator proposes to estimate individual counterfactual untreated outcomes for treated units, and directly calculating the individual treatment effects:\n\\[\n\\tau_{it} = \\pT - \\pCred\n\\]\nThus, the goal of this interactive fixed effects counterfactual estimator (IFEct) is to estimate \\(\\pCred\\). They model these counterfactuals as:\n\\[\n\\pCred = \\underbrace{\\unit + \\time + \\cov}_{\\text{classic TWFE}} + \\b\\lambda_i'\\b\\xi_t + \\eps_{it}\n\\]\n\n\\(\\b\\xi_t\\) is a vector of latent variables/factors \\(\\xi_{1t}, \\xi_{2t}, \\dots\\). These latent variables change in magnitude depending on time \\(t\\) (they are time-varying). \\(\\b\\lambda_i\\) is a vector of factor loadings \\(\\lambda_{1i}, \\lambda_{2i}, \\dots\\), that describes the “influence” every latent factor has on unit \\(i\\). Different units \\(i\\) can have different relationships with each factor.\n\n\nset.seed(17)\n\nmodel &lt;- fect(\n  formula = outcome ~ treatment,  # add covariates after treatment\n  data    = df,\n  index   = c(\"unit\",\"time\"),     # unit var and time var\n  method  = \"ife\",                # do not change\n  se      = TRUE,\n  nboots  = 50                    # more is more accurate se, but takes longer\n) \n\nprint(model)\n\n#&gt; Call:\n#&gt; fect.formula(formula = outcome ~ treatment, data = df, index = c(\"unit\", \n#&gt;     \"time\"), method = \"ife\", se = TRUE, nboots = 50)\n#&gt; \n#&gt; ATT:\n#&gt;                             ATT    S.E. CI.lower CI.upper p.value\n#&gt; Tr obs equally weighted   2.281 0.07787    2.128    2.433       0\n#&gt; Tr units equally weighted 2.180 0.07752    2.028    2.332       0\n\n\nWe have two outputs - both are plausible ATT’s. It is generally recommended to use the first row - treated observations equally weighted, as it is typically the most accurate ATT estimate.\nWe can also plot our results for dynamic treatment effects and parallel trends tests:\n\nplot(model)\n\n\n\n\n\n\n\n\n\nSee Liu et al (2024) for more technical details.",
    "crumbs": [
      "Difference-in-Differences",
      "Imputation Estimator"
    ]
  },
  {
    "objectID": "rdd/rdd.html",
    "href": "rdd/rdd.html",
    "title": "rdd",
    "section": "",
    "text": "Coming Soon",
    "crumbs": [
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "linear.html",
    "href": "linear.html",
    "title": "Linear Regression",
    "section": "",
    "text": "# Pakcages Needed\nlibrary(fixest)\n\nLinear regression explores the relationship between several input/explanatory variables \\(X_1, X_2, \\dots, X_p\\) and a continuous outcome variable \\(Y\\).\n\nA binary \\(Y\\) outcome variable also works if your goal is correlation. For prediction, use logistic.\n\nThe linear regression model is specified as:\n\\[\nY_i = \\beta_0 +\\beta_1X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_pX_{ip} + \\eps_i\n\\]\nWhere \\(\\beta_0\\) is the intercept, and \\(\\beta_j\\) for \\(j \\in [1, p]\\) being the relationship between variable \\(X_j\\) and \\(Y\\). The model is estimated with the Ordinary Least Squares Estimator.\n\nmodel &lt;- feols(\n  fml  = outcome ~ var1 + var2 + var3,\n  data = df,\n  se   = \"hetero\"                      # heteroscedasticity robust standard errors\n)\n\nsummary(model)\n\n#&gt; OLS estimation, Dep. Var.: outcome\n#&gt; Observations: 1,000\n#&gt; Standard-errors: Heteroskedasticity-robust \n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; (Intercept) 0.735941   0.043874 16.7741 &lt; 2.2e-16 ***\n#&gt; var1        0.951199   0.060564 15.7056 &lt; 2.2e-16 ***\n#&gt; var2        0.490729   0.032025 15.3233 &lt; 2.2e-16 ***\n#&gt; var3        1.872961   0.100573 18.6228 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 0.927491   Adj. R2: 0.426251\n\n\n\nThe \\(\\beta_0\\) is the (intercept) estimate. \\(\\beta_j\\) are the other estimates.\n\nThe estimated coefficients are interpreted as follows:\n\n\n\n\n\n\n\n\n\nContinuous \\(X_j\\)\nBinary \\(X_j\\)\n\n\n\\(\\hat\\beta_j\\)\nFor every one unit increase in \\(X_j\\), there is an expected \\(\\hat\\beta_j\\) unit change in \\(Y\\), holding all other explanatory variables constant.\nThere is a \\(\\hat\\beta_j\\) unit difference in \\(Y\\) between category \\(X_j = 1\\) and category \\(X_j = 0\\)\n\n\n\\(\\hat\\beta_0\\) (intercept)\nWhen all explanatory variables equal 0, the expected value of \\(Y\\) is \\(\\hat\\beta_0\\).\nWhen all explanatory variables equal 0, the expected value of \\(Y\\) is \\(\\hat\\beta_0\\).\n\n\n\n\nFor binary \\(Y\\), change expected unit to expected probability of \\(Y=1\\).\n\nThe coefficients have a t-test of significance run on them. Anything p&lt;0.05 (with stars) is considered statistically significant. A statistically significant \\(\\hat\\beta_j\\) indicates that there is a relationship/correlation between \\(X_j\\) and \\(Y\\).\nWe can also predict values of \\(Y\\) given we have the values of the explanatory variables using the fitted values equation:\n\\[\n\\widehat Y_i = \\hat\\beta_0 + \\hat\\beta_1 X_{i1} + \\hat\\beta_2X_{i2} + \\dots + \\hat\\beta_pX_{ip}\n\\]\n\npred.df &lt;- predict(\n  object  = model,  # regression model object\n  newdata = df      # replace with df of X values for which to predict Y\n)\n\nhead(pred.df)\n\n#&gt; [1] 1.5463586 2.5480953 0.8505077 1.9486251 1.6628466 1.7628463\n\n\n\nThese are the first 6 observations’ predicted Y. You can save these predictions into any dataframe/vector.",
    "crumbs": [
      "Home",
      "Linear Regression"
    ]
  },
  {
    "objectID": "random.html",
    "href": "random.html",
    "title": "The Magic of Randomisation",
    "section": "",
    "text": "Let us say we are interested in this question:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nOur concern is a confounder. For example, smartness of an individual could mean they are more likely to get a scholarship. Since smart people tend to perform well at university, that means the people who get treated are different from those who don’t get treated.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmartness is not the only confounder. Other confounders could be family income, athletic ability, etc.\n\nBut what if randomness (like flipping a coin) controls who gets the treatment or not. A coin will be flipped to decide if every person in our study will get the scholarship. This means that the randomness (the coin), and not the confounder, are causing selection into treatment:\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;Y\n\n\n\n\n\n\nO\n\nCoin\n\n\n\nO-&gt;D\n\n\n\n\n\n\n\n\n\n\n\nSince the confounder is no longer causing who gets the treatment and who doesn’t, that means there is no more concern of selection bias.\n\nRandomisation also means that every individual has the same chance of being treated or untreated, so the two groups will, on average, the same as each other. That means:\n\\[\n\\textcolor{red}{\\text{grades}_\\text{untreated}^{(0)}} = \\textcolor{red}{\\text{grades}_\\text{treated}^{(0)}}\n\\]\nAnd thus, since that is true, that means correlation equals causation.\n\nThis is established by the law of large numbers, but it is a little technical for here.\n\nSo if our treatment is randomly assigned (individuals randomly assigned to treatment or control), correlation does equal causation.\n\nRandomisation is the gold standard of causal inference. There is no better method.\n\nRandomisation is possible if you are running your own experiment: you can use a random number generator to assign treatment.\nRandomisation is also possible if there is something that is being randomly assigned in the real world. For example, the US green card lottery randomly chooses who gets accepted.\n\nHowever, randomisation is not always possible to due to cost of running experiments, non-compliance of individuals within experiments, and impracticality.\n\nNon-compliance is an issue that can be solved pretty easily with an instrumental variable, given a few assumptions about the non-compliance.",
    "crumbs": [
      "Home",
      "Magic of Randomisation"
    ]
  },
  {
    "objectID": "soo/distance.html",
    "href": "soo/distance.html",
    "title": "Distance Matching",
    "section": "",
    "text": "# Packages Needed:\nlibrary(Matching)\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red), since Mia receives the treatment.. However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Matched Individual}}\n\\]\nDistance matching matches an individual that is treated (like Mia) with one that is not treated based on how close their confounding values are. We define closeness by Mahalanobis distance:\n\\[\n\\text{distance}_{i, j} = \\sqrt{(\\b x_i - \\b x_j)' \\ \\b\\Sigma_x^{-1} (\\b x_i - \\b x_j)}\n\\]\n\nWhere \\(i\\) and \\(j\\) are two units we want to measure the distance between, \\(\\b x\\) is a vector of confounder values, and \\(\\b\\Sigma_x\\) is the covariance matrix of confounders. We must include all confounders.\n\n\nmodel &lt;- Match(\n  Y          = df$outcome,             # outcome var\n  Tr         = df$treatment,           # treatment var\n  X          = df[,c(\"cov1\",\"cov2\")],  # confounder vars\n  M          = 1,                      # number of matches for each treated unit\n  BiasAdjust = TRUE,                   # modern adjustment - don't change\n  Weight     = 2                       # don't change\n)\n\nsummary(model)\n\n#&gt; \n#&gt; Estimate...  1.4387 \n#&gt; AI SE......  0.12251 \n#&gt; T-stat.....  11.744 \n#&gt; p.val......  &lt; 2.22e-16 \n#&gt; \n#&gt; Original number of observations..............  1000 \n#&gt; Original number of treated obs...............  262 \n#&gt; Matched number of observations...............  262 \n#&gt; Matched number of observations  (unweighted).  328\n\n\nIf all confounders are included, our output estimate will be the ATT - the average treatment effect for those units who received the treatment.\n\nOmission of any confounder will cause inaccurate matches, thus causing inaccurate results.\n\nDistance matching suffers from the curse of dimensionality. This means that as the number of confounders increase, the bias in distance matching estimates becomes larger. Thus, we should avoid using distance matching with more than 3-5 confounders.\n\nKevin’s comments: there is little reason to use distance matching over genetic matching, unless your machine lacks the computational power for genetic matching.\n\nDistance matching (and all matching) also throws out a lot of data that is unmatched, which can be wasteful.",
    "crumbs": [
      "Selection on Observables",
      "Distance Matching"
    ]
  },
  {
    "objectID": "soo/soo.html",
    "href": "soo/soo.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "Our issue in causal inference is that a confounder is causing pre-existing differences:\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nReceiving Scholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nIntellegence (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\nBy definition, as the confounder changes, your likelihood of getting treatment changes. As the confounder changes, the outcome value will also change.\nThese issues occur when the confounder changes in value. So what if we hold the confounders constant? Then, there would be no changes in confounders - so the variation in treatment assignment and outcomes cannot be attributed to the confounder.\nFor example, let’s assume that intelligence has two values: smart and dumb. Let us calculate the treatment effect on university grades within each level of intelligence:\n\\[\n\\begin{align}\n\\tau_\\text{smart} & = \\green{Y_\\text{smart}^{(1)}} - \\red{Y_\\text{smart}^{(0)}}\\\\\n\\tau_\\text{dumb} & = \\green{Y_\\text{dumb}^{(1)}} - \\red{Y_\\text{dumb}^{(0)}}\n\\end{align}\n\\]\nThe confounder is constant here, so no selection bias. Thus, within each category, correlation is equal to causation. Our overall causal effect will be a weighted average of the categories:\n\\[\n\\tau = \\tau_\\text{smart} Pr(\\text{smart})  \\ + \\ \\tau_\\text{dumb} Pr(\\text{dumb})\n\\]\n\nThe weights of this weighted average are the probability/frequency of that value of the confounder.\n\nObviously, most confounders have more than 2 categories, and we often have more confounders. But the same intuition applies.\n\\[\n\\tau = \\sum_X \\tau_\\text{X} \\cdot Pr(X)\n\\]\n\nFor selection on observables to work, we need to meet 3 assumptions:\n\n\n\n\n\n\n\nAssumption\nDescription\n\n\nConditional Ignorability\nThis means that we must account for all possible confounders (cannot miss a single one).\n\n\nCommon Support\nThis means no one can have a 100% chance of being in treatment or control. They always have a chance to be in either, no matter their confounding values.\n\n\nStable Unit Treatment Value Assumption (SUTVA)\nThis means that if Ava is treated, that does not affect Mia’s outcome (and for any other 2 individuals).\n\n\n\n\nWe have a wide choice of estimators that we can use. Use the sidebar or links in the table to access each estimator’s page.",
    "crumbs": [
      "Selection on Observables"
    ]
  },
  {
    "objectID": "soo/pscore.html",
    "href": "soo/pscore.html",
    "title": "Propensity Score Matching",
    "section": "",
    "text": "# Packages Needed:\nlibrary(Matching)\nlibrary(fixest)\n\nMia is in our study and receives the treatment. Mia’s causal effect is:\n\\[\n\\tau_{\\text{Mia}} = \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Mia}}\n\\]\nWe cannot observe Mia’s counterfactual (in red), since Mia receives the treatment.. However, what we can do is to find an untreated individual similar to Mia to approximate Mia’s counterfactual:\n\\[\n\\tau_{\\text{Mia}} \\approx \\textcolor{green}{Y^{(1)}_\\text{Mia}} - \\textcolor{red}{Y^{(0)}_\\text{Matched Individual}}\n\\]\nPropensity Score Matching matches an individual that is treated (like Mia) with one that is not treated based on how similar their likelihoods of treatment are.\nWhat is a likelihood of treatment? Well we know confounders cause people to get the treatment or not treatment. Thus, using an individual’s confounder values, we can estimate their likelihood of getting treatment, called a propensity score.\n\\[\n\\text{propensity score } \\pi =Pr(\\green{D_i = 1})\n\\]\nPropensity scores are typically estimated with a logistic regression with all confounders as explanatory variables.\n\nThis also means that propensity score matching shares the same weaknesses of logistic regression - including assuming linear relationships between confounders and propensities, and only being unbiased in large sample sizes.\n\n\n# Logistic Model for Propensity Scores\npropensity &lt;- femlm(\n  fml    = treatment ~ cov1 + cov2,  # include all confounders\n  data   = df,                       # your data\n  family = \"logit\"                  # tells R to use logistic reg\n)\n\n# Predice Propensity Scores for All Observations\ndf$pscore &lt;- predict(propensity, type = \"response\")\n\nNow with the propensity scores, we can conduct distance matching, matching units by how close their propensity scores are.\n\natt &lt;- Match(\n  Y          = df$outcome,     # outcome var\n  Tr         = df$treatment,   # treatment var\n  X          = df[,\"pscore\"],  # propensity score var\n  M          = 1,              # number of matches for each treated unit\n  BiasAdjust = TRUE,           # modern adjustment - don't change\n  Weight     = 2               # don't change\n)\n\nsummary(att)\n\n#&gt; \n#&gt; Estimate...  1.5479 \n#&gt; AI SE......  0.13624 \n#&gt; T-stat.....  11.362 \n#&gt; p.val......  &lt; 2.22e-16 \n#&gt; \n#&gt; Original number of observations..............  1000 \n#&gt; Original number of treated obs...............  253 \n#&gt; Matched number of observations...............  253 \n#&gt; Matched number of observations  (unweighted).  412\n\n\nIf all confounders are included, our output estimate will be the ATT - the average treatment effect for those units who received the treatment.\n\nOmission of any confounder will cause inaccurate matches, thus causing inaccurate results.\n\nBecause propensity-score matching is dependent on the estimation of propensity scores, it is sensitive to poorly estimated propensity scores with the logistic regression, This tends to happen with smaller sample sizes, so the propensity score matching estimator can be poor in small sample sizes.\n\nKevin’s Comments: there is little reason to use propensity score matching over genetic matching, unless your machine cannot estimate genetic matching.\n\nPropensity score matching (and all matching) also throws out a lot of data that is unmatched, which can be wasteful.",
    "crumbs": [
      "Selection on Observables",
      "Propensity Score Matching"
    ]
  },
  {
    "objectID": "soo/random.html",
    "href": "soo/random.html",
    "title": "The Magic of Randomisation",
    "section": "",
    "text": "Let us say we are interested in this question:\n\n\n\n\n\n\n\n\nexample1\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\n\n\n\n\n\n\nOur concern is a confounder. For example, smartness of an individual could mean they are more likely to get a scholarship. Since smart people tend to perform well at university, that means the people who get treated are different from those who don’t get treated.\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;D\n\n\n\n\n\nX-&gt;Y\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmartness is not the only confounder. Other confounders could be family income, athletic ability, etc.\n\nBut what if randomness (like flipping a coin) controls who gets the treatment or not. A coin will be flipped to decide if every person in our study will get the scholarship. This means that the randomness (the coin), and not the confounder, are causing selection into treatment:\n\n\n\n\n\n\n\n\nexample2\n\n\nD\n\nScholarship (D)\n\n\n\nY\n\nUniversity Grades (Y)\n\n\n\nD-&gt;Y\n\n\nCausal Effect\n\n\n\nX\n\nSmartness (Confounder)\n\n\n\nX-&gt;Y\n\n\n\n\n\n\nO\n\nCoin\n\n\n\nO-&gt;D\n\n\n\n\n\n\n\n\n\n\n\nSince the confounder is no longer causing who gets the treatment and who doesn’t, that means there is no more concern of selection bias.\n\nRandomisation also means that every individual has the same chance of being treated or untreated, so the two groups will, on average, the same as each other. That means:\n\\[\n\\textcolor{red}{\\text{grades}_\\text{untreated}^{(0)}} = \\textcolor{red}{\\text{grades}_\\text{treated}^{(0)}}\n\\]\nAnd thus, since that is true, that means correlation equals causation.\n\nThis is established by the law of large numbers, but it is a little technical for here.\n\nSo if our treatment is randomly assigned (individuals randomly assigned to treatment or control), correlation does equal causation.\n\nRandomisation is the gold standard of causal inference. There is no better method.\n\nRandomisation is possible if you are running your own experiment: you can use a random number generator to assign treatment.\nRandomisation is also possible if there is something that is being randomly assigned in the real world. For example, the US green card lottery randomly chooses who gets accepted.\n\nHowever, randomisation is not always possible to due to cost of running experiments, non-compliance of individuals within experiments, and impracticality.\n\nNon-compliance is an issue that can be solved pretty easily with an instrumental variable, given a few assumptions about the non-compliance."
  },
  {
    "objectID": "latent/sem.html",
    "href": "latent/sem.html",
    "title": "Structural Equation Models",
    "section": "",
    "text": "# Packages Needed\nlibrary(lavaan)\n\nStructural Equation Models (SEMs) allow us to combine latent models with linear regression models between the latent variables.\n\n\n\n\n\n\n\n\nexample2\n\n\nF1\n\nξ (Unobserved)\n\n\n\nX1\n\nX1\n\n\n\nF1-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2\n\n\n\nF1-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3\n\n\n\nF1-&gt;X3\n\n\nλ\n\n\n\nF2\n\nη (Unobserved)\n\n\n\nF1-&gt;F2\n\n\nLinear Model\n\n\n\nX4\n\nY1\n\n\n\nF2-&gt;X4\n\n\nλ\n\n\n\nX5\n\nY2\n\n\n\nF2-&gt;X5\n\n\nλ\n\n\n\nX6\n\nY3\n\n\n\nF2-&gt;X6\n\n\nλ\n\n\n\n\n\n\n\n\n\nThis structural equation model contains a linear regression between the latent explanatory variable explaining the latent outcome variable.\n\\[\n\\eta = \\beta_0 + \\beta_1\\ \\xi + \\eps\n\\]\nBoth \\(\\eta\\) and \\(\\xi\\) are latent variables, which need to be estimated with a latent measurement model like factor analysis. We can estimate all measurement models and structural regressions together in a structural equation model:\n\nformula &lt;- '\n# 2 Measurement Models\n  factor1 =~ rich + admire + success + respect\n  factor2 =~ friend + equal + nature + care\n\n# Stuctural regression\n  factor1 ~ factor2\n'\n\nmodel &lt;- sem(\n  model   = formula,  # formula from above\n  data    = df,       # your data\n  std.ov  = TRUE,     # standardises the observed variables\n  std.lv  = TRUE,\n  missing = \"fiml\"    # this allows use of missing data, but may take longer\n)\n      \ncoef(model)\n    # You can also use summary(model), which gives a more comprehensive output\n\n#&gt;    factor1=~rich  factor1=~admire factor1=~success factor1=~respect \n#&gt;            0.473            0.677            0.808            0.481 \n#&gt;  factor2=~friend   factor2=~equal  factor2=~nature    factor2=~care \n#&gt;            0.598            0.375            0.426            0.668 \n#&gt;  factor1~factor2       rich~~rich   admire~~admire success~~success \n#&gt;            0.202            0.767            0.522            0.321 \n#&gt; respect~~respect   friend~~friend     equal~~equal   nature~~nature \n#&gt;            0.759            0.642            0.859            0.819 \n#&gt;       care~~care           rich~1         admire~1        success~1 \n#&gt;            0.554            0.000            0.000            0.001 \n#&gt;        respect~1         friend~1          equal~1         nature~1 \n#&gt;            0.002            0.000            0.001            0.001 \n#&gt;           care~1 \n#&gt;            0.000\n\n\n\nSee factor analysis on how most terms are interpreted. factor1~factor2 is the coefficient \\(\\beta_1\\) of the structural regression.\n\nThe interpretations of the model are quite straight forward - the measurment models are interpreted in the same way as factor analysis, and the linear model between latent variable is interpreted the same way as a linear model between any other variables.\n\nEx: the linear model says that as explanatory factor increases by 1, response increases by an expected \\(\\beta\\) units.\n\nStructural can be far more complicated. For example:\n\n\n\n\n\n\n\n\nexample2\n\n\nX1\n\nξ 1\n\n\n\nX2\n\nξ 2\n\n\n\nX1-&gt;X2\n\n\n\nCorrelation\n\n\n\nY1\n\nη 2\n\n\n\nX1-&gt;Y1\n\n\nβ1\n\n\n\nY2\n\nη 2\n\n\n\nX1-&gt;Y2\n\n\nγ1\n\n\n\nX2-&gt;Y1\n\n\nβ2\n\n\n\nX2-&gt;Y2\n\n\nγ2\n\n\n\nY1-&gt;Y2\n\n\n\nCorrelation\n\n\n\n\n\n\n\n\n\n\nNote: we cannot have linear models between two independent, or two dependent variables. They have to be seperate - within each group, only correlations are possible.\n\nIn this example, we are measuring the correlation between the independent variables, the correlation between the dependent variables, and we have two regression models:\n\\[\n\\begin{align}\n\\eta_1 & = \\beta_0 + \\beta_1\\ \\xi_1 + \\beta_2\\ \\xi_2 + \\eps_1 \\\\\n\\eta_2 & = \\gamma_0 + \\gamma_1\\ \\xi_1 + \\gamma_2\\ \\xi_2 + \\eps_2 \\\\\n\\end{align}\n\\]\nTo implement these more complicated SEMs in R, we just adjust the formula input. Below shows different examples of how we can specify structural models.\n\nformula &lt;- '\n# Measurement Models, same format, see above.\n\n# Stuctural regression with multiple explanatory variables\n  factor1 ~ factor2 + factor3\n\n# structural covariances\n  factor1 ~~ factor2\n'\n\n\nThe estimation is the same - use the sem() command.",
    "crumbs": [
      "Latent Variable Models",
      "Structural Equation Models"
    ]
  },
  {
    "objectID": "latent/irt.html",
    "href": "latent/irt.html",
    "title": "Item Response Theory",
    "section": "",
    "text": "Item Response Theory (IRT) is an extension of factor analysis, that deals with cases with a continuous latent factor but a binary/categorical set of observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nWe must have at least 3 observed items for estimation purposes. The items are associated with the continuous latent factor with a binomial logistic model:\n\\[\n\\begin{align}\n\\text{logit}[Pr(X_1 = 1 \\ | \\ \\xi)] & = \\tau_1 + \\lambda_1 \\ \\xi \\\\\n\\text{logit}[Pr(X_2 = 1 \\ | \\ \\xi)] & = \\tau_2 + \\lambda_2 \\ \\xi \\\\\n\\text{logit}[Pr(X_3 = 1 \\ | \\ \\xi)] & = \\tau_3 + \\lambda_3 \\ \\xi \\\\\n\\end{align}\n\\]\n\nFor categorical (more than 2 categories) items, a multinomial or ordinal logistic model is used. But this is pretty rare for someone to actually fit a model of this type.\n\n\\(\\tau_i\\) is the intercept of the model, called the difficulty. \\(\\lambda_i\\) is the coefficeint that describes the relationship between any item \\(X_i\\) and the factor. These are called factor loadings. It is also called the discrimination parameter.\nThese factor loadings \\(\\lambda\\) are interepreted in a very similar way to factor analysis. The intercepts \\(\\tau_i\\) also tell us how “common” a value of 1 for the item is. For example, below, the items are policies that politicians support/don’t oppose.\n\n\n\n\n\n\na1 are the loadings \\(\\lambda\\), and d is the intercept. The rows are binary items - with 1 indicating a politician supports that, and 0 indicating a politician opposes that.\n\nWe can first look at the signs of \\(\\lambda\\): The latent factor increases with agreeing (a value of 1) with PartialBirth, CapitalGains, and CAFTA. The latent factor decreases with agreeing (a value of 1) with StemCell, IraqWithdraw, Immigration, and Minimum Wage.\nFrom this, we can kind of get a sense that latent variable is measuring right-left political affiliation, with higher values indicating right-wing, since agreeing left wing policies like IraqWithdraw, Immigration, and Minimum Wage are negative.\nThe absolute size of the factor loadings are quite similar - we can see that CAFTA is the least important item to the factor.\n\nTo run an item response theory mode, we need the mirt package:\n\nlibrary(mirt)\n\nThen, we run the model as follows (make sure to subset your data to only include the items).\n\nmodel &lt;- mirt(data = my_data, model = 1, SE = TRUE)\ncoef(result)\n\n\nmodel = 1 indicates how many factors you want to include.\n\nTo get factor scores, we do the following:\n\nscores &lt;- as.vector(fscores(model, method = \"EAP\"))\n\n\n\n\n\n\n\nMore on Choosing Between Models\n\n\n\n\n\nTo compare and choose between different models, we have a few ways.\n\nWe can use a likelihood ratio test for nested models (see the regression section for more details).\n\n\nlavTestLRT(model1, model2)\n\n\nWe can use a global goodness of fit test - essentially a likelihood ratio but with the full sample covariance matrix as the null model. We want to fail to reject the null, because we want our model to be as close to the sample covariance matrix as possible.\n\n\nlavTestLRT(model)\n\n\nWe can use AIC and BIC to compare models since factor models are estimated with MLE. These are included in the output.\n\nUnfortunately, fit indicies like RMSEA do not work on IRT models.",
    "crumbs": [
      "Latent Variable Models",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "latent/class.html",
    "href": "latent/class.html",
    "title": "Latent Class Model",
    "section": "",
    "text": "Latent class models are an extension of factor analysis, that deals with cases of a categorical latent variable with categorical/binary observed items.\n\n\n\n\n\n\n\n\nexample2\n\n\nF\n\nξ (Unobserved)\n\n\n\nX1\n\nX1 (Observed)\n\n\n\nF-&gt;X1\n\n\nλ\n\n\n\nX2\n\nX2 (Observed)\n\n\n\nF-&gt;X2\n\n\nλ\n\n\n\nX3\n\nX3 (Observed)\n\n\n\nF-&gt;X3\n\n\nλ\n\n\n\n\n\n\n\n\n\nThe factors a categorical variable, which each category \\(c\\) called a latent class. We also have categorical observed items. The latent class models connects each item with a factor through a item response probability:\n\\[\n\\begin{align}\n& Pr(X_1 = \\text{category }k_i \\ | \\ \\xi = \\text{category }c)\\\\\n& Pr(X_2 = \\text{category }k_i  \\ | \\ \\xi = \\text{category }c)\\\\\n& Pr(X_3 = \\text{category }k_i  \\ | \\ \\xi = \\text{category }c)\\\\\n\\end{align}\n\\]\n\n\\(k_1\\) is one specific category of item \\(1\\), same for \\(k_2\\) for item \\(2\\), and so on. \\(c\\) is one specific category of the factor.\n\nWe also have another part of the measurement mode, the structural model, which determines the probability of each category in the factor:\n\\[\n\\alpha_c = Pr(\\xi=\\text{category }c)\n\\]\nInterpretation of the latent factor depends on the item response probabilities. Below, the columns are the different classes/categories of the factor, and the big rows are each item.\n\n\n\n\n\nThe first class (the first column) has the highest probabilities if individuals never worry about crime, no real effect on quality of life, never worry about burglary, and no real effect on quality of life. Thus, we can conclude this first category of the latent variable is something like - not worried about crime.\nThe second class (the 2nd column), where the top responses have the highest probabilities except for the frequency of worry about burglaries - where the probabilities are highest for some of the time and just occasionally. This suggests that this second category is measuring something like - only worried about burglary, and no other crime.\nWe can also create factor scores - which is a little different, because now we are basically assigning every unit in our data to a category of the latent factor. This is done by calculating the posterior probability of being in each class:\n\\[\n\\widehat{Pr}(\\xi = \\text{category }c \\ | \\ X_1 = \\text{category }k_1, X_2 = \\text{category }k_2, \\dots )\n\\]\nWe calculate this probability for all categories \\(c\\) in the factor. Whichever category \\(c\\) of the latent variable has the highest probability, is the category a unit is assigned to.\n\nThis can be considered quite similar to that of cluster analysis, which will be introduced later.\n\n\nTo implement latent class models, we will need the polLCA package:\n\nlibrary(poLCA)\n\nThis package requires that our categories of items are labelled starting with 1. This means if you have a binary variable of 0 and 1, you will need to change it to 1 and 2.\nTo begin, we will first need to create a vector of our item names:\n\nvars &lt;- c(\"item1\",\"item2\",\"item3\",\"item4\")\n\nThen, let us fit our model as follows:\n\nform &lt;- cbind(item1, item2, item3, item4) ~1\nmodel &lt;- poLCA(form,\n              my_data[,vars],\n              nclass = 2, #number of categories for factor\n              na.rm = F,\n              nrep = 10) \n\n\nna.rm = F means to include missing values when estimating (which is recommended). nrep = 10 indicates how many times to run the gradient descent algorithm - more is better, but will take longer.\n\nThe traditional output is hard to read, so we will use a function:\n\n# function\nLCA.probs &lt;- function(res){\n  probs &lt;- res$probs\n  item.p &lt;- NULL\n  for(i in seq_along(probs)){\n        m.tmp &lt;- t(probs[[i]])\n        rownames(m.tmp) &lt;- paste(names(probs)[i],colnames(probs[[i]]),sep=\".\")\n        item.p &lt;- rbind(item.p,m.tmp)\n  }\n  item.p &lt;- round(item.p,3)\n  class.p &lt;- res$P\n  names(class.p) &lt;- colnames(item.p)\n  list(item.probabilities=item.p,class.probabilities=class.p)\n}\n\n# output results\nLCA.probs(model)\n\nWe can calculate factor scores/classification as follows:\n\nmodel$predclass\n\nWe can choose our model based on the AIC or BIC score.\n\nmodel$aic\nmodel$bic",
    "crumbs": [
      "Latent Variable Models",
      "Latent Class Models"
    ]
  },
  {
    "objectID": "latent/pca.html",
    "href": "latent/pca.html",
    "title": "Principle Components Analysis",
    "section": "",
    "text": "Note: PCA is not a latent variable model itself, but can be used to approximate a latent variable model.\n\nPrinciple components analysis (PCA) takes observed variables (called features), and change them into new variables called principle components, without losing any information.\nEssentially, what PCA does is it takes our features, and finds the “axis” in which there is the most variation, and makes that the 1st component. Then it finds the “axis” with the 2nd most variation, and makes that the 2nd component, and so on.\n\n\n\n\n\n\nNote that each principle component is orthogonal/uncorrelated with each other by design. Also note that the 1st component explains the most variance in our features, then the 2nd component, and so on.\n\nEach new principle component is a weighted average of the observed variables:\n\\[\n\\begin{align}\n\\comp_1 & = \\omega_{11}\\feat_1 + \\omega_{21}\\feat_2 + \\omega_{31}\\feat_3 + \\dots \\\\\n\\comp_2 & = \\omega_{12}\\feat_1 + \\omega_{22} \\feat_2 + \\omega_{32}\\feat_3 + \\dots \\\\\n\\end{align}\n\\]\nThe \\(\\omega\\)’s are the weights of each observed feature in creating a principle component, and are determined by the correlation matrix of the features. For interpreting a component, we often “normalise” the weights \\(\\omega\\) to get the correlation between a observed feature and a principle component:\n\\[\nCorr(\\feat_i, \\comp_j) =  \\omega_{ij} \\cdot \\sqrt{Var(\\comp_j)}\n\\]\n\nThis value is also called a component loading. Note that this is only true given we perform PCA on the correlation matrix (which is standard, but you can use a covariance matrix).\n\nInterpreting the principle components is identical to that of factor analysis - just using correlations rather than factor loadings. Below, the rows are features describing how much an individual trusts different institutions. The columns are the principle components.\n\n\n\n\n\nIn component 1, we see that the correlation is pretty high for all of the variables, and positive. We might conclude component 1 measures general trust in institutions.\nIn component 2, we can see that the loadgins for legal and police are the highest (in absolute terms) and negative. Meanwhile, politicians, pol_parties, and EP are positive and still not too small. The other loadings are quite small. We might interpret this component as sort of a tradeoff between political trust and legal/law enforcement trust, with higher values indicating more political trust, and lower values indicating more trust in legal/police.\nJust like factor analysis, we can also calculate principle component scores, which are the individual values of each principle component for each individual in our data.\n\nTo implement principle components analysis, we do the following:\n\npca &lt;- princomp(~ X1 + X2 + X3,\n                data = my_data,\n                cor = TRUE, #use correlation matrix\n                scores = TRUE, #calculate pc scores\n                na.action=na.exclude)\nsummary(pca)\n\nNow, to get the component loadings/corelation, we do the following:\n\n# grab the weights\nweights &lt;- loadings(pca)\n\n# grab the sqrt of variance\nsqrt.var &lt;- pca$sdev\n\n# calculate component loadings/correlation\nprint(t(t(weights)*sqrt.var), cutoff = 0, digits=4)\n\nTo access principle component scores, we do:\n\npca$score\n\nIf we are performing other statistical analysis, we might want to choose how many of the new principle components we want to use. A scree-plot shows the percentage of variance each component explains.\n\nscreeplot(pca, type='l', main=\"\")\n\n\nTo choose the amount of components to use, we look for the “elbow” in the plot - basically when adding another additional PC does not really increase the amount of variance explained significantly anymore.",
    "crumbs": [
      "Latent Variable Models",
      "Principle Components Analysis"
    ]
  }
]