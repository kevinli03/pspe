---
title: "Inference and Model Fit"
---

::: small
Make sure you have read the previous page on [factor analysis](factor.qmd) before this.
:::

Each factor loading and other parameter estimated in a factor analysis model has significance tests. As factor analysis models are estimated with maximum likelihood estimation, we use **z-tests** to test each parameter.

-   The most common test is to see if a factor loading $\lambda$ between an item and factor is statistically significant. If a loading is not significant, it may make sense to fix it to zero in a confirmatory analysis or drop it.
-   We can also test if there is any non-zero covariances between factors, and many other parameters.

Let us say we have fit multiple factor analysis models, with different numbers of items and factors. Which one should we choose? Since factor analysis models are estimated with maximum likelihood estimation, we can use a **likelihood ratio test** between two models to see which is better.

-   The likelihood ratio test compares the likelihood $L$ between two models. One model is "smaller" (in parameters) compared to the other model. We want a higher likelihood, which indicates a better model.
-   The null hypothesis is that there is no difference between our two models. The alternate hypothesis is that there is a difference between our two models.
-   If our test is statistically significant, we can conclude one model is better than another.

::: small
For a likelihood ratio test to work, the models need to be nested - the null model must be smaller than the alternate. For example, you can compare a 1-factor model (null) to a 3-factor model.
:::

Another way to decide is with **global goodness of fit tests**, which is a variation of the likelihood ratio test, but with the saturated "full" model as one of the models. This allows us to compare non-nested models.

-   Our null hypothesis is the "full model" that is the exact sample covariance matrix $\b S$.
-   Our alternate hypothesis is our fitted model with a covariance matrix of $\hat{\b\Sigma}$.
-   We can run a likelihood ratio test to compare the two models. We want to **fail to reject** the null hypothesis (so p\>0.05), because the null hypothesis is that the full model and our fitted model are no different. Since the "full model" is the best model, we do not want to reject the null.

However, all likelihood ratio tests have an issue - larger sample sizes tend to reject the null more often. This is not great - because this doesn't actually tell us objectively how good our model is.

Thus, a solution to these are **information crieterion** statistics, and **fit indicies**. These allow us to compare any two models. There are a large number of these, and it is often standard to just report them all.

-   Akaike's Information Criterion (AIC): lower values are better.
-   Bayesian Information Criterion (BIC): lower values are better.
-   Root Mean Square Error of Approximation (RMSEA): 0 is a perfect fit, 0.05 or smaller is a good fit, and anything above 0.1 is a poor fit.
-   Standard Root Mean Square Residual (SRMR): smaller values the better, anything below 0.08 is a good fit.
-   Tucker and Lewis Index (TLI): 1 is the best fit, anything below 0.9 is a poor fit, and anything above 1 may indicate overfitting.
-   Comparative Fit Index: values between 0 and 1, anything close to 1 is a good fit.

<br />

To implement these tests in R, we will need the **lavaan** package:

```{r, eval = FALSE}
library(lavaan)
```

For an overall goodness of fit test, we use the command:

```{r, eval = F}
lavTestLRT(model)
```

For a likelihood ratio test of nested models, we do:

```{r, eval = F}
lavTestLRT(model1, model2)
```

For goodness of fit statistics, we do:

```{r, eval = F}
summary(model, fit.measures=T)
```
