---
title: "Factor Analysis Model"
---

Factor analysis is a way to model a unobserved continuous latent variable, based on a bunch of continuous observed items.

::: center-graph
```{dot}
//| fig-width: 4
//| fig-height: 1.2
digraph example2 {
  bgcolor="transparent";
  // Nodes
  F [shape=box, pos = "0,0!", label="Latent Factor Variable (Unobserved)"]
  X1 [shape=box, pos = "2,0!", label="Item 1 (Observed)"]
  X2 [shape=box, pos = "1,-2!", label="Item 2 (Observed)"]
  X3 [shape=box, pos="1,0!", label="Item 3 (Observed)"]

  // Edges
  {F -> X1 [label=<<FONT FACE="Arial">&lambda;</FONT>>]}
  {F -> X2 [label=<<FONT FACE="Arial">&lambda;</FONT>>]}
  {F -> X3 [label=<<FONT FACE="Arial">&lambda;</FONT>>]}
  
  graph [nodesep=0.5, ranksep=0.5]

}
```
:::

Each item is connected with the latent factor by a **linear regression**:

$$
\begin{align}
\I_1 & = \tau_1 + \lambda_1 \ \F + \delta_1 \\
\I_2 & = \tau_2 + \lambda_2 \ \F + \delta_2 \\
\I_3 & = \tau_3 + \lambda_3 \ \F + \delta_3 \\
\end{align}
$$

::: small
$\tau$ is the intercept of the model. $\lambda$ is the coefficient, called the factor loadings. $\delta$ is the error term - the part of an item not explained by the factor - they are called the **unique factors**.
:::

::: {.callout-note collapse="true"}
## Details: Assumptions of the Model

::: append
We make a few assumptions on this model:

1.  We assume the factor is continuous, and normally distributed with mean $\kappa$ and variance $\phi$. We often assume $\kappa = 0$ and $\phi = 1$ for identification purposes.
2.  We assume the items are also normally distributed.
3.  We assume that the error terms are normally distributed $\delta_i \sim \mathcal N(0, \theta_{ii})$, that the different error terms $\delta_1, \dots, \delta_p$ are uncorrelated with each other. This implies that the correlation between any two items is entirely explained by the factor (there is no separate correlation between items).
4.  We assume the factor is uncorrelated with the error term (exogeneity).
:::
:::

The factor loadings $\lambda_i$ represent the relationship/**covariance** between any item and a factor. These factor loadings help us interpret our latent variable. The sign of the factor loading tells us the direction in which our latent variable is measuring. The absolute size of the factor loading tells us how important that item is to the factor.

For example, take this factor variable called **personality**, explained by a set of items (rich, admire, success, respect) that show how important a certain quality is to an individual.

![](images/clipboard-2966252668.png){fig-align="center" width="55%"}

We can see all the factor loadings $\lambda$ are positive, which means that the higher values of the **personality** factor is measuring higher levels of importance of being rich, being admired, being successful, and being respected.

We can also see the loadings for **respect** and **success** are much higher than **admire** or **rich**. Thus, we can conclude the **personality** factor is more measuring the importance of being respected or successful, than being rich or admired.

::: {.callout-note collapse="true"}
## Details: More and Factor Loadings

::: append
If all our items are standardised to a standard normal distribution, that also implies that our factor loadings $\lambda$ are equal to the correlation coefficients between items and factor.

We can conduct hypothesis testing with each factor loading $\lambda$ with a **z-test** to see if there is a significant relationship between a factor an an item.
:::
:::

<br />

To implement factor analysis, we will need the **psych** and **GPArotation** package:

```{r, eval = F}
library(psych)
library(GPArotation)
```

First, we should get rid of missing observations:

```{r, eval = F}
all.obs <- apply(my_data, 1, FUN=function(x){all(!is.na(x))})
dta <- my_data[all.obs,]
```

For factor analysis with one factor, we use the syntax:

```{r, eval = F}
fa <- fa(data[,items], nfactors=1, fm="ml")
print(fa1)
```
