---
title: "Principle Components Analysis"
---

::: small
Note: PCA is not a latent variable model itself, but can be used to approximate a latent variable model.
:::

Principle components analysis (PCA) takes observed variables (called **features**), and change them into new variables called **principle components**, without losing any information.

Essentially, what PCA does is it takes our features, and finds the "axis" in which there is the most variation, and makes that the 1st component. Then it finds the "axis" with the 2nd most variation, and makes that the 2nd component, and so on.

![](images/clipboard-1131163029.png){fig-align="center" width="60%"}

::: small
Note that each principle component is orthogonal/uncorrelated with each other by design. Also note that the 1st component explains the most variance in our features, then the 2nd component, and so on.
:::

Each new principle component is a weighted average of the observed variables:

$$
\begin{align}
\comp_1 & = \omega_{11}\feat_1 + \omega_{21}\feat_2 + \omega_{31}\feat_3 + \dots \\
\comp_2 & = \omega_{12}\feat_1 + \omega_{22} \feat_2 + \omega_{32}\feat_3 + \dots \\
\end{align}
$$

The $\omega$'s are the weights of each observed feature in creating a principle component, and are determined by the correlation matrix of the features. For interpreting a component, we often "normalise" the weights $\omega$ to get the correlation between a observed feature and a principle component:

$$
Corr(\feat_i, \comp_j) =  \omega_{ij} \cdot \sqrt{Var(\comp_j)}
$$

::: small
This value is also called a **component loading**. Note that this is only true given we perform PCA on the correlation matrix (which is standard, but you can use a covariance matrix).
:::

Interpreting the principle components is identical to that of [factor analysis](factor.qmd) - just using correlations rather than factor loadings. Below, the rows are features describing how much an individual trusts different institutions. The columns are the principle components.

![](images/clipboard-8264992.png){fig-align="center" width="80%"}

In component 1, we see that the correlation is pretty high for all of the variables, and positive. We might conclude component 1 measures general trust in institutions.

In component 2, we can see that the loadgins for *legal* and *police* are the highest (in absolute terms) and negative. Meanwhile, *politicians, pol_parties*, and *EP* are positive and still not too small. The other loadings are quite small. We might interpret this component as sort of a tradeoff between political trust and legal/law enforcement trust, with higher values indicating more political trust, and lower values indicating more trust in legal/police.

Just like factor analysis, we can also calculate **principle component scores,** which are the individual values of each principle component for each individual in our data.

<br />

To implement principle components analysis, we do the following:

```{r, eval = F}
pca <- princomp(~ X1 + X2 + X3,
                data = my_data,
                cor = TRUE, #use correlation matrix
                scores = TRUE, #calculate pc scores
                na.action=na.exclude)
summary(pca)
```

Now, to get the component loadings/corelation, we do the following:

```{r, eval = F}
# grab the weights
weights <- loadings(pca)

# grab the sqrt of variance
sqrt.var <- pca$sdev

# calculate component loadings/correlation
print(t(t(weights)*sqrt.var), cutoff = 0, digits=4)
```

To access principle component scores, we do:

```{r, eval = F}
pca$score
```

If we are performing other statistical analysis, we might want to choose how many of the new principle components we want to use. A scree-plot shows the percentage of variance each component explains.

```{r, eval = F}
screeplot(pca, type='l', main="")
```

::: small
To choose the amount of components to use, we look for the "elbow" in the plot - basically when adding another additional PC does not really increase the amount of variance explained significantly anymore.
:::
